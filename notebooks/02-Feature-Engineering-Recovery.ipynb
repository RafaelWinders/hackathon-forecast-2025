{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-B - Feature Engineering Recovery (Otimizado para Baixa Memória)\n",
    "\n",
    "Este notebook implementa uma versão simplificada e otimizada do feature engineering, projetada para funcionar em ambientes com limitações de memória.\n",
    "\n",
    "## Estratégia:\n",
    "1. **Processar apenas dados essenciais**\n",
    "2. **Usar agregações em chunks**\n",
    "3. **Salvar resultados intermediários**\n",
    "4. **Features mínimas mas eficazes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('📚 Bibliotecas carregadas - Versão Recovery')\n",
    "print('💡 Estratégia: Processamento otimizado para baixa memória')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar apenas dados de transações (o mais importante)\n",
    "print('📂 Carregando dados de transações...')\n",
    "transacoes = pd.read_parquet('../data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet')\n",
    "\n",
    "print(f'📊 Transações carregadas: {transacoes.shape}')\n",
    "print(f'💾 Memória: {transacoes.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "\n",
    "# Renomear e preparar colunas essenciais\n",
    "dados = transacoes[['internal_store_id', 'internal_product_id', 'transaction_date', 'quantity']].copy()\n",
    "dados.columns = ['pdv_id', 'produto_id', 'data', 'quantidade']\n",
    "dados['data'] = pd.to_datetime(dados['data'])\n",
    "\n",
    "# Liberar memória\n",
    "del transacoes\n",
    "gc.collect()\n",
    "\n",
    "print('✅ Dados preparados e memória otimizada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agregação Semanal Simplificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar semanas\n",
    "dados['semana'] = dados['data'].dt.to_period('W-MON').dt.start_time\n",
    "\n",
    "print(f'📅 Período: {dados[\"semana\"].min()} até {dados[\"semana\"].max()}')\n",
    "print(f'📊 Total de semanas: {dados[\"semana\"].nunique()}')\n",
    "\n",
    "# Agregação semanal\n",
    "print('🔄 Realizando agregação semanal...')\n",
    "agregacao_semanal = dados.groupby(['semana', 'pdv_id', 'produto_id']).agg({\n",
    "    'quantidade': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(f'📊 Agregação criada: {agregacao_semanal.shape}')\n",
    "\n",
    "# Liberar dados originais\n",
    "del dados\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid Inteligente Simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar apenas combinações ativas (estratégia que funcionou)\n",
    "combinacoes_ativas = agregacao_semanal[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "semanas_unicas = sorted(agregacao_semanal['semana'].unique())\n",
    "\n",
    "print(f'🎯 Combinações ativas: {len(combinacoes_ativas):,}')\n",
    "print(f'📅 Semanas: {len(semanas_unicas)}')\n",
    "\n",
    "# Criar grid em lotes pequenos\n",
    "batch_size = 5000  # Lotes pequenos\n",
    "grid_parts = []\n",
    "\n",
    "for i in range(0, len(combinacoes_ativas), batch_size):\n",
    "    batch = combinacoes_ativas.iloc[i:i+batch_size]\n",
    "    print(f'📦 Processando lote {i//batch_size + 1}: {len(batch)} combinações')\n",
    "    \n",
    "    # Criar grid para este lote\n",
    "    batch_grids = []\n",
    "    for _, row in batch.iterrows():\n",
    "        grid = pd.DataFrame({\n",
    "            'semana': semanas_unicas,\n",
    "            'pdv_id': row['pdv_id'],\n",
    "            'produto_id': row['produto_id']\n",
    "        })\n",
    "        batch_grids.append(grid)\n",
    "    \n",
    "    # Concatenar lote\n",
    "    batch_grid = pd.concat(batch_grids, ignore_index=True)\n",
    "    \n",
    "    # Merge com vendas\n",
    "    batch_grid = batch_grid.merge(agregacao_semanal, on=['semana', 'pdv_id', 'produto_id'], how='left')\n",
    "    batch_grid['quantidade'] = batch_grid['quantidade'].fillna(0)\n",
    "    \n",
    "    grid_parts.append(batch_grid)\n",
    "    \n",
    "    # Limpeza\n",
    "    del batch_grids, batch_grid\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenar todas as partes\n",
    "print('🔗 Concatenando grid completo...')\n",
    "dados_grid = pd.concat(grid_parts, ignore_index=True)\n",
    "\n",
    "# Limpeza\n",
    "del grid_parts, agregacao_semanal\n",
    "gc.collect()\n",
    "\n",
    "print(f'✅ Grid criado: {dados_grid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Features Essenciais (Mínimas mas Eficazes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar dados\n",
    "dados_grid = dados_grid.sort_values(['pdv_id', 'produto_id', 'semana'])\n",
    "\n",
    "# Features temporais básicas\n",
    "dados_grid['mes'] = dados_grid['semana'].dt.month\n",
    "dados_grid['semana_ano'] = dados_grid['semana'].dt.isocalendar().week\n",
    "\n",
    "# Features de lag (apenas as mais importantes)\n",
    "print('⏰ Criando features de lag...')\n",
    "for lag in [1, 2, 4]:  # Apenas 3 lags mais importantes\n",
    "    dados_grid[f'quantidade_lag_{lag}'] = dados_grid.groupby(['pdv_id', 'produto_id'])['quantidade'].shift(lag)\n",
    "\n",
    "# Rolling features (simplificado)\n",
    "print('📊 Criando rolling features...')\n",
    "dados_grid['quantidade_media_4w'] = (\n",
    "    dados_grid.groupby(['pdv_id', 'produto_id'])['quantidade']\n",
    "    .rolling(window=4, min_periods=1)\n",
    "    .mean()\n",
    "    .reset_index(level=[0,1], drop=True)\n",
    ")\n",
    "\n",
    "# Features categóricas simples\n",
    "dados_grid['pdv_hash'] = dados_grid['pdv_id'].astype(str).apply(hash) % 100\n",
    "dados_grid['produto_hash'] = dados_grid['produto_id'].astype(str).apply(hash) % 100\n",
    "\n",
    "print('✅ Features essenciais criadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Limpeza Final Simplificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover registros sem lag_4 (mais simples)\n",
    "print(f'📊 Dados antes da limpeza: {dados_grid.shape}')\n",
    "\n",
    "dados_final = dados_grid[dados_grid['quantidade_lag_4'].notna()].copy()\n",
    "\n",
    "print(f'📊 Dados após limpeza: {dados_final.shape}')\n",
    "print(f'💾 Memória final: {dados_final.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "\n",
    "# Verificar distribuição\n",
    "zeros = (dados_final['quantidade'] == 0).sum()\n",
    "non_zeros = (dados_final['quantidade'] > 0).sum()\n",
    "print(f'📈 Zeros: {zeros:,} ({zeros/len(dados_final)*100:.1f}%)')\n",
    "print(f'📈 Não-zeros: {non_zeros:,} ({non_zeros/len(dados_final)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvamento Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar dataset final\n",
    "dados_final.to_csv('../data/dados_features_completo.csv', index=False)\n",
    "print('💾 Dataset salvo: dados_features_completo.csv')\n",
    "\n",
    "# Salvar metadados\n",
    "import pickle\n",
    "\n",
    "metadata = {\n",
    "    'data_processamento': pd.Timestamp.now(),\n",
    "    'total_registros': len(dados_final),\n",
    "    'total_features': len(dados_final.columns),\n",
    "    'estrategia': 'Grid Inteligente Simplificado - Recovery',\n",
    "    'features_criadas': list(dados_final.columns),\n",
    "    'periodo_treino': f\"{dados_final['semana'].min()} a {dados_final['semana'].max()}\"\n",
    "}\n",
    "\n",
    "with open('../data/feature_engineering_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print('📋 Metadados salvos: feature_engineering_metadata.pkl')\n",
    "\n",
    "# Mostrar resumo final\n",
    "print('\\n🎉 FEATURE ENGINEERING RECOVERY CONCLUÍDO!')\n",
    "print('=' * 50)\n",
    "print(f'📊 Dataset final: {dados_final.shape}')\n",
    "print(f'🏷️ Features: {list(dados_final.columns)}')\n",
    "print('🚀 Pronto para modelagem!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}