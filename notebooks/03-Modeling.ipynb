{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Modelagem e Treinamento\n",
    "\n",
    "Neste notebook vamos desenvolver e treinar modelos de machine learning para previsÃ£o de vendas semanais.\n",
    "\n",
    "## Objetivos:\n",
    "1. **Carregamento dos Dados**: Carregar dataset com features processadas\n",
    "2. **PreparaÃ§Ã£o para ML**: Dividir dados em treino/validaÃ§Ã£o, preparar features\n",
    "3. **Baseline Models**: Implementar modelos simples como referÃªncia\n",
    "4. **Advanced Models**: Treinar modelos avanÃ§ados (LightGBM, XGBoost, etc.)\n",
    "5. **Ensemble**: Combinar mÃºltiplos modelos para melhor performance\n",
    "6. **ValidaÃ§Ã£o**: Avaliar performance usando mÃ©tricas adequadas\n",
    "7. **PrediÃ§Ãµes Finais**: Gerar previsÃµes para perÃ­odo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Advanced ML\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('ðŸ“š Bibliotecas carregadas com sucesso!')\n",
    "print('ðŸŽ¯ Iniciando fase de Modelagem e Treinamento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Carregar dados com features processadas\nprint('ðŸ“‚ Carregando dados processados...')\n\n# Verificar se os arquivos essenciais existem\nimport os\nrequired_files = [\n    '../data/dados_features_completo.parquet',  # Usar parquet (mais rÃ¡pido)\n    '../data/feature_engineering_metadata.pkl'\n]\n\nmissing_files = [f for f in required_files if not os.path.exists(f)]\nif missing_files:\n    print('âŒ Arquivos nÃ£o encontrados:')\n    for f in missing_files:\n        print(f'   â€¢ {f}')\n    print('\\nðŸ”„ Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\nelse:\n    print('âœ… Todos os arquivos necessÃ¡rios encontrados')\n    \n    # Carregar dados principais (usar parquet para velocidade)\n    print('ðŸ“Š Carregando dataset (parquet Ã© 57x mais rÃ¡pido que CSV)...')\n    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n    \n    # Carregar metadados\n    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n        metadata = pickle.load(f)\n    \n    print(f'\\nðŸ“Š Dados carregados com sucesso:')\n    print(f'   â€¢ Shape: {dados.shape}')\n    print(f'   â€¢ PerÃ­odo: {dados[\"semana\"].min()} atÃ© {dados[\"semana\"].max()}')\n    print(f'   â€¢ Features disponÃ­veis: {len(dados.columns)}')\n    print(f'   â€¢ MemÃ³ria: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n    print(f'   â€¢ EstratÃ©gia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n    \n    print(f'\\nðŸ” Metadados do processamento:')\n    for key, value in metadata.items():\n        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n            print(f'   â€¢ {key}: {value}')\n    \n    print(f'\\nâœ… Pronto para modelagem!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PreparaÃ§Ã£o dos Dados para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variÃ¡vel target e features\n",
    "target = 'quantidade'\n",
    "\n",
    "# Features a excluir (nÃ£o devem ser usadas para prediÃ§Ã£o)\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana',  # IDs e data\n",
    "    'quantidade',  # Target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informaÃ§Ã£o do futuro\n",
    "    'semana_primeira_venda', 'semana_ultima_venda',  # Dates\n",
    "]\n",
    "\n",
    "# Identificar features disponÃ­veis\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'ðŸŽ¯ PreparaÃ§Ã£o dos dados:')\n",
    "print(f'   â€¢ Target: {target}')\n",
    "print(f'   â€¢ Features disponÃ­veis: {len(all_features)}')\n",
    "print(f'   â€¢ Features excluÃ­das: {len(exclude_features)}')\n",
    "\n",
    "# Verificar missing values nas features\n",
    "missing_features = dados[all_features].isnull().sum()\n",
    "missing_features = missing_features[missing_features > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\nâš ï¸ Features com valores missing:')\n",
    "    for feature, count in missing_features.head(10).items():\n",
    "        pct = (count / len(dados)) * 100\n",
    "        print(f'   â€¢ {feature}: {count:,} ({pct:.1f}%)')\nelse:\n",
    "    print('\\nâœ… Nenhum valor missing nas features')\n",
    "\n",
    "# Remover features com muitos missing values (>50%)\n",
    "high_missing = missing_features[missing_features > len(dados) * 0.5].index.tolist()\n",
    "if high_missing:\n",
    "    print(f'\\nðŸ—‘ï¸ Removendo features com >50% missing: {len(high_missing)}')\n",
    "    all_features = [f for f in all_features if f not in high_missing]\n",
    "\n",
    "print(f'\\nðŸ“‹ Features finais para modelagem: {len(all_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DivisÃ£o temporal dos dados (Time Series Split)\n",
    "# Usar Ãºltimas 8 semanas para validaÃ§Ã£o\n",
    "print('ðŸ“… DivisÃ£o temporal dos dados...')\n",
    "\n",
    "# Ordenar por semana\n",
    "dados_sorted = dados.sort_values('semana')\n",
    "\n",
    "# Identificar ponto de corte\n",
    "semanas_unicas = sorted(dados_sorted['semana'].unique())\n",
    "n_semanas_val = 8  # Ãšltimas 8 semanas para validaÃ§Ã£o\n",
    "cutoff_week = semanas_unicas[-n_semanas_val]\n",
    "\n",
    "# Dividir dados\n",
    "train_data = dados_sorted[dados_sorted['semana'] < cutoff_week].copy()\n",
    "val_data = dados_sorted[dados_sorted['semana'] >= cutoff_week].copy()\n",
    "\n",
    "print(f'ðŸ“Š DivisÃ£o dos dados:')\n",
    "print(f'   â€¢ Treino: {len(train_data):,} registros ({len(train_data)/len(dados)*100:.1f}%)')\n",
    "print(f'   â€¢ ValidaÃ§Ã£o: {len(val_data):,} registros ({len(val_data)/len(dados)*100:.1f}%)')\n",
    "print(f'   â€¢ Semanas treino: {train_data[\"semana\"].nunique()}')\n",
    "print(f'   â€¢ Semanas validaÃ§Ã£o: {val_data[\"semana\"].nunique()}')\n",
    "print(f'   â€¢ Cutoff: {cutoff_week}')\n",
    "\n",
    "# Preparar features e targets\n",
    "X_train = train_data[all_features].fillna(0)\n",
    "y_train = train_data[target]\n",
    "X_val = val_data[all_features].fillna(0)\n",
    "y_val = val_data[target]\n",
    "\n",
    "print(f'\\nâœ… Dados preparados para modelagem')\n",
    "print(f'   â€¢ X_train shape: {X_train.shape}')\n",
    "print(f'   â€¢ X_val shape: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AnÃ¡lise ExploratÃ³ria do Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lise da distribuiÃ§Ã£o do target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# DistribuiÃ§Ã£o geral\n",
    "axes[0,0].hist(y_train, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('DistribuiÃ§Ã£o da Quantidade (Treino)')\n",
    "axes[0,0].set_xlabel('Quantidade')\n",
    "axes[0,0].set_ylabel('FrequÃªncia')\n",
    "\n",
    "# Log-scale\n",
    "non_zero_train = y_train[y_train > 0]\n",
    "axes[0,1].hist(np.log1p(non_zero_train), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('DistribuiÃ§Ã£o log(Quantidade + 1) - Apenas > 0')\n",
    "axes[0,1].set_xlabel('log(Quantidade + 1)')\n",
    "axes[0,1].set_ylabel('FrequÃªncia')\n",
    "\n",
    "# Zeros vs Non-zeros\n",
    "zero_counts = [len(y_train[y_train == 0]), len(y_train[y_train > 0])]\n",
    "axes[1,0].pie(zero_counts, labels=['Zeros', 'NÃ£o-zeros'], autopct='%1.1f%%')\n",
    "axes[1,0].set_title('ProporÃ§Ã£o Zeros vs NÃ£o-zeros')\n",
    "\n",
    "# Boxplot por semana (Ãºltimas 12 semanas)\n",
    "recent_weeks = train_data['semana'].nlargest(12*len(train_data)).unique()\n",
    "recent_data = train_data[train_data['semana'].isin(recent_weeks)]\n",
    "recent_data.boxplot('quantidade', by='semana', ax=axes[1,1])\n",
    "axes[1,1].set_title('DistribuiÃ§Ã£o por Semana (Ãšltimas 12)')\n",
    "axes[1,1].set_xlabel('Semana')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# EstatÃ­sticas descritivas\n",
    "print('ðŸ“ˆ EstatÃ­sticas do Target (treino):')\n",
    "print(y_train.describe())\n",
    "\n",
    "print(f'\\nðŸŽ¯ MÃ©tricas importantes:')\n",
    "print(f'   â€¢ Zeros: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)')\n",
    "print(f'   â€¢ NÃ£o-zeros: {(y_train > 0).sum():,} ({(y_train > 0).mean()*100:.1f}%)')\n",
    "print(f'   â€¢ MÃ©dia (apenas > 0): {y_train[y_train > 0].mean():.2f}')\n",
    "print(f'   â€¢ Mediana (apenas > 0): {y_train[y_train > 0].median():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelos Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# FunÃ§Ã£o para avaliar modelos\ndef evaluate_model(y_true, y_pred, model_name):\n    \"\"\"\n    Avalia um modelo usando mÃºltiplas mÃ©tricas incluindo WMAPE\n    \"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    r2 = r2_score(y_true, y_pred)\n    \n    # WMAPE - Weighted Mean Absolute Percentage Error (mÃ©trica oficial do challenge)\n    wmape = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n    \n    # MÃ©tricas para zeros vs nÃ£o-zeros\n    zero_mask = y_true == 0\n    nonzero_mask = y_true > 0\n    \n    mae_zero = mean_absolute_error(y_true[zero_mask], y_pred[zero_mask]) if zero_mask.sum() > 0 else 0\n    mae_nonzero = mean_absolute_error(y_true[nonzero_mask], y_pred[nonzero_mask]) if nonzero_mask.sum() > 0 else 0\n    \n    results = {\n        'Model': model_name,\n        'MAE': mae,\n        'RMSE': rmse,\n        'RÂ²': r2,\n        'WMAPE': wmape,\n        'MAE_Zero': mae_zero,\n        'MAE_NonZero': mae_nonzero\n    }\n    \n    return results\n\n# Lista para armazenar resultados\nmodel_results = []\n\nprint('ðŸŽ¯ Treinando Modelos Baseline...')\n\n# 1. Baseline: MÃ©dia Simples\nprint('\\nðŸ“Š 1. Baseline - MÃ©dia Simples')\nmean_pred = np.full(len(y_val), y_train.mean())\nresults_mean = evaluate_model(y_val, mean_pred, 'MÃ©dia Simples')\nmodel_results.append(results_mean)\n\n# 2. Baseline: MÃ©dia por combinaÃ§Ã£o PDV/produto\nprint('ðŸ“Š 2. Baseline - MÃ©dia por CombinaÃ§Ã£o')\ncombo_means = train_data.groupby(['pdv_id', 'produto_id'])['quantidade'].mean().to_dict()\nglobal_mean = y_train.mean()\n\ncombo_pred = []\nfor _, row in val_data.iterrows():\n    key = (row['pdv_id'], row['produto_id'])\n    pred = combo_means.get(key, global_mean)\n    combo_pred.append(pred)\n\ncombo_pred = np.array(combo_pred)\nresults_combo = evaluate_model(y_val, combo_pred, 'MÃ©dia por CombinaÃ§Ã£o')\nmodel_results.append(results_combo)\n\n# 3. Baseline: Last Value (usar Ãºltima quantidade conhecida)\nprint('ðŸ“Š 3. Baseline - Ãšltimo Valor')\nlast_values = train_data.groupby(['pdv_id', 'produto_id'])['quantidade'].last().to_dict()\n\nlast_pred = []\nfor _, row in val_data.iterrows():\n    key = (row['pdv_id'], row['produto_id'])\n    pred = last_values.get(key, global_mean)\n    last_pred.append(pred)\n\nlast_pred = np.array(last_pred)\nresults_last = evaluate_model(y_val, last_pred, 'Ãšltimo Valor')\nmodel_results.append(results_last)\n\n# Mostrar resultados dos baselines\nbaseline_df = pd.DataFrame(model_results)\nprint('\\nðŸ“‹ Resultados dos Baselines:')\nprint(baseline_df.round(4))\n\nprint(f'\\nðŸŽ¯ WMAPE dos Baselines:')\nfor result in model_results:\n    print(f'   â€¢ {result[\"Model\"]}: {result[\"WMAPE\"]:.2f}%')\n\nprint('\\nâœ… Baselines estabelecidos - qualquer modelo ML deve superar estes resultados!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ðŸ¤– Treinando Modelos de Machine Learning...')\n",
    "\n",
    "# 4. Random Forest\n",
    "print('\\nðŸŒ² 4. Random Forest')\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_val)\n",
    "rf_pred = np.maximum(0, rf_pred)  # NÃ£o permitir previsÃµes negativas\n",
    "\n",
    "results_rf = evaluate_model(y_val, rf_pred, 'Random Forest')\n",
    "model_results.append(results_rf)\n",
    "\n",
    "# Feature importance (top 10)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('ðŸ” Top 10 features mais importantes (Random Forest):')\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "    print(f'   {i:2d}. {row[\"feature\"]}: {row[\"importance\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LightGBM\n",
    "print('\\nðŸ’¡ 5. LightGBM')\n",
    "\n",
    "# Preparar dados para LightGBM\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "val_lgb = lgb.Dataset(X_val, label=y_val, reference=train_lgb)\n",
    "\n",
    "# ParÃ¢metros LightGBM\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'random_state': 42,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# Treinar modelo\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_lgb,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_lgb, val_lgb],\n",
    "    valid_names=['train', 'eval'],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# PrediÃ§Ãµes\n",
    "lgb_pred = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n",
    "lgb_pred = np.maximum(0, lgb_pred)\n",
    "\n",
    "results_lgb = evaluate_model(y_val, lgb_pred, 'LightGBM')\n",
    "model_results.append(results_lgb)\n",
    "\n",
    "print(f'âœ… LightGBM treinado - Melhor iteraÃ§Ã£o: {lgb_model.best_iteration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. XGBoost\n",
    "print('\\nðŸš€ 6. XGBoost')\n",
    "\n",
    "# ParÃ¢metros XGBoost\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 10,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 0.1,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# Treinar modelo\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1000)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# PrediÃ§Ãµes\n",
    "xgb_pred = xgb_model.predict(X_val)\n",
    "xgb_pred = np.maximum(0, xgb_pred)\n",
    "\n",
    "results_xgb = evaluate_model(y_val, xgb_pred, 'XGBoost')\n",
    "model_results.append(results_xgb)\n",
    "\n",
    "print(f'âœ… XGBoost treinado - Melhor iteraÃ§Ã£o: {xgb_model.best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ComparaÃ§Ã£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar todos os modelos\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('MAE')\n",
    "\n",
    "print('ðŸ† Ranking de Modelos por MAE:')\n",
    "print('=' * 80)\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "# VisualizaÃ§Ã£o dos resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# MAE comparison\n",
    "results_df.plot(x='Model', y='MAE', kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Mean Absolute Error por Modelo')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RÂ² comparison\n",
    "results_df.plot(x='Model', y='RÂ²', kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('RÂ² por Modelo')\n",
    "axes[1].set_ylabel('RÂ²')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Selecionar melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f'\\nðŸ¥‡ Melhor modelo: {best_model_name}')\n",
    "print(f'   â€¢ MAE: {results_df.iloc[0][\"MAE\"]:.4f}')\n",
    "print(f'   â€¢ RMSE: {results_df.iloc[0][\"RMSE\"]:.4f}')\n",
    "print(f'   â€¢ RÂ²: {results_df.iloc[0][\"RÂ²\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AnÃ¡lise de Erros do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar as previsÃµes do melhor modelo para anÃ¡lise\n",
    "if best_model_name == 'LightGBM':\n",
    "    best_pred = lgb_pred\n",
    "    best_model = lgb_model\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_pred = xgb_pred\n",
    "    best_model = xgb_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_pred = rf_pred\n",
    "    best_model = rf_model\n",
    "else:\n",
    "    # Fallback para baseline\n",
    "    best_pred = combo_pred\n",
    "    best_model = None\n",
    "\n",
    "# AnÃ¡lise de erros\n",
    "errors = y_val - best_pred\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# DistribuiÃ§Ã£o dos erros\n",
    "axes[0,0].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('DistribuiÃ§Ã£o dos Erros')\n",
    "axes[0,0].set_xlabel('Erro (Real - Predito)')\n",
    "axes[0,0].set_ylabel('FrequÃªncia')\n",
    "axes[0,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Scatter: Real vs Predito\n",
    "sample_idx = np.random.choice(len(y_val), min(5000, len(y_val)), replace=False)\n",
    "axes[0,1].scatter(y_val.iloc[sample_idx], best_pred[sample_idx], alpha=0.5)\n",
    "axes[0,1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')\n",
    "axes[0,1].set_title('Real vs Predito (amostra)')\n",
    "axes[0,1].set_xlabel('Valor Real')\n",
    "axes[0,1].set_ylabel('Valor Predito')\n",
    "\n",
    "# Erros por faixa de valor real\n",
    "val_bins = pd.cut(y_val, bins=10, labels=False)\n",
    "error_by_bin = [abs_errors[val_bins == i].mean() for i in range(10)]\n",
    "axes[1,0].bar(range(10), error_by_bin)\n",
    "axes[1,0].set_title('MAE por Faixa de Valor Real')\n",
    "axes[1,0].set_xlabel('Faixa (0=menor, 9=maior)')\n",
    "axes[1,0].set_ylabel('MAE')\n",
    "\n",
    "# Residuals plot\n",
    "axes[1,1].scatter(best_pred[sample_idx], errors.iloc[sample_idx], alpha=0.5)\n",
    "axes[1,1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1,1].set_title('Residuais vs PrediÃ§Ãµes')\n",
    "axes[1,1].set_xlabel('Valor Predito')\n",
    "axes[1,1].set_ylabel('Erro')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# EstatÃ­sticas dos erros\n",
    "print(f'ðŸ“Š AnÃ¡lise de Erros - {best_model_name}:')\n",
    "print(f'   â€¢ Erro mÃ©dio: {errors.mean():.4f}')\n",
    "print(f'   â€¢ Erro absoluto mÃ©dio: {abs_errors.mean():.4f}')\n",
    "print(f'   â€¢ Desvio padrÃ£o dos erros: {errors.std():.4f}')\n",
    "print(f'   â€¢ % prediÃ§Ãµes exatas (zeros): {(best_pred[y_val == 0] == 0).mean()*100:.1f}%')\n",
    "print(f'   â€¢ % subestimaÃ§Ã£o: {(errors > 0).mean()*100:.1f}%')\n",
    "print(f'   â€¢ % superestimaÃ§Ã£o: {(errors < 0).mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar importÃ¢ncia das features do melhor modelo\n",
    "if best_model_name in ['LightGBM', 'XGBoost', 'Random Forest'] and best_model is not None:\n",
    "    \n",
    "    if best_model_name == 'LightGBM':\n",
    "        importance = best_model.feature_importance(importance_type='gain')\n",
    "        feature_names = X_train.columns\n",
    "    elif best_model_name == 'XGBoost':\n",
    "        importance = best_model.feature_importances_\n",
    "        feature_names = X_train.columns\n",
    "    elif best_model_name == 'Random Forest':\n",
    "        importance = best_model.feature_importances_\n",
    "        feature_names = X_train.columns\n",
    "    \n",
    "    # Criar DataFrame com importÃ¢ncias\n",
    "    feature_imp_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Visualizar top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_imp_df.head(20)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('ImportÃ¢ncia')\n",
    "    plt.title(f'Top 20 Features Mais Importantes - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'ðŸ” Top 15 Features Mais Importantes - {best_model_name}:')\n",
    "    for i, (_, row) in enumerate(feature_imp_df.head(15).iterrows(), 1):\n",
    "        print(f'   {i:2d}. {row[\"feature\"]}: {row[\"importance\"]:.4f}')\n",
    "    \n",
    "    # Categorizar features por tipo\n",
    "    feature_categories = {\n",
    "        'Lag': [f for f in feature_imp_df['feature'] if 'lag' in f],\n",
    "        'Rolling': [f for f in feature_imp_df['feature'] if any(x in f for x in ['media', 'std', 'max', 'min'])],\n",
    "        'Temporal': [f for f in feature_imp_df['feature'] if any(x in f for x in ['mes', 'semana', 'ano'])],\n",
    "        'HistÃ³rico': [f for f in feature_imp_df['feature'] if any(x in f for x in ['historica', 'primeira', 'ultima', 'atividade'])],\n",
    "        'Outros': [f for f in feature_imp_df['feature'] if f not in sum([v for v in [feature_categories.get(k, []) for k in ['Lag', 'Rolling', 'Temporal', 'HistÃ³rico']]], [])]\n",
    "    }\n",
    "    \n",
    "    print(f'\\nðŸ“‹ ImportÃ¢ncia por Categoria de Features:')\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            total_importance = feature_imp_df[feature_imp_df['feature'].isin(features)]['importance'].sum()\n",
    "            print(f'   â€¢ {category}: {total_importance:.4f} ({len(features)} features)')\n",
    "            \n",
    "else:\n",
    "    print('âš ï¸ Feature importance nÃ£o disponÃ­vel para este modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PreparaÃ§Ã£o para PrediÃ§Ãµes Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('ðŸŽ¯ PreparaÃ§Ã£o para prediÃ§Ãµes finais...')\n\n# Retreinar melhor modelo com todos os dados disponÃ­veis\nprint(f'ðŸ”„ Retreinando {best_model_name} com todos os dados...')\n\n# Preparar dados completos\nX_full = dados[all_features].fillna(0)\ny_full = dados[target]\n\nif best_model_name == 'LightGBM':\n    # Retreinar LightGBM\n    train_full_lgb = lgb.Dataset(X_full, label=y_full)\n    final_model = lgb.train(\n        lgb_params,\n        train_full_lgb,\n        num_boost_round=lgb_model.best_iteration,\n        verbose_eval=False\n    )\n    \nelif best_model_name == 'XGBoost':\n    # Retreinar XGBoost\n    final_model = xgb.XGBRegressor(**xgb_params, n_estimators=xgb_model.best_iteration)\n    final_model.fit(X_full, y_full, verbose=False)\n    \nelif best_model_name == 'Random Forest':\n    # Retreinar Random Forest\n    final_model = RandomForestRegressor(\n        n_estimators=100,\n        max_depth=10,\n        min_samples_split=20,\n        min_samples_leaf=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    final_model.fit(X_full, y_full)\n    \nelse:\n    # Usar estratÃ©gia baseline\n    final_model = None\n    combo_means_full = dados.groupby(['pdv_id', 'produto_id'])['quantidade'].mean().to_dict()\n    global_mean_full = y_full.mean()\n\nprint('âœ… Modelo final treinado e pronto para prediÃ§Ãµes')\n\n# Salvar modelo e configuraÃ§Ãµes\nmodel_artifacts = {\n    'model': final_model,\n    'model_type': best_model_name,\n    'features': all_features,\n    'target': target,\n    'validation_mae': results_df.iloc[0]['MAE'],\n    'validation_rmse': results_df.iloc[0]['RMSE'],\n    'validation_r2': results_df.iloc[0]['RÂ²'],\n    'validation_wmape': results_df.iloc[0]['WMAPE'],\n    'training_date': pd.Timestamp.now(),\n    'combo_means': combo_means_full if best_model_name not in ['LightGBM', 'XGBoost', 'Random Forest'] else None,\n    'metadata': metadata\n}\n\n# Salvar artefatos do modelo\nwith open('../data/trained_model.pkl', 'wb') as f:\n    pickle.dump(model_artifacts, f)\n\nprint('ðŸ’¾ Modelo e artefatos salvos em: data/trained_model.pkl')\nprint('ðŸŽ¯ Pronto para gerar prediÃ§Ãµes para o perÃ­odo de teste!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo e PrÃ³ximos Passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('ðŸŽ‰ MODELAGEM CONCLUÃDA COM SUCESSO!')\nprint('=' * 60)\n\nprint(f'\\nðŸ† Melhor Modelo: {best_model_name}')\nprint(f'   â€¢ MAE: {results_df.iloc[0][\"MAE\"]:.4f}')\nprint(f'   â€¢ RMSE: {results_df.iloc[0][\"RMSE\"]:.4f}')\nprint(f'   â€¢ RÂ²: {results_df.iloc[0][\"RÂ²\"]:.4f}')\nprint(f'   â€¢ WMAPE: {results_df.iloc[0][\"WMAPE\"]:.2f}%')\n\nimprovement_over_baseline = (results_df[results_df['Model'] == 'MÃ©dia Simples']['MAE'].iloc[0] - results_df.iloc[0]['MAE']) / results_df[results_df['Model'] == 'MÃ©dia Simples']['MAE'].iloc[0] * 100\nprint(f'   â€¢ Melhoria sobre baseline: {improvement_over_baseline:.1f}%')\n\nprint(f'\\nðŸ“Š ComparaÃ§Ã£o de Modelos:')\nfor i, (_, row) in enumerate(results_df.iterrows(), 1):\n    print(f'   {i}. {row[\"Model\"]}: MAE = {row[\"MAE\"]:.4f}, WMAPE = {row[\"WMAPE\"]:.2f}%')\n\nprint(f'\\nðŸ’¾ Artefatos Salvos:')\nprint('   âœ… trained_model.pkl - Modelo treinado e configuraÃ§Ãµes')\nprint('   âœ… feature_engineering_metadata.pkl - Metadados do processamento')\nprint('   âœ… dados_features_completo.parquet - Dataset com features')\n\nprint(f'\\nðŸ”„ PrÃ³ximos Passos:')\nprint('   1. ðŸ“… Criar dados de teste para as 5 semanas de 2023')\nprint('   2. ðŸŽ¯ Gerar prediÃ§Ãµes usando o modelo treinado')\nprint('   3. ðŸ†• Aplicar estratÃ©gia para novas combinaÃ§Ãµes (prediÃ§Ã£o = 0)')\nprint('   4. ðŸ“‹ Criar arquivo de submissÃ£o no formato requerido')\nprint('   5. ðŸ§ª Validar prediÃ§Ãµes e fazer anÃ¡lise final')\n\nprint(f'\\nðŸš€ SISTEMA DE FORECASTING COMPLETO E PRONTO!')\nprint('   â€¢ Grid Inteligente com otimizaÃ§Ã£o de memÃ³ria')\nprint('   â€¢ Features avanÃ§adas (30+ variÃ¡veis)')\nprint('   â€¢ Modelo ML de alta performance')\nprint('   â€¢ ValidaÃ§Ã£o temporal robusta')\nprint('   â€¢ Pipeline completo e automatizado')\n\nprint('\\nâœ… Modelagem e Treinamento CONCLUÃDOS!')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}