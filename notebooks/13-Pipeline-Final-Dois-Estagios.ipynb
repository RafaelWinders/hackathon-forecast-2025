{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 13 - Pipeline Final de PrevisÃ£o com Dois EstÃ¡gios (VERSÃƒO LIMPA)\n",
    "\n",
    "Este notebook combina os dois modelos treinados para gerar as previsÃµes finais.\n",
    "\n",
    "## Pipeline de Dois EstÃ¡gios:\n",
    "1. **ClassificaÃ§Ã£o**: Prever SE vai vender (LGBMClassifier)\n",
    "2. **DecisÃ£o**: Aplicar threshold de probabilidade otimizado\n",
    "3. **RegressÃ£o**: Prever QUANTO vai vender apenas para casos positivos (LGBMRegressor)\n",
    "4. **CombinaÃ§Ã£o**: Juntar resultados para submissÃ£o final\n",
    "\n",
    "## Objetivo:\n",
    "- Gerar previsÃµes para as 5 semanas de Janeiro 2023\n",
    "- Criar arquivo de submissÃ£o no formato exigido\n",
    "- Aplicar otimizaÃ§Ãµes para WMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Iniciando Pipeline Final de Dois EstÃ¡gios - VERSÃƒO LIMPA\n",
      "ğŸ¯ Objetivo: Gerar previsÃµes para Janeiro 2023 combinando classificaÃ§Ã£o + regressÃ£o\n",
      "âš¡ VersÃ£o: Otimizada para performance e sem overflow\n",
      "ğŸ“ Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('ğŸš€ Iniciando Pipeline Final de Dois EstÃ¡gios - VERSÃƒO LIMPA')\n",
    "print('ğŸ¯ Objetivo: Gerar previsÃµes para Janeiro 2023 combinando classificaÃ§Ã£o + regressÃ£o')\n",
    "print('âš¡ VersÃ£o: Otimizada para performance e sem overflow')\n",
    "\n",
    "# Criar pasta submissao3 se nÃ£o existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('ğŸ“ Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Modelos Treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando modelos treinados...\n",
      "âœ… Modelos carregados:\n",
      "   ğŸ¯ Classificador: LGBMClassifier\n",
      "   ğŸ“Š Regressor: LGBMRegressor\n",
      "   ğŸ“‹ Features CLASSIFICAÃ‡ÃƒO: 41\n",
      "   ğŸ“‹ Features REGRESSÃƒO: 45\n",
      "\n",
      "ğŸ“Š Performance dos modelos:\n",
      "   ğŸ¯ Classificador - AUC: 1.0000\n",
      "   ğŸ“Š Regressor - RMSE: 13.1673\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelos treinados\n",
    "print('ğŸ“‚ Carregando modelos treinados...')\n",
    "\n",
    "# Classificador\n",
    "with open('../data/submissao3/lgbm_classifier.pkl', 'rb') as f:\n",
    "    lgbm_classifier = pickle.load(f)\n",
    "\n",
    "# Regressor\n",
    "with open('../data/submissao3/lgbm_regressor.pkl', 'rb') as f:\n",
    "    lgbm_regressor = pickle.load(f)\n",
    "\n",
    "# Features para classificaÃ§Ã£o\n",
    "with open('../data/submissao3/classification_features.pkl', 'rb') as f:\n",
    "    features_classificacao = pickle.load(f)\n",
    "\n",
    "# Features para regressÃ£o\n",
    "try:\n",
    "    with open('../data/submissao3/regression_features.pkl', 'rb') as f:\n",
    "        features_regressao = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('   ğŸ“ Arquivo regression_features.pkl nÃ£o encontrado - usando mesmo da classificaÃ§Ã£o')\n",
    "    features_regressao = features_classificacao\n",
    "\n",
    "# Metadados para verificaÃ§Ã£o\n",
    "with open('../data/submissao3/lgbm_classifier_metadata.pkl', 'rb') as f:\n",
    "    meta_classifier = pickle.load(f)\n",
    "\n",
    "with open('../data/submissao3/lgbm_regressor_metadata.pkl', 'rb') as f:\n",
    "    meta_regressor = pickle.load(f)\n",
    "\n",
    "print(f'âœ… Modelos carregados:')\n",
    "print(f'   ğŸ¯ Classificador: {type(lgbm_classifier).__name__}')\n",
    "print(f'   ğŸ“Š Regressor: {type(lgbm_regressor).__name__}')\n",
    "print(f'   ğŸ“‹ Features CLASSIFICAÃ‡ÃƒO: {len(features_classificacao)}')\n",
    "print(f'   ğŸ“‹ Features REGRESSÃƒO: {len(features_regressao)}')\n",
    "\n",
    "print(f'\\nğŸ“Š Performance dos modelos:')\n",
    "print(f'   ğŸ¯ Classificador - AUC: {meta_classifier[\"metricas_validacao\"][\"auc\"]:.4f}')\n",
    "print(f'   ğŸ“Š Regressor - RMSE: {meta_regressor[\"metricas_validacao\"][\"rmse\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. PreparaÃ§Ã£o dos Dados HistÃ³ricos e CombinaÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando dados histÃ³ricos...\n",
      "ğŸ“Š Dados histÃ³ricos de 2022: (55348430, 6)\n",
      "ğŸ“… Ãšltima semana de 2022: 2022-12-27 00:00:00\n",
      "ğŸ“… Semanas de teste (Janeiro 2023):\n",
      "      1. 2023-01-03\n",
      "      2. 2023-01-10\n",
      "      3. 2023-01-17\n",
      "      4. 2023-01-24\n",
      "      5. 2023-01-31\n",
      "ğŸ“Š CombinaÃ§Ãµes ativas: 1,044,310\n",
      "âœ… Dados histÃ³ricos preparados\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados histÃ³ricos\n",
    "print('ğŸ“‚ Carregando dados histÃ³ricos...')\n",
    "\n",
    "train_data = pd.read_parquet('../data/submissao3/train_data.parquet')\n",
    "validation_data = pd.read_parquet('../data/submissao3/validation_data.parquet')\n",
    "\n",
    "# Combinar todos os dados histÃ³ricos\n",
    "dados_completos_2022 = pd.concat([train_data, validation_data], ignore_index=True)\n",
    "print(f'ğŸ“Š Dados histÃ³ricos de 2022: {dados_completos_2022.shape}')\n",
    "\n",
    "# Definir semanas de Janeiro 2023\n",
    "ultima_semana_2022 = dados_completos_2022['semana'].max()\n",
    "print(f'ğŸ“… Ãšltima semana de 2022: {ultima_semana_2022}')\n",
    "\n",
    "semanas_teste = []\n",
    "for i in range(1, 6):\n",
    "    proxima_semana = ultima_semana_2022 + pd.Timedelta(weeks=i)\n",
    "    semanas_teste.append(proxima_semana)\n",
    "\n",
    "print(f'ğŸ“… Semanas de teste (Janeiro 2023):')\n",
    "for i, semana in enumerate(semanas_teste):\n",
    "    print(f'      {i+1}. {semana.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "# Identificar combinaÃ§Ãµes ativas\n",
    "combinacoes_ativas = dados_completos_2022[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "print(f'ğŸ“Š CombinaÃ§Ãµes ativas: {len(combinacoes_ativas):,}')\n",
    "\n",
    "print('âœ… Dados histÃ³ricos preparados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. CriaÃ§Ã£o do Grid de Teste com Features Otimizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando dados auxiliares...\n",
      "      ğŸ”§ MemÃ³ria: 4.0 MB â†’ 4.0 MB\n",
      "      ğŸ”§ MemÃ³ria: 3.0 MB â†’ 3.0 MB\n",
      "ğŸ“Š Produtos: (7092, 8) - Categorias: 7\n",
      "ğŸ“Š PDVs: (14419, 4) - Zipcodes: 788\n",
      "âœ… Dados auxiliares carregados\n"
     ]
    }
   ],
   "source": [
    "# FunÃ§Ã£o para otimizar tipos de dados SEM overflow\n",
    "def otimizar_dtypes_seguro(df):\n",
    "    \"\"\"Otimiza tipos de dados de forma segura, sem overflow.\"\"\"\n",
    "    memoria_antes = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            # Verificar se pode usar int32 sem overflow\n",
    "            if df[col].min() >= np.iinfo(np.int32).min and df[col].max() <= np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    memoria_depois = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f'      ğŸ”§ MemÃ³ria: {memoria_antes:.1f} MB â†’ {memoria_depois:.1f} MB')\n",
    "    return df\n",
    "\n",
    "# Carregar dados auxiliares\n",
    "print('ğŸ“‚ Carregando dados auxiliares...')\n",
    "\n",
    "produtos = pd.read_parquet(\n",
    "    '../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet'\n",
    ")\n",
    "produtos = produtos.rename(columns={'produto': 'produto_id'})\n",
    "produtos = otimizar_dtypes_seguro(produtos)\n",
    "\n",
    "pdvs = pd.read_parquet(\n",
    "    '../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
    ")\n",
    "pdvs = pdvs.rename(columns={'pdv': 'pdv_id', 'categoria_pdv': 'tipo_loja'})\n",
    "pdvs = otimizar_dtypes_seguro(pdvs)\n",
    "\n",
    "print(f'ğŸ“Š Produtos: {produtos.shape} - Categorias: {produtos[\"categoria\"].nunique()}')\n",
    "print(f'ğŸ“Š PDVs: {pdvs.shape} - Zipcodes: {pdvs[\"zipcode\"].nunique()}')\n",
    "\n",
    "print('âœ… Dados auxiliares carregados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Criando grid de teste para 5 semanas...\n",
      "   ğŸ“Š Otimizando dados de entrada...\n",
      "      ğŸ”§ MemÃ³ria: 9066.6 MB â†’ 9066.6 MB\n",
      "      ğŸ”§ MemÃ³ria: 159.1 MB â†’ 159.1 MB\n",
      "   ğŸ¯ Criando grid de teste...\n",
      "      ğŸ”§ MemÃ³ria: 994.8 MB â†’ 954.9 MB\n",
      "   ğŸ“Š Grid criado: (5221550, 6)\n",
      "   ğŸ” Filtrando histÃ³rico relevante...\n",
      "   ğŸ“Š HistÃ³rico filtrado: (8354480, 6)\n",
      "      ğŸ”§ MemÃ³ria: 2223.9 MB â†’ 2223.9 MB\n",
      "   ğŸ“Š Dados completos: (13576030, 6)\n",
      "   ğŸ”— Fazendo merges...\n",
      "      ğŸ”§ MemÃ³ria: 4088.1 MB â†’ 4036.3 MB\n",
      "   âš¡ Criando features...\n",
      "   ğŸ”§ Features de lag e rolling...\n",
      "   ğŸ“ˆ Features EWMA...\n",
      "   ğŸ“ˆ Features agregadas...\n",
      "   ğŸ’° Features de preÃ§o relativo...\n",
      "      ğŸ”§ MemÃ³ria: 5810.1 MB â†’ 5810.1 MB\n",
      "   âœ… Dados de teste finais: (5221550, 54)\n",
      "   ğŸ’¾ MemÃ³ria total: 2274.5 MB\n",
      "âœ… Grid de teste criado com features otimizadas!\n"
     ]
    }
   ],
   "source": [
    "def criar_grid_teste_com_features(semanas_teste, combinacoes, dados_historicos):\n",
    "    \"\"\"Cria grid de teste com features otimizadas para mÃ¡xima performance.\"\"\"\n",
    "    print(f'ğŸ”§ Criando grid de teste para {len(semanas_teste)} semanas...')\n",
    "    \n",
    "    # Otimizar dados de entrada\n",
    "    print('   ğŸ“Š Otimizando dados de entrada...')\n",
    "    dados_historicos = otimizar_dtypes_seguro(dados_historicos.copy())\n",
    "    combinacoes = otimizar_dtypes_seguro(combinacoes.copy())\n",
    "    \n",
    "    # Criar grid de teste\n",
    "    print('   ğŸ¯ Criando grid de teste...')\n",
    "    grid_parts = []\n",
    "    for semana in semanas_teste:\n",
    "        df_semana = combinacoes.copy()\n",
    "        df_semana['semana'] = semana\n",
    "        df_semana['quantidade'] = 0\n",
    "        df_semana['faturamento'] = 0.0\n",
    "        df_semana['distributor_id'] = None\n",
    "        grid_parts.append(df_semana)\n",
    "    \n",
    "    dados_teste_raw = pd.concat(grid_parts, ignore_index=True)\n",
    "    dados_teste_raw = otimizar_dtypes_seguro(dados_teste_raw)\n",
    "    print(f'   ğŸ“Š Grid criado: {dados_teste_raw.shape}')\n",
    "    \n",
    "    # Filtrar dados histÃ³ricos relevantes (Ãºltimas 8 semanas por combinaÃ§Ã£o)\n",
    "    print('   ğŸ” Filtrando histÃ³rico relevante...')\n",
    "    pdvs_relevantes = set(combinacoes['pdv_id'])\n",
    "    produtos_relevantes = set(combinacoes['produto_id'])\n",
    "    \n",
    "    mask_relevantes = (\n",
    "        dados_historicos['pdv_id'].isin(pdvs_relevantes) & \n",
    "        dados_historicos['produto_id'].isin(produtos_relevantes)\n",
    "    )\n",
    "    dados_historicos_filtrados = dados_historicos[mask_relevantes].copy()\n",
    "    \n",
    "    # Manter apenas Ãºltimas 8 semanas por combinaÃ§Ã£o (suficiente para lags e EWMA)\n",
    "    dados_historicos_filtrados = dados_historicos_filtrados.sort_values(['pdv_id', 'produto_id', 'semana'])\n",
    "    dados_historicos_filtrados = dados_historicos_filtrados.groupby(['pdv_id', 'produto_id']).tail(8)\n",
    "    print(f'   ğŸ“Š HistÃ³rico filtrado: {dados_historicos_filtrados.shape}')\n",
    "    \n",
    "    # Combinar histÃ³rico + teste\n",
    "    dados_completos = pd.concat([dados_historicos_filtrados, dados_teste_raw], ignore_index=True)\n",
    "    dados_completos = dados_completos.sort_values(['pdv_id', 'produto_id', 'semana']).reset_index(drop=True)\n",
    "    dados_completos = otimizar_dtypes_seguro(dados_completos)\n",
    "    print(f'   ğŸ“Š Dados completos: {dados_completos.shape}')\n",
    "    \n",
    "    # Merge com dados auxiliares\n",
    "    print('   ğŸ”— Fazendo merges...')\n",
    "    dados_completos = dados_completos.merge(produtos[['produto_id', 'categoria']], on='produto_id', how='left')\n",
    "    dados_completos = dados_completos.merge(pdvs[['pdv_id', 'zipcode', 'tipo_loja']], on='pdv_id', how='left')\n",
    "    dados_completos = otimizar_dtypes_seguro(dados_completos)\n",
    "    \n",
    "    # Preencher distributor_id\n",
    "    dados_completos['distributor_id'] = dados_completos.groupby('produto_id')['distributor_id'].transform('ffill').transform('bfill')\n",
    "    \n",
    "    # ========== CRIAR FEATURES COMPLETAS ==========\n",
    "    print('   âš¡ Criando features...')\n",
    "    \n",
    "    # 1. Features bÃ¡sicas\n",
    "    dados_completos['preco_unitario_atual'] = np.where(\n",
    "        dados_completos['quantidade'] > 0,\n",
    "        dados_completos['faturamento'] / dados_completos['quantidade'],\n",
    "        0\n",
    "    ).astype('float32')\n",
    "    \n",
    "    # 2. Features temporais COMPLETAS\n",
    "    dados_completos['mes'] = dados_completos['semana'].dt.month.astype('int8')\n",
    "    dados_completos['mes_sin'] = np.sin(2 * np.pi * dados_completos['mes'] / 12).astype('float32')\n",
    "    dados_completos['mes_cos'] = np.cos(2 * np.pi * dados_completos['mes'] / 12).astype('float32')\n",
    "    dados_completos['eh_inicio_mes'] = (dados_completos['semana'].dt.day <= 7).astype('int8')\n",
    "    dados_completos['eh_fim_mes'] = (dados_completos['semana'].dt.day >= 22).astype('int8')\n",
    "    dados_completos['dia_do_mes'] = dados_completos['semana'].dt.day.astype('int8')\n",
    "    dados_completos['semana_do_mes'] = ((dados_completos['semana'].dt.day - 1) // 7 + 1).astype('int8')\n",
    "\n",
    "    # Features temporais ADICIONAIS (que estavam faltando)\n",
    "    dados_completos['semana_do_ano'] = dados_completos['semana'].dt.isocalendar().week.astype('int8')\n",
    "    dados_completos['eh_primeira_semana_mes'] = (dados_completos['semana'].dt.day <= 7).astype('int8')\n",
    "    dados_completos['eh_dezembro'] = (dados_completos['mes'] == 12).astype('int8')\n",
    "    dados_completos['eh_janeiro'] = (dados_completos['mes'] == 1).astype('int8')\n",
    "    dados_completos['eh_pos_festas'] = ((dados_completos['mes'] == 1) |\n",
    "                                        ((dados_completos['mes'] == 12) & (dados_completos['dia_do_mes'] >= 26))).astype('int8')\n",
    "\n",
    "    # Features temporais circulares\n",
    "    dados_completos['semana_ano_sin'] = np.sin(2 * np.pi * dados_completos['semana_do_ano'] / 52).astype('float32')\n",
    "    dados_completos['semana_ano_cos'] = np.cos(2 * np.pi * dados_completos['semana_do_ano'] / 52).astype('float32')\n",
    "    \n",
    "    # 3. Features de lag (CRÃTICAS)\n",
    "    print('   ğŸ”§ Features de lag e rolling...')\n",
    "    gb = dados_completos.groupby(['pdv_id', 'produto_id'])\n",
    "    \n",
    "    # Lags de quantidade\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        dados_completos[f'quantidade_lag_{lag}'] = gb['quantidade'].shift(lag).astype('float32')\n",
    "        \n",
    "    # Lags de preÃ§o\n",
    "    dados_completos['preco_lag_1'] = gb['preco_unitario_atual'].shift(1).astype('float32')\n",
    "    dados_completos['preco_lag_2'] = gb['preco_unitario_atual'].shift(2).astype('float32')\n",
    "    dados_completos['variacao_preco_sku_semanal'] = (dados_completos['preco_lag_1'] - dados_completos['preco_lag_2']).fillna(0).astype('float32')\n",
    "    \n",
    "    # Rolling features (OTIMIZADAS)\n",
    "    rolling_gb = gb['quantidade'].rolling(window=4, min_periods=1)\n",
    "    dados_completos['quantidade_media_4w'] = rolling_gb.mean().reset_index(level=[0,1], drop=True).astype('float32')\n",
    "    dados_completos['quantidade_std_4w'] = rolling_gb.std().reset_index(level=[0,1], drop=True).fillna(0).astype('float32')\n",
    "    dados_completos['quantidade_max_4w'] = rolling_gb.max().reset_index(level=[0,1], drop=True).astype('float32')\n",
    "    \n",
    "    # EWMA features (CORRIGIDAS!)\n",
    "    print('   ğŸ“ˆ Features EWMA...')\n",
    "    alpha_4w = 2.0 / (4 + 1)  # EWMA para 4 semanas\n",
    "    alpha_8w = 2.0 / (8 + 1)  # EWMA para 8 semanas\n",
    "\n",
    "    dados_completos['quantidade_ewma_4w'] = gb['quantidade'].ewm(alpha=alpha_4w, adjust=False).mean().reset_index(level=[0,1], drop=True).astype('float32')\n",
    "    dados_completos['quantidade_ewma_8w'] = gb['quantidade'].ewm(alpha=alpha_8w, adjust=False).mean().reset_index(level=[0,1], drop=True).astype('float32')\n",
    "    dados_completos['preco_ewma_4w'] = gb['preco_unitario_atual'].ewm(alpha=alpha_4w, adjust=False).mean().reset_index(level=[0,1], drop=True).astype('float32')\n",
    "    \n",
    "    # 4. Features derivadas COMPLETAS\n",
    "    dados_completos['momentum_ratio'] = (dados_completos['quantidade_lag_1'] / dados_completos['quantidade_media_4w']).fillna(0).astype('float32')\n",
    "    dados_completos['momentum_ratio_ewma'] = (dados_completos['quantidade_lag_1'] / dados_completos['quantidade_ewma_4w']).fillna(0).astype('float32')\n",
    "    dados_completos['aceleracao'] = (dados_completos['quantidade_lag_1'] - dados_completos['quantidade_lag_2']).fillna(0).astype('float32')\n",
    "    \n",
    "    # 5. Features categÃ³ricas hash\n",
    "    dados_completos['pdv_hash'] = (dados_completos['pdv_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados_completos['produto_hash'] = (dados_completos['produto_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados_completos['categoria_hash'] = (dados_completos['categoria'].astype(str).apply(hash).abs() % 50).astype('int8')\n",
    "    dados_completos['zipcode_hash'] = (dados_completos['zipcode'].astype(str).apply(hash).abs() % 1000).astype('int16')\n",
    "    \n",
    "    dados_completos['pdv_produto_hash'] = (dados_completos['pdv_hash'] * 100 + dados_completos['produto_hash']).astype('int16')\n",
    "    dados_completos['categoria_zipcode_hash'] = (dados_completos['categoria_hash'].astype('int32') * 1000 + dados_completos['zipcode_hash']).astype('int32')\n",
    "    \n",
    "    # 6. Features agregadas COMPLETAS\n",
    "    print('   ğŸ“ˆ Features agregadas...')\n",
    "    dados_completos['preco_medio_semanal_sku_atual'] = dados_completos.groupby(['semana', 'produto_id'])['preco_unitario_atual'].transform('mean').astype('float32')\n",
    "    dados_completos['media_vendas_categoria_pdv_lag_1'] = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('mean').astype('float32')\n",
    "    \n",
    "    # Features de preÃ§o relativo (que estavam faltando!)\n",
    "    print('   ğŸ’° Features de preÃ§o relativo...')\n",
    "    preco_medio_categoria = dados_completos.groupby(['semana', 'categoria'])['preco_unitario_atual'].transform('mean')\n",
    "    preco_medio_pdv = dados_completos.groupby(['semana', 'pdv_id'])['preco_unitario_atual'].transform('mean')\n",
    "    \n",
    "    dados_completos['preco_relativo_categoria'] = (dados_completos['preco_unitario_atual'] / preco_medio_categoria).fillna(1.0).astype('float32')\n",
    "    dados_completos['preco_relativo_pdv'] = (dados_completos['preco_unitario_atual'] / preco_medio_pdv).fillna(1.0).astype('float32')\n",
    "    \n",
    "    # Volatilidade de preÃ§o (que estava faltando!)\n",
    "    preco_rolling_std = gb['preco_unitario_atual'].rolling(window=4, min_periods=1).std()\n",
    "    dados_completos['preco_volatilidade'] = preco_rolling_std.reset_index(level=[0,1], drop=True).fillna(0).astype('float32')\n",
    "    \n",
    "    # Share de vendas (para classificaÃ§Ã£o - com lag)\n",
    "    vendas_categoria_lag = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('sum')\n",
    "    dados_completos['share_vendas_sku_categoria_lag_1'] = (dados_completos['quantidade_lag_1'] / vendas_categoria_lag).fillna(0).astype('float32')\n",
    "    \n",
    "    # Share de vendas (para regressÃ£o - atual)\n",
    "    dados_completos['media_vendas_categoria_pdv_atual'] = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('mean').astype('float32')\n",
    "    vendas_categoria_atual = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('sum')\n",
    "    dados_completos['share_vendas_sku_categoria_atual'] = (dados_completos['quantidade'] / vendas_categoria_atual).fillna(0).astype('float32')\n",
    "    \n",
    "    # Limpeza final\n",
    "    dados_completos.fillna(0, inplace=True)\n",
    "    dados_completos = otimizar_dtypes_seguro(dados_completos)\n",
    "    \n",
    "    # Filtrar apenas semanas de teste\n",
    "    dados_teste_final = dados_completos[dados_completos['semana'].isin(semanas_teste)].copy()\n",
    "    \n",
    "    print(f'   âœ… Dados de teste finais: {dados_teste_final.shape}')\n",
    "    print(f'   ğŸ’¾ MemÃ³ria total: {dados_teste_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB')\n",
    "    \n",
    "    # Liberar memÃ³ria\n",
    "    del dados_completos, dados_historicos_filtrados, dados_teste_raw, grid_parts\n",
    "    gc.collect()\n",
    "    \n",
    "    return dados_teste_final\n",
    "\n",
    "# EXECUTAR CRIAÃ‡ÃƒO DO GRID\n",
    "dados_teste = criar_grid_teste_com_features(semanas_teste, combinacoes_ativas, dados_completos_2022)\n",
    "\n",
    "print('âœ… Grid de teste criado com features otimizadas!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. AplicaÃ§Ã£o do Pipeline de Dois EstÃ¡gios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ESTÃGIO 1: CLASSIFICAÃ‡ÃƒO (prever SE vai vender)\n",
      "   ğŸ”§ Aplicando mapeamentos categÃ³ricos...\n",
      "      âœ… categoria: mapeamento aplicado com sucesso\n",
      "      âœ… zipcode: mapeamento aplicado com sucesso\n",
      "      âš ï¸ tipo_loja: 58730 valores desconhecidos mapeados para -1\n",
      "   âœ… Mapeamentos categÃ³ricos aplicados com sucesso!\n",
      "   ğŸ”§ Preparando features para classificaÃ§Ã£o...\n",
      "   âœ… Features classificaÃ§Ã£o: (5221550, 41)\n",
      "   ğŸ” Verificando consistÃªncia de tipos...\n",
      "      preco_lag_1: float32\n",
      "      preco_lag_2: float32\n",
      "      variacao_preco_sku_semanal: float32\n",
      "      quantidade_lag_1: float32\n",
      "      quantidade_lag_2: float32\n",
      "   âš¡ Executando classificaÃ§Ã£o...\n",
      "   ğŸ“Š Probabilidades: mÃ©dia=0.0113, mediana=0.0007\n",
      "\n",
      "ğŸ¯ ESTÃGIO 2: THRESHOLD OTIMIZADO PARA WMAPE\n",
      "   ğŸ† Usando threshold OTIMIZADO para WMAPE: 0.290\n",
      "   ğŸ“Š WMAPE esperado: 110.34%\n",
      "   ğŸ“Š Casos positivos: 42,032 (0.8%)\n",
      "   ğŸ“Š AnÃ¡lise de thresholds (referÃªncia):\n",
      "      Threshold 0.1: 201,582 positivos (  3.9%) \n",
      "      Threshold 0.2:  71,703 positivos (  1.4%) \n",
      "      Threshold 0.3:  38,968 positivos (  0.7%) â† ESCOLHIDO\n",
      "      Threshold 0.4:  22,260 positivos (  0.4%) \n",
      "      Threshold 0.5:  11,567 positivos (  0.2%) \n",
      "âœ… EstÃ¡gios 1 e 2 concluÃ­dos com threshold otimizado!\n"
     ]
    }
   ],
   "source": [
    "# ESTÃGIO 1: CLASSIFICAÃ‡ÃƒO\n",
    "print('ğŸ¯ ESTÃGIO 1: CLASSIFICAÃ‡ÃƒO (prever SE vai vender)')\n",
    "\n",
    "# CORREÃ‡ÃƒO CRÃTICA: Aplicar mapeamentos categÃ³ricos\n",
    "print('   ğŸ”§ Aplicando mapeamentos categÃ³ricos...')\n",
    "\n",
    "try:\n",
    "    # Carregar os mapeamentos salvos no treinamento\n",
    "    with open('../data/submissao3/categorical_mappings.pkl', 'rb') as f:\n",
    "        categorical_mappings = pickle.load(f)\n",
    "    \n",
    "    # Fazer uma cÃ³pia para aplicar os mapeamentos\n",
    "    dados_teste_mapeados = dados_teste.copy()\n",
    "    \n",
    "    # Aplicar EXATAMENTE os mesmos mapeamentos usados no treino\n",
    "    for col, mapping in categorical_mappings.items():\n",
    "        if col in dados_teste_mapeados.columns:\n",
    "            # Mapear valores conhecidos, valores desconhecidos recebem -1\n",
    "            dados_teste_mapeados[col] = dados_teste_mapeados[col].map(mapping).fillna(-1).astype('int32')\n",
    "            valores_desconhecidos = (dados_teste_mapeados[col] == -1).sum()\n",
    "            if valores_desconhecidos > 0:\n",
    "                print(f'      âš ï¸ {col}: {valores_desconhecidos} valores desconhecidos mapeados para -1')\n",
    "            else:\n",
    "                print(f'      âœ… {col}: mapeamento aplicado com sucesso')\n",
    "    \n",
    "    print('   âœ… Mapeamentos categÃ³ricos aplicados com sucesso!')\n",
    "    \n",
    "    # Usar os dados mapeados para as previsÃµes\n",
    "    dados_para_predicao = dados_teste_mapeados\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print('   âš ï¸ Arquivo categorical_mappings.pkl nÃ£o encontrado')\n",
    "    print('   ğŸ“ Usando dados originais - pode causar erro se modelos foram treinados com mappings')\n",
    "    print('   ğŸ’¡ RecomendaÃ§Ã£o: Execute o notebook 11b primeiro para gerar os mappings')\n",
    "    dados_para_predicao = dados_teste\n",
    "\n",
    "# Preparar features para classificaÃ§Ã£o\n",
    "print('   ğŸ”§ Preparando features para classificaÃ§Ã£o...')\n",
    "\n",
    "# Verificar features faltando ANTES de tentar acessar\n",
    "features_faltando = set(features_classificacao) - set(dados_para_predicao.columns)\n",
    "if features_faltando:\n",
    "    print(f'   âš ï¸ Features faltando: {len(features_faltando)} - adicionando com zeros')\n",
    "    print(f'      Features: {list(features_faltando)}')\n",
    "    for feat in features_faltando:\n",
    "        dados_para_predicao[feat] = 0.0\n",
    "\n",
    "# Agora criar o DataFrame com as features corretas\n",
    "X_teste_classificacao = dados_para_predicao[features_classificacao].copy().fillna(0)\n",
    "\n",
    "# Reordenar para match com modelo\n",
    "X_teste_classificacao = X_teste_classificacao[features_classificacao]\n",
    "print(f'   âœ… Features classificaÃ§Ã£o: {X_teste_classificacao.shape}')\n",
    "\n",
    "# Verificar tipos de dados sÃ£o consistentes\n",
    "print('   ğŸ” Verificando consistÃªncia de tipos...')\n",
    "for i, col in enumerate(features_classificacao[:5]):  # Mostrar apenas 5 para nÃ£o poluir\n",
    "    dtype_treino = X_teste_classificacao[col].dtype\n",
    "    print(f'      {col}: {dtype_treino}')\n",
    "\n",
    "# Aplicar classificaÃ§Ã£o\n",
    "print('   âš¡ Executando classificaÃ§Ã£o...')\n",
    "probabilidades_venda = lgbm_classifier.predict_proba(X_teste_classificacao)[:, 1]\n",
    "print(f'   ğŸ“Š Probabilidades: mÃ©dia={probabilidades_venda.mean():.4f}, mediana={np.median(probabilidades_venda):.4f}')\n",
    "\n",
    "# ESTÃGIO 2: THRESHOLD OTIMIZADO PARA WMAPE\n",
    "print('\\nğŸ¯ ESTÃGIO 2: THRESHOLD OTIMIZADO PARA WMAPE')\n",
    "\n",
    "# USAR THRESHOLD OTIMIZADO ENCONTRADO NO NOTEBOOK 11b\n",
    "try:\n",
    "    # Tentar carregar threshold otimizado do notebook 11b\n",
    "    with open('../data/submissao3/threshold_otimo_wmape.pkl', 'rb') as f:\n",
    "        threshold_data = pickle.load(f)\n",
    "    best_threshold = threshold_data['threshold']\n",
    "    print(f'   ğŸ† Usando threshold OTIMIZADO para WMAPE: {best_threshold:.3f}')\n",
    "    print(f'   ğŸ“Š WMAPE esperado: {threshold_data[\"wmape\"]:.2f}%')\n",
    "except FileNotFoundError:\n",
    "    # Fallback: usar threshold baseado na experiÃªncia da otimizaÃ§Ã£o\n",
    "    best_threshold = 0.15  # Valor tÃ­pico encontrado nas otimizaÃ§Ãµes\n",
    "    print(f'   ğŸ¯ Usando threshold BASEADO EM OTIMIZAÃ‡ÃƒO: {best_threshold:.3f}')\n",
    "    print(f'   ğŸ’¡ Execute o notebook 11b para obter o threshold Ã³timo especÃ­fico')\n",
    "\n",
    "# Aplicar threshold otimizado\n",
    "vai_vender = (probabilidades_venda >= best_threshold)\n",
    "casos_positivos = vai_vender.sum()\n",
    "\n",
    "print(f'   ğŸ“Š Casos positivos: {casos_positivos:,} ({casos_positivos/len(vai_vender)*100:.1f}%)')\n",
    "\n",
    "# AnÃ¡lise de thresholds para referÃªncia\n",
    "thresholds_ref = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "print('   ğŸ“Š AnÃ¡lise de thresholds (referÃªncia):')\n",
    "for t in thresholds_ref:\n",
    "    pos = (probabilidades_venda >= t).sum()\n",
    "    pct = pos / len(probabilidades_venda) * 100\n",
    "    status = \"â† ESCOLHIDO\" if abs(t - best_threshold) < 0.01 else \"\"\n",
    "    print(f'      Threshold {t:.1f}: {pos:7,} positivos ({pct:5.1f}%) {status}')\n",
    "\n",
    "print('âœ… EstÃ¡gios 1 e 2 concluÃ­dos com threshold otimizado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ESTÃGIO 3: REGRESSÃƒO (prever QUANTO vai vender)\n",
      "   ğŸ“Š Aplicando regressÃ£o a 42,032 casos positivos...\n",
      "   ğŸ“Š Features para regressÃ£o: (42032, 45)\n",
      "   ğŸ” Verificando consistÃªncia de tipos (regressÃ£o)...\n",
      "      preco_lag_1: float32\n",
      "      preco_lag_2: float32\n",
      "      variacao_preco_sku_semanal: float32\n",
      "   âš¡ Executando regressÃ£o...\n",
      "   ğŸ“Š PrevisÃµes: mÃ©dia=7.45, mediana=7\n",
      "   ğŸ“Š Min: 6, Max: 19\n",
      "\n",
      "ğŸ‰ PIPELINE DE DOIS ESTÃGIOS CONCLUÃDO!\n",
      "============================================================\n",
      "ğŸ“Š Total de previsÃµes: 5,221,550\n",
      "ğŸ“Š PrevisÃµes zero: 5,179,518 (99.2%)\n",
      "ğŸ“Š PrevisÃµes positivas: 42,032 (0.8%)\n",
      "ğŸ“Š Soma total prevista: 313,030.0\n",
      "\n",
      "ğŸ“Š VALIDAÃ‡ÃƒO:\n",
      "   ğŸ“ˆ Taxa histÃ³rica de vendas: 11.0%\n",
      "   ğŸ“ˆ Taxa prevista de vendas: 0.8%\n",
      "   ğŸ“ˆ Ratio previsto/histÃ³rico: 0.07\n",
      "   âš ï¸ ALERTA: Taxa de previsÃµes positivas muito baixa!\n",
      "\n",
      "âœ… Pipeline aplicado com sucesso (COM MAPEAMENTOS CATEGÃ“RICOS)!\n"
     ]
    }
   ],
   "source": [
    "# ESTÃGIO 3: REGRESSÃƒO\n",
    "print('ğŸ“Š ESTÃGIO 3: REGRESSÃƒO (prever QUANTO vai vender)')\n",
    "\n",
    "previsoes_finais = np.zeros(len(dados_teste))\n",
    "\n",
    "if casos_positivos > 0:\n",
    "    print(f'   ğŸ“Š Aplicando regressÃ£o a {casos_positivos:,} casos positivos...')\n",
    "    \n",
    "    # CORREÃ‡ÃƒO CRÃTICA: Usar os dados com mapeamentos categÃ³ricos aplicados\n",
    "    # (dados_para_predicao jÃ¡ foi criado na cÃ©lula anterior com os mappings)\n",
    "    \n",
    "    # Verificar features faltando para regressÃ£o ANTES de tentar acessar\n",
    "    features_faltando_reg = set(features_regressao) - set(dados_para_predicao.columns)\n",
    "    if features_faltando_reg:\n",
    "        print(f'   âš ï¸ Features faltando na regressÃ£o: {len(features_faltando_reg)}')\n",
    "        print(f'      Features: {list(features_faltando_reg)}')\n",
    "        for feat in features_faltando_reg:\n",
    "            dados_para_predicao[feat] = 0.0\n",
    "    \n",
    "    # Preparar features para regressÃ£o USANDO OS DADOS COM MAPPINGS\n",
    "    X_teste_regressao = dados_para_predicao[vai_vender][features_regressao].copy().fillna(0)\n",
    "    X_teste_regressao = X_teste_regressao[features_regressao]\n",
    "    print(f'   ğŸ“Š Features para regressÃ£o: {X_teste_regressao.shape}')\n",
    "    \n",
    "    # Verificar consistÃªncia de tipos para regressÃ£o\n",
    "    print('   ğŸ” Verificando consistÃªncia de tipos (regressÃ£o)...')\n",
    "    for i, col in enumerate(features_regressao[:3]):  # Mostrar apenas 3\n",
    "        dtype_reg = X_teste_regressao[col].dtype\n",
    "        print(f'      {col}: {dtype_reg}')\n",
    "    \n",
    "    # Aplicar regressÃ£o\n",
    "    print('   âš¡ Executando regressÃ£o...')\n",
    "    previsoes_quantidade_raw = lgbm_regressor.predict(X_teste_regressao)\n",
    "    \n",
    "    # PÃ³s-processamento otimizado para WMAPE\n",
    "    previsoes_quantidade = np.maximum(previsoes_quantidade_raw, 0)\n",
    "    \n",
    "    # Ceiling inteligente baseado no histÃ³rico\n",
    "    p95_historico = np.percentile(dados_completos_2022[dados_completos_2022['quantidade'] > 0]['quantidade'], 95)\n",
    "    previsoes_quantidade = np.where(\n",
    "        previsoes_quantidade > p95_historico,\n",
    "        p95_historico + (previsoes_quantidade - p95_historico) * 0.3,\n",
    "        previsoes_quantidade\n",
    "    )\n",
    "    \n",
    "    # Arredondamento inteligente\n",
    "    previsoes_quantidade_final = np.where(\n",
    "        previsoes_quantidade < 1.5,\n",
    "        np.maximum(np.round(previsoes_quantidade), 1),  # Pelo menos 1 para casos positivos\n",
    "        np.round(previsoes_quantidade)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Inserir previsÃµes nos casos positivos\n",
    "    previsoes_finais[vai_vender] = previsoes_quantidade_final\n",
    "    \n",
    "    print(f'   ğŸ“Š PrevisÃµes: mÃ©dia={previsoes_quantidade_final.mean():.2f}, mediana={np.median(previsoes_quantidade_final):.0f}')\n",
    "    print(f'   ğŸ“Š Min: {previsoes_quantidade_final.min()}, Max: {previsoes_quantidade_final.max()}')\n",
    "\n",
    "else:\n",
    "    print('   âš ï¸ Nenhum caso positivo detectado - todas as previsÃµes serÃ£o zero!')\n",
    "\n",
    "# RESULTADO FINAL\n",
    "print('\\nğŸ‰ PIPELINE DE DOIS ESTÃGIOS CONCLUÃDO!')\n",
    "print('=' * 60)\n",
    "print(f'ğŸ“Š Total de previsÃµes: {len(previsoes_finais):,}')\n",
    "print(f'ğŸ“Š PrevisÃµes zero: {(previsoes_finais == 0).sum():,} ({(previsoes_finais == 0).mean()*100:.1f}%)')\n",
    "print(f'ğŸ“Š PrevisÃµes positivas: {(previsoes_finais > 0).sum():,} ({(previsoes_finais > 0).mean()*100:.1f}%)')\n",
    "print(f'ğŸ“Š Soma total prevista: {previsoes_finais.sum():,}')\n",
    "\n",
    "# Adicionar resultados ao dataframe (usando dados originais para preservar categorias legÃ­veis)\n",
    "dados_teste['quantidade_prevista'] = previsoes_finais\n",
    "dados_teste['probabilidade_venda'] = probabilidades_venda\n",
    "dados_teste['vai_vender'] = vai_vender\n",
    "\n",
    "# ValidaÃ§Ã£o dos resultados\n",
    "taxa_prevista = (previsoes_finais > 0).mean()\n",
    "taxa_historica_validacao = (dados_completos_2022['quantidade'] > 0).mean()\n",
    "\n",
    "print(f'\\nğŸ“Š VALIDAÃ‡ÃƒO:')\n",
    "print(f'   ğŸ“ˆ Taxa histÃ³rica de vendas: {taxa_historica_validacao:.1%}')\n",
    "print(f'   ğŸ“ˆ Taxa prevista de vendas: {taxa_prevista:.1%}')\n",
    "print(f'   ğŸ“ˆ Ratio previsto/histÃ³rico: {taxa_prevista/taxa_historica_validacao:.2f}')\n",
    "\n",
    "if taxa_prevista < 0.01:\n",
    "    print('   âš ï¸ ALERTA: Taxa de previsÃµes positivas muito baixa!')\n",
    "elif taxa_prevista > 0.20:\n",
    "    print('   âš ï¸ ALERTA: Taxa de previsÃµes positivas muito alta!')\n",
    "else:\n",
    "    print('   âœ… Taxa de previsÃµes positivas dentro do esperado')\n",
    "\n",
    "print('\\nâœ… Pipeline aplicado com sucesso (COM MAPEAMENTOS CATEGÃ“RICOS)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. GeraÃ§Ã£o do Arquivo de SubmissÃ£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Gerando arquivo de submissÃ£o...\n",
      "\n",
      "   ğŸ” VerificaÃ§Ãµes de consistÃªncia:\n",
      "      â€¢ Shape: (5221550, 4)\n",
      "      â€¢ Colunas: ['semana', 'pdv_id', 'produto_id', 'quantity']\n",
      "      â€¢ PerÃ­odo: 2023-01-03 00:00:00 atÃ© 2023-01-31 00:00:00\n",
      "      â€¢ Semanas Ãºnicas: 5 (esperado: 5)\n",
      "      â€¢ Valores negativos: 0 (deve ser 0)\n",
      "      â€¢ Valores NaN: 0 (deve ser 0)\n",
      "      â€¢ Duplicatas: 0 (deve ser 0)\n",
      "      â€¢ Total registros: 5,221,550 (esperado: 5,221,550)\n",
      "      âœ… Total de registros correto!\n",
      "\n",
      "   ğŸ“Š EstatÃ­sticas da submissÃ£o:\n",
      "      â€¢ PrevisÃµes zero: 5,179,518 (99.2%)\n",
      "      â€¢ PrevisÃµes positivas: 42,032 (0.8%)\n",
      "      â€¢ Soma total: 313,030.0\n",
      "      â€¢ Quantidade mÃ¡xima: 19.0\n",
      "      â€¢ Quantidade mÃ©dia (apenas >0): 7.45\n",
      "\n",
      "   âœ… Arquivo principal salvo: ../data/submissao3/submission_final_two_stage_LIMPO.parquet\n",
      "   âœ… Arquivo CSV salvo: ../data/submissao3/submission_final_two_stage_LIMPO.csv\n",
      "   âœ… Arquivo detalhado salvo: ../data/submissao3/submission_detailed_LIMPO.parquet\n",
      "\n",
      "   ğŸ“… ANÃLISE POR SEMANA:\n",
      "      Semana 1 (2023-01-03): Total= 150013 | Positivos=20,375 ( 2.0%)\n",
      "      Semana 2 (2023-01-10): Total= 128575 | Positivos=17,321 ( 1.7%)\n",
      "      Semana 3 (2023-01-17): Total=  34442 | Positivos= 4,336 ( 0.4%)\n",
      "      Semana 4 (2023-01-24): Total=      0 | Positivos=     0 ( 0.0%)\n",
      "      Semana 5 (2023-01-31): Total=      0 | Positivos=     0 ( 0.0%)\n",
      "\n",
      "âœ… ARQUIVOS DE SUBMISSÃƒO GERADOS!\n",
      "ğŸ“„ Arquivo principal: submission_final_two_stage_LIMPO.parquet\n",
      "ğŸ¯ Pronto para submissÃ£o ao desafio!\n"
     ]
    }
   ],
   "source": [
    "# Gerar arquivo de submissÃ£o no formato exigido\n",
    "print('ğŸ“„ Gerando arquivo de submissÃ£o...')\n",
    "\n",
    "# Preparar dados para submissÃ£o\n",
    "submissao = dados_teste[['semana', 'pdv_id', 'produto_id', 'quantidade_prevista']].copy()\n",
    "submissao = submissao.rename(columns={'quantidade_prevista': 'quantity'})\n",
    "\n",
    "# VerificaÃ§Ãµes de consistÃªncia\n",
    "print(f'\\n   ğŸ” VerificaÃ§Ãµes de consistÃªncia:')\n",
    "print(f'      â€¢ Shape: {submissao.shape}')\n",
    "print(f'      â€¢ Colunas: {list(submissao.columns)}')\n",
    "print(f'      â€¢ PerÃ­odo: {submissao[\"semana\"].min()} atÃ© {submissao[\"semana\"].max()}')\n",
    "print(f'      â€¢ Semanas Ãºnicas: {submissao[\"semana\"].nunique()} (esperado: 5)')\n",
    "print(f'      â€¢ Valores negativos: {(submissao[\"quantity\"] < 0).sum()} (deve ser 0)')\n",
    "print(f'      â€¢ Valores NaN: {submissao[\"quantity\"].isna().sum()} (deve ser 0)')\n",
    "print(f'      â€¢ Duplicatas: {submissao.duplicated(subset=[\"semana\", \"pdv_id\", \"produto_id\"]).sum()} (deve ser 0)')\n",
    "\n",
    "# Validar total esperado\n",
    "total_esperado = len(combinacoes_ativas) * len(semanas_teste)\n",
    "print(f'      â€¢ Total registros: {len(submissao):,} (esperado: {total_esperado:,})')\n",
    "\n",
    "if len(submissao) == total_esperado:\n",
    "    print(f'      âœ… Total de registros correto!')\n",
    "else:\n",
    "    print(f'      âš ï¸ DiscrepÃ¢ncia no total de registros!')\n",
    "\n",
    "# EstatÃ­sticas da submissÃ£o\n",
    "print(f'\\n   ğŸ“Š EstatÃ­sticas da submissÃ£o:')\n",
    "print(f'      â€¢ PrevisÃµes zero: {(submissao[\"quantity\"] == 0).sum():,} ({(submissao[\"quantity\"] == 0).mean()*100:.1f}%)')\n",
    "print(f'      â€¢ PrevisÃµes positivas: {(submissao[\"quantity\"] > 0).sum():,} ({(submissao[\"quantity\"] > 0).mean()*100:.1f}%)')\n",
    "print(f'      â€¢ Soma total: {submissao[\"quantity\"].sum():,}')\n",
    "print(f'      â€¢ Quantidade mÃ¡xima: {submissao[\"quantity\"].max()}')\n",
    "if (submissao[\"quantity\"] > 0).sum() > 0:\n",
    "    print(f'      â€¢ Quantidade mÃ©dia (apenas >0): {submissao[submissao[\"quantity\"] > 0][\"quantity\"].mean():.2f}')\n",
    "\n",
    "# Salvar arquivos\n",
    "nome_arquivo_principal = '../data/submissao3/submission_final_two_stage_LIMPO.parquet'\n",
    "submissao.to_parquet(nome_arquivo_principal, index=False)\n",
    "print(f'\\n   âœ… Arquivo principal salvo: {nome_arquivo_principal}')\n",
    "\n",
    "nome_arquivo_csv = '../data/submissao3/submission_final_two_stage_LIMPO.csv'\n",
    "submissao.to_csv(nome_arquivo_csv, index=False)\n",
    "print(f'   âœ… Arquivo CSV salvo: {nome_arquivo_csv}')\n",
    "\n",
    "# Salvar dados detalhados para anÃ¡lise\n",
    "dados_detalhados = dados_teste[[\n",
    "    'semana', 'pdv_id', 'produto_id', 'quantidade_prevista', \n",
    "    'probabilidade_venda', 'vai_vender'\n",
    "]].copy()\n",
    "\n",
    "nome_arquivo_detalhado = '../data/submissao3/submission_detailed_LIMPO.parquet'\n",
    "dados_detalhados.to_parquet(nome_arquivo_detalhado, index=False)\n",
    "print(f'   âœ… Arquivo detalhado salvo: {nome_arquivo_detalhado}')\n",
    "\n",
    "# AnÃ¡lise por semana\n",
    "print(f'\\n   ğŸ“… ANÃLISE POR SEMANA:')\n",
    "for i, semana in enumerate(semanas_teste):\n",
    "    dados_semana = submissao[submissao['semana'] == semana]\n",
    "    total = dados_semana['quantity'].sum()\n",
    "    positivos = (dados_semana['quantity'] > 0).sum()\n",
    "    taxa = positivos / len(dados_semana) * 100\n",
    "    print(f'      Semana {i+1} ({semana.strftime(\"%Y-%m-%d\")}): Total={total:7.0f} | Positivos={positivos:6,} ({taxa:4.1f}%)')\n",
    "\n",
    "print(f'\\nâœ… ARQUIVOS DE SUBMISSÃƒO GERADOS!')\n",
    "print(f'ğŸ“„ Arquivo principal: submission_final_two_stage_LIMPO.parquet')\n",
    "print(f'ğŸ¯ Pronto para submissÃ£o ao desafio!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Salvamento de Metadados Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Salvando metadados do pipeline...\n",
      "ğŸ“‹ Metadados salvos em: pipeline_final_LIMPO_metadata.pkl\n",
      "\n",
      "ğŸ‰ PIPELINE FINAL LIMPO CONCLUÃDO COM SUCESSO!\n",
      "================================================================================\n",
      "ğŸ¯ RESUMO FINAL:\n",
      "   ğŸ“Š EstratÃ©gia: Pipeline de Dois EstÃ¡gios - ClassificaÃ§Ã£o + RegressÃ£o\n",
      "   ğŸ¯ Threshold: 0.29000000000000004\n",
      "   ğŸ“Š Total previsÃµes: 5,221,550\n",
      "   ğŸ“Š Taxa positivos: 0.8%\n",
      "   ğŸ“Š Soma total: 313,030.0\n",
      "\n",
      "ğŸ“„ ARQUIVO PRINCIPAL:\n",
      "   âœ… submission_final_two_stage_LIMPO.parquet\n",
      "\n",
      "ğŸš€ PRONTO PARA SUBMISSÃƒO AO DESAFIO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Salvar metadados completos do pipeline\n",
    "print('ğŸ“‹ Salvando metadados do pipeline...')\n",
    "\n",
    "# Calcular estatÃ­sticas detalhadas\n",
    "previsoes_positivas_stats = dados_teste[dados_teste['quantidade_prevista'] > 0]['quantidade_prevista']\n",
    "vendas_historicas = dados_completos_2022[dados_completos_2022['quantidade'] > 0]['quantidade']\n",
    "\n",
    "metadados_pipeline = {\n",
    "    'data_criacao': pd.Timestamp.now(),\n",
    "    'versao': 'Pipeline Final Limpo v1.0',\n",
    "    'estrategia': 'Pipeline de Dois EstÃ¡gios - ClassificaÃ§Ã£o + RegressÃ£o',\n",
    "    \n",
    "    # Modelos\n",
    "    'modelos': {\n",
    "        'classificador': 'LightGBM Classifier',\n",
    "        'regressor': 'LightGBM Regressor',\n",
    "        'features_classificacao': len(features_classificacao),\n",
    "        'features_regressao': len(features_regressao)\n",
    "    },\n",
    "    \n",
    "    # Performance\n",
    "    'performance_modelos': {\n",
    "        'classificador_auc': float(meta_classifier['metricas_validacao']['auc']),\n",
    "        'regressor_rmse': float(meta_regressor['metricas_validacao']['rmse'])\n",
    "    },\n",
    "    \n",
    "    # ConfiguraÃ§Ã£o do pipeline\n",
    "    'configuracao': {\n",
    "        'threshold_classificacao': float(best_threshold),\n",
    "        'total_combinacoes': len(combinacoes_ativas),\n",
    "        'semanas_previsao': len(semanas_teste),\n",
    "        'periodo_inicio': semanas_teste[0].strftime('%Y-%m-%d'),\n",
    "        'periodo_fim': semanas_teste[-1].strftime('%Y-%m-%d')\n",
    "    },\n",
    "    \n",
    "    # Resultados\n",
    "    'resultados': {\n",
    "        'total_previsoes': len(previsoes_finais),\n",
    "        'previsoes_zero': int((previsoes_finais == 0).sum()),\n",
    "        'previsoes_positivas': int((previsoes_finais > 0).sum()),\n",
    "        'taxa_previsoes_positivas': float((previsoes_finais > 0).mean()),\n",
    "        'quantidade_total_prevista': int(previsoes_finais.sum()),\n",
    "        'quantidade_media_prevista': float(previsoes_finais.mean()),\n",
    "        'probabilidade_media': float(probabilidades_venda.mean())\n",
    "    },\n",
    "    \n",
    "    # EstatÃ­sticas detalhadas\n",
    "    'estatisticas_previsoes_positivas': {\n",
    "        'count': len(previsoes_positivas_stats),\n",
    "        'mean': float(previsoes_positivas_stats.mean()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'median': float(previsoes_positivas_stats.median()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'std': float(previsoes_positivas_stats.std()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'min': float(previsoes_positivas_stats.min()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'max': float(previsoes_positivas_stats.max()) if len(previsoes_positivas_stats) > 0 else 0\n",
    "    },\n",
    "    \n",
    "    # ComparaÃ§Ã£o histÃ³rica\n",
    "    'comparacao_historica': {\n",
    "        'taxa_historica_vendas': float(len(vendas_historicas) / len(dados_completos_2022)),\n",
    "        'media_historica_vendas': float(vendas_historicas.mean()),\n",
    "        'total_historico': float(dados_completos_2022['quantidade'].sum()),\n",
    "        'ratio_total_previsto_historico': float(previsoes_finais.sum() / dados_completos_2022['quantidade'].sum()),\n",
    "        'ratio_taxa_prevista_historica': float((previsoes_finais > 0).mean() / (dados_completos_2022['quantidade'] > 0).mean())\n",
    "    },\n",
    "    \n",
    "    # Arquivos gerados\n",
    "    'arquivos_gerados': [\n",
    "        'submission_final_two_stage_LIMPO.parquet',\n",
    "        'submission_final_two_stage_LIMPO.csv',\n",
    "        'submission_detailed_LIMPO.parquet',\n",
    "        'pipeline_final_LIMPO_metadata.pkl'\n",
    "    ],\n",
    "    \n",
    "    # OtimizaÃ§Ãµes aplicadas\n",
    "    'otimizacoes': [\n",
    "        'OtimizaÃ§Ã£o de tipos de dados sem overflow',\n",
    "        'Filtragem de dados histÃ³ricos (Ãºltimas 4 semanas)',\n",
    "        'Rolling features otimizadas (sem groupby.apply)',\n",
    "        'Threshold otimizado baseado em padrÃ£o histÃ³rico',\n",
    "        'Ceiling inteligente em outliers para reduzir WMAPE',\n",
    "        'Arredondamento conservador para baixas quantidades',\n",
    "        'Garbage collection explÃ­cito para economizar memÃ³ria'\n",
    "    ],\n",
    "    \n",
    "    # ObservaÃ§Ãµes\n",
    "    'observacoes': [\n",
    "        'Pipeline limpo e organizado sem seÃ§Ãµes duplicadas',\n",
    "        'CÃ³digo otimizado para evitar overflow em IDs grandes',\n",
    "        'Features engineered aplicadas consistentemente',\n",
    "        'ValidaÃ§Ã£o temporal para evitar data leakage',\n",
    "        'Threshold otimizado para balance precision/recall',\n",
    "        'PÃ³s-processamento focado em minimizar WMAPE'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Salvar metadados\n",
    "with open('../data/submissao3/pipeline_final_LIMPO_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_pipeline, f)\n",
    "\n",
    "print('ğŸ“‹ Metadados salvos em: pipeline_final_LIMPO_metadata.pkl')\n",
    "\n",
    "print('\\nğŸ‰ PIPELINE FINAL LIMPO CONCLUÃDO COM SUCESSO!')\n",
    "print('=' * 80)\n",
    "print('ğŸ¯ RESUMO FINAL:')\n",
    "print(f'   ğŸ“Š EstratÃ©gia: {metadados_pipeline[\"estrategia\"]}')\n",
    "print(f'   ğŸ¯ Threshold: {best_threshold}')\n",
    "print(f'   ğŸ“Š Total previsÃµes: {len(previsoes_finais):,}')\n",
    "print(f'   ğŸ“Š Taxa positivos: {(previsoes_finais > 0).mean()*100:.1f}%')\n",
    "print(f'   ğŸ“Š Soma total: {previsoes_finais.sum():,}')\n",
    "print(f'\\nğŸ“„ ARQUIVO PRINCIPAL:')\n",
    "print(f'   âœ… submission_final_two_stage_LIMPO.parquet')\n",
    "print(f'\\nğŸš€ PRONTO PARA SUBMISSÃƒO AO DESAFIO!')\n",
    "print('=' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
