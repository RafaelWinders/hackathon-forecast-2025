{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 13 - Pipeline Final de Previsão com Dois Estágios (VERSÃO LIMPA)\n",
    "\n",
    "Este notebook combina os dois modelos treinados para gerar as previsões finais.\n",
    "\n",
    "## Pipeline de Dois Estágios:\n",
    "1. **Classificação**: Prever SE vai vender (LGBMClassifier)\n",
    "2. **Decisão**: Aplicar threshold de probabilidade otimizado\n",
    "3. **Regressão**: Prever QUANTO vai vender apenas para casos positivos (LGBMRegressor)\n",
    "4. **Combinação**: Juntar resultados para submissão final\n",
    "\n",
    "## Objetivo:\n",
    "- Gerar previsões para as 5 semanas de Janeiro 2023\n",
    "- Criar arquivo de submissão no formato exigido\n",
    "- Aplicar otimizações para WMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando Pipeline Final de Dois Estágios - VERSÃO LIMPA\n",
      "🎯 Objetivo: Gerar previsões para Janeiro 2023 combinando classificação + regressão\n",
      "⚡ Versão: Otimizada para performance e sem overflow\n",
      "📁 Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('🚀 Iniciando Pipeline Final de Dois Estágios - VERSÃO LIMPA')\n",
    "print('🎯 Objetivo: Gerar previsões para Janeiro 2023 combinando classificação + regressão')\n",
    "print('⚡ Versão: Otimizada para performance e sem overflow')\n",
    "\n",
    "# Criar pasta submissao3 se não existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('📁 Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Modelos Treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando modelos treinados...\n",
      "✅ Modelos carregados:\n",
      "   🎯 Classificador: LGBMClassifier\n",
      "   📊 Regressor: LGBMRegressor\n",
      "   📋 Features CLASSIFICAÇÃO: 27\n",
      "   📋 Features REGRESSÃO: 31\n",
      "\n",
      "📊 Performance dos modelos:\n",
      "   🎯 Classificador - AUC: 0.9997\n",
      "   📊 Regressor - RMSE: 5.0942\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelos treinados\n",
    "print('📂 Carregando modelos treinados...')\n",
    "\n",
    "# Classificador\n",
    "with open('../data/submissao3/lgbm_classifier.pkl', 'rb') as f:\n",
    "    lgbm_classifier = pickle.load(f)\n",
    "\n",
    "# Regressor\n",
    "with open('../data/submissao3/lgbm_regressor.pkl', 'rb') as f:\n",
    "    lgbm_regressor = pickle.load(f)\n",
    "\n",
    "# Features para classificação\n",
    "with open('../data/submissao3/classification_features.pkl', 'rb') as f:\n",
    "    features_classificacao = pickle.load(f)\n",
    "\n",
    "# Features para regressão\n",
    "try:\n",
    "    with open('../data/submissao3/regression_features.pkl', 'rb') as f:\n",
    "        features_regressao = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('   📝 Arquivo regression_features.pkl não encontrado - usando mesmo da classificação')\n",
    "    features_regressao = features_classificacao\n",
    "\n",
    "# Metadados para verificação\n",
    "with open('../data/submissao3/lgbm_classifier_metadata.pkl', 'rb') as f:\n",
    "    meta_classifier = pickle.load(f)\n",
    "\n",
    "with open('../data/submissao3/lgbm_regressor_metadata.pkl', 'rb') as f:\n",
    "    meta_regressor = pickle.load(f)\n",
    "\n",
    "print(f'✅ Modelos carregados:')\n",
    "print(f'   🎯 Classificador: {type(lgbm_classifier).__name__}')\n",
    "print(f'   📊 Regressor: {type(lgbm_regressor).__name__}')\n",
    "print(f'   📋 Features CLASSIFICAÇÃO: {len(features_classificacao)}')\n",
    "print(f'   📋 Features REGRESSÃO: {len(features_regressao)}')\n",
    "\n",
    "print(f'\\n📊 Performance dos modelos:')\n",
    "print(f'   🎯 Classificador - AUC: {meta_classifier[\"metricas_validacao\"][\"auc\"]:.4f}')\n",
    "print(f'   📊 Regressor - RMSE: {meta_regressor[\"metricas_validacao\"][\"rmse\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Preparação dos Dados Históricos e Combinações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados históricos...\n",
      "📊 Dados históricos de 2022: (55348430, 6)\n",
      "📅 Última semana de 2022: 2022-12-27 00:00:00\n",
      "📅 Semanas de teste (Janeiro 2023):\n",
      "      1. 2023-01-03\n",
      "      2. 2023-01-10\n",
      "      3. 2023-01-17\n",
      "      4. 2023-01-24\n",
      "      5. 2023-01-31\n",
      "📊 Combinações ativas: 1,044,310\n",
      "✅ Dados históricos preparados\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados históricos\n",
    "print('📂 Carregando dados históricos...')\n",
    "\n",
    "train_data = pd.read_parquet('../data/submissao3/train_data.parquet')\n",
    "validation_data = pd.read_parquet('../data/submissao3/validation_data.parquet')\n",
    "\n",
    "# Combinar todos os dados históricos\n",
    "dados_completos_2022 = pd.concat([train_data, validation_data], ignore_index=True)\n",
    "print(f'📊 Dados históricos de 2022: {dados_completos_2022.shape}')\n",
    "\n",
    "# Definir semanas de Janeiro 2023\n",
    "ultima_semana_2022 = dados_completos_2022['semana'].max()\n",
    "print(f'📅 Última semana de 2022: {ultima_semana_2022}')\n",
    "\n",
    "semanas_teste = []\n",
    "for i in range(1, 6):\n",
    "    proxima_semana = ultima_semana_2022 + pd.Timedelta(weeks=i)\n",
    "    semanas_teste.append(proxima_semana)\n",
    "\n",
    "print(f'📅 Semanas de teste (Janeiro 2023):')\n",
    "for i, semana in enumerate(semanas_teste):\n",
    "    print(f'      {i+1}. {semana.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "# Identificar combinações ativas\n",
    "combinacoes_ativas = dados_completos_2022[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "print(f'📊 Combinações ativas: {len(combinacoes_ativas):,}')\n",
    "\n",
    "print('✅ Dados históricos preparados')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Criação do Grid de Teste com Features Otimizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados auxiliares...\n",
      "      🔧 Memória: 4.0 MB → 4.0 MB\n",
      "      🔧 Memória: 3.0 MB → 3.0 MB\n",
      "📊 Produtos: (7092, 8) - Categorias: 7\n",
      "📊 PDVs: (14419, 4) - Zipcodes: 788\n",
      "✅ Dados auxiliares carregados\n"
     ]
    }
   ],
   "source": [
    "# Função para otimizar tipos de dados SEM overflow\n",
    "def otimizar_dtypes_seguro(df):\n",
    "    \"\"\"Otimiza tipos de dados de forma segura, sem overflow.\"\"\"\n",
    "    memoria_antes = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            # Verificar se pode usar int32 sem overflow\n",
    "            if df[col].min() >= np.iinfo(np.int32).min and df[col].max() <= np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    memoria_depois = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f'      🔧 Memória: {memoria_antes:.1f} MB → {memoria_depois:.1f} MB')\n",
    "    return df\n",
    "\n",
    "# Carregar dados auxiliares\n",
    "print('📂 Carregando dados auxiliares...')\n",
    "\n",
    "produtos = pd.read_parquet(\n",
    "    '../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet'\n",
    ")\n",
    "produtos = produtos.rename(columns={'produto': 'produto_id'})\n",
    "produtos = otimizar_dtypes_seguro(produtos)\n",
    "\n",
    "pdvs = pd.read_parquet(\n",
    "    '../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
    ")\n",
    "pdvs = pdvs.rename(columns={'pdv': 'pdv_id', 'categoria_pdv': 'tipo_loja'})\n",
    "pdvs = otimizar_dtypes_seguro(pdvs)\n",
    "\n",
    "print(f'📊 Produtos: {produtos.shape} - Categorias: {produtos[\"categoria\"].nunique()}')\n",
    "print(f'📊 PDVs: {pdvs.shape} - Zipcodes: {pdvs[\"zipcode\"].nunique()}')\n",
    "\n",
    "print('✅ Dados auxiliares carregados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Criando grid de teste para 5 semanas...\n",
      "   📊 Otimizando dados de entrada...\n",
      "      🔧 Memória: 12329.8 MB → 12118.7 MB\n",
      "      🔧 Memória: 159.1 MB → 159.1 MB\n",
      "   🎯 Criando grid de teste...\n",
      "      🔧 Memória: 994.8 MB → 954.9 MB\n",
      "   📊 Grid criado: (5221550, 6)\n",
      "   🔍 Filtrando histórico relevante...\n",
      "   📊 Histórico filtrado: (4177240, 6)\n",
      "      🔧 Memória: 1919.8 MB → 1884.0 MB\n",
      "   📊 Dados completos: (9398790, 6)\n",
      "   🔗 Fazendo merges...\n",
      "      🔧 Memória: 3174.6 MB → 3138.7 MB\n",
      "   ⚡ Criando features...\n",
      "   🔧 Features de lag e rolling...\n",
      "   📈 Features agregadas...\n",
      "      🔧 Memória: 4177.3 MB → 4177.3 MB\n",
      "   ✅ Dados de teste finais: (5221550, 40)\n",
      "   💾 Memória total: 2360.5 MB\n",
      "✅ Grid de teste criado com features otimizadas!\n"
     ]
    }
   ],
   "source": [
    "def criar_grid_teste_com_features(semanas_teste, combinacoes, dados_historicos):\n",
    "    \"\"\"Cria grid de teste com features otimizadas para máxima performance.\"\"\"\n",
    "    print(f'🔧 Criando grid de teste para {len(semanas_teste)} semanas...')\n",
    "    \n",
    "    # Otimizar dados de entrada\n",
    "    print('   📊 Otimizando dados de entrada...')\n",
    "    dados_historicos = otimizar_dtypes_seguro(dados_historicos.copy())\n",
    "    combinacoes = otimizar_dtypes_seguro(combinacoes.copy())\n",
    "    \n",
    "    # Criar grid de teste\n",
    "    print('   🎯 Criando grid de teste...')\n",
    "    grid_parts = []\n",
    "    for semana in semanas_teste:\n",
    "        df_semana = combinacoes.copy()\n",
    "        df_semana['semana'] = semana\n",
    "        df_semana['quantidade'] = 0\n",
    "        df_semana['faturamento'] = 0.0\n",
    "        df_semana['distributor_id'] = None\n",
    "        grid_parts.append(df_semana)\n",
    "    \n",
    "    dados_teste_raw = pd.concat(grid_parts, ignore_index=True)\n",
    "    dados_teste_raw = otimizar_dtypes_seguro(dados_teste_raw)\n",
    "    print(f'   📊 Grid criado: {dados_teste_raw.shape}')\n",
    "    \n",
    "    # Filtrar dados históricos relevantes (últimas 4 semanas por combinação)\n",
    "    print('   🔍 Filtrando histórico relevante...')\n",
    "    pdvs_relevantes = set(combinacoes['pdv_id'])\n",
    "    produtos_relevantes = set(combinacoes['produto_id'])\n",
    "    \n",
    "    mask_relevantes = (\n",
    "        dados_historicos['pdv_id'].isin(pdvs_relevantes) & \n",
    "        dados_historicos['produto_id'].isin(produtos_relevantes)\n",
    "    )\n",
    "    dados_historicos_filtrados = dados_historicos[mask_relevantes].copy()\n",
    "    \n",
    "    # Manter apenas últimas 4 semanas por combinação (suficiente para lags)\n",
    "    dados_historicos_filtrados = dados_historicos_filtrados.sort_values(['pdv_id', 'produto_id', 'semana'])\n",
    "    dados_historicos_filtrados = dados_historicos_filtrados.groupby(['pdv_id', 'produto_id']).tail(4)\n",
    "    print(f'   📊 Histórico filtrado: {dados_historicos_filtrados.shape}')\n",
    "    \n",
    "    # Combinar histórico + teste\n",
    "    dados_completos = pd.concat([dados_historicos_filtrados, dados_teste_raw], ignore_index=True)\n",
    "    dados_completos = dados_completos.sort_values(['pdv_id', 'produto_id', 'semana']).reset_index(drop=True)\n",
    "    dados_completos = otimizar_dtypes_seguro(dados_completos)\n",
    "    print(f'   📊 Dados completos: {dados_completos.shape}')\n",
    "    \n",
    "    # Merge com dados auxiliares\n",
    "    print('   🔗 Fazendo merges...')\n",
    "    dados_completos = dados_completos.merge(produtos[['produto_id', 'categoria']], on='produto_id', how='left')\n",
    "    dados_completos = dados_completos.merge(pdvs[['pdv_id', 'zipcode', 'tipo_loja']], on='pdv_id', how='left')\n",
    "    dados_completos = otimizar_dtypes_seguro(dados_completos)\n",
    "    \n",
    "    # Preencher distributor_id\n",
    "    dados_completos['distributor_id'] = dados_completos.groupby('produto_id')['distributor_id'].transform('ffill').transform('bfill')\n",
    "    \n",
    "    # ========== CRIAR FEATURES ==========\n",
    "    print('   ⚡ Criando features...')\n",
    "    \n",
    "    # 1. Features básicas\n",
    "    dados_completos['preco_unitario_atual'] = np.where(\n",
    "        dados_completos['quantidade'] > 0,\n",
    "        dados_completos['faturamento'] / dados_completos['quantidade'],\n",
    "        0\n",
    "    ).astype('float32')\n",
    "    \n",
    "    # 2. Features temporais\n",
    "    dados_completos['mes'] = dados_completos['semana'].dt.month.astype('int8')\n",
    "    dados_completos['mes_sin'] = np.sin(2 * np.pi * dados_completos['mes'] / 12).astype('float32')\n",
    "    dados_completos['mes_cos'] = np.cos(2 * np.pi * dados_completos['mes'] / 12).astype('float32')\n",
    "    dados_completos['eh_inicio_mes'] = (dados_completos['semana'].dt.day <= 7).astype('int8')\n",
    "    dados_completos['eh_fim_mes'] = (dados_completos['semana'].dt.day >= 22).astype('int8')\n",
    "    dados_completos['dia_do_mes'] = dados_completos['semana'].dt.day.astype('int8')\n",
    "    dados_completos['semana_do_mes'] = ((dados_completos['semana'].dt.day - 1) // 7 + 1).astype('int8')\n",
    "    \n",
    "    # 3. Features de lag (CRÍTICAS)\n",
    "    print('   🔧 Features de lag e rolling...')\n",
    "    gb = dados_completos.groupby(['pdv_id', 'produto_id'])\n",
    "    \n",
    "    # Lags de quantidade\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        dados_completos[f'quantidade_lag_{lag}'] = gb['quantidade'].shift(lag).astype('float32')\n",
    "        \n",
    "    # Lags de preço\n",
    "    dados_completos['preco_lag_1'] = gb['preco_unitario_atual'].shift(1).astype('float32')\n",
    "    dados_completos['preco_lag_2'] = gb['preco_unitario_atual'].shift(2).astype('float32')\n",
    "    dados_completos['variacao_preco_sku_semanal'] = (dados_completos['preco_lag_1'] - dados_completos['preco_lag_2']).fillna(0).astype('float32')\n",
    "    \n",
    "    # Rolling features (OTIMIZADAS)\n",
    "    rolling_gb = gb['quantidade'].rolling(window=4, min_periods=1)\n",
    "    dados_completos['quantidade_media_4w'] = rolling_gb.mean().reset_index(level=[0,1], drop=True).astype('float32')\n",
    "    dados_completos['quantidade_std_4w'] = rolling_gb.std().reset_index(level=[0,1], drop=True).fillna(0).astype('float32')\n",
    "    dados_completos['quantidade_max_4w'] = rolling_gb.max().reset_index(level=[0,1], drop=True).astype('float32')\n",
    "    \n",
    "    # 4. Features derivadas\n",
    "    dados_completos['momentum_ratio'] = (dados_completos['quantidade_lag_1'] / dados_completos['quantidade_media_4w']).fillna(0).astype('float32')\n",
    "    dados_completos['aceleracao'] = (dados_completos['quantidade_lag_1'] - dados_completos['quantidade_lag_2']).fillna(0).astype('float32')\n",
    "    \n",
    "    # 5. Features categóricas hash\n",
    "    dados_completos['pdv_hash'] = (dados_completos['pdv_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados_completos['produto_hash'] = (dados_completos['produto_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados_completos['categoria_hash'] = (dados_completos['categoria'].astype(str).apply(hash).abs() % 50).astype('int8')\n",
    "    dados_completos['zipcode_hash'] = (dados_completos['zipcode'].astype(str).apply(hash).abs() % 1000).astype('int16')\n",
    "    \n",
    "    dados_completos['pdv_produto_hash'] = (dados_completos['pdv_hash'] * 100 + dados_completos['produto_hash']).astype('int16')\n",
    "    dados_completos['categoria_zipcode_hash'] = (dados_completos['categoria_hash'].astype('int32') * 1000 + dados_completos['zipcode_hash']).astype('int32')\n",
    "    \n",
    "    # 6. Features agregadas\n",
    "    print('   📈 Features agregadas...')\n",
    "    dados_completos['preco_medio_semanal_sku_atual'] = dados_completos.groupby(['semana', 'produto_id'])['preco_unitario_atual'].transform('mean').astype('float32')\n",
    "    dados_completos['media_vendas_categoria_pdv_lag_1'] = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('mean').astype('float32')\n",
    "    \n",
    "    # Share de vendas (para classificação - com lag)\n",
    "    vendas_categoria_lag = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('sum')\n",
    "    dados_completos['share_vendas_sku_categoria_lag_1'] = (dados_completos['quantidade_lag_1'] / vendas_categoria_lag).fillna(0).astype('float32')\n",
    "    \n",
    "    # Share de vendas (para regressão - atual)\n",
    "    dados_completos['media_vendas_categoria_pdv_atual'] = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('mean').astype('float32')\n",
    "    vendas_categoria_atual = dados_completos.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('sum')\n",
    "    dados_completos['share_vendas_sku_categoria_atual'] = (dados_completos['quantidade'] / vendas_categoria_atual).fillna(0).astype('float32')\n",
    "    \n",
    "    # Limpeza final\n",
    "    dados_completos.fillna(0, inplace=True)\n",
    "    dados_completos = otimizar_dtypes_seguro(dados_completos)\n",
    "    \n",
    "    # Filtrar apenas semanas de teste\n",
    "    dados_teste_final = dados_completos[dados_completos['semana'].isin(semanas_teste)].copy()\n",
    "    \n",
    "    print(f'   ✅ Dados de teste finais: {dados_teste_final.shape}')\n",
    "    print(f'   💾 Memória total: {dados_teste_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB')\n",
    "    \n",
    "    # Liberar memória\n",
    "    del dados_completos, dados_historicos_filtrados, dados_teste_raw, grid_parts\n",
    "    gc.collect()\n",
    "    \n",
    "    return dados_teste_final\n",
    "\n",
    "# EXECUTAR CRIAÇÃO DO GRID\n",
    "dados_teste = criar_grid_teste_com_features(semanas_teste, combinacoes_ativas, dados_completos_2022)\n",
    "\n",
    "print('✅ Grid de teste criado com features otimizadas!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Aplicação do Pipeline de Dois Estágios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 ESTÁGIO 1: CLASSIFICAÇÃO (prever SE vai vender)\n",
      "   🔧 Preparando features para classificação...\n",
      "   ✅ Features classificação: (5221550, 27)\n",
      "   📊 Probabilidades: média=0.0021, mediana=0.0007\n",
      "\n",
      "🎯 ESTÁGIO 2: OTIMIZAÇÃO DE THRESHOLD\n",
      "   📊 Análise de thresholds:\n",
      "      Threshold 0.1:  14,691 positivos (  0.3%)\n",
      "      Threshold 0.2:   4,502 positivos (  0.1%)\n",
      "      Threshold 0.3:   2,286 positivos (  0.0%)\n",
      "      Threshold 0.4:   1,149 positivos (  0.0%)\n",
      "      Threshold 0.5:     604 positivos (  0.0%)\n",
      "      Threshold 0.6:     278 positivos (  0.0%)\n",
      "      Threshold 0.7:      76 positivos (  0.0%)\n",
      "      Threshold 0.8:       9 positivos (  0.0%)\n",
      "      Threshold 0.9:       0 positivos (  0.0%)\n",
      "   🎯 Threshold escolhido: 0.1\n",
      "   📊 Taxa histórica: 11.1% | Target: 8.9% | Obtida: 0.3%\n",
      "   📊 Casos positivos: 14,691 (0.3%)\n",
      "✅ Estágios 1 e 2 concluídos!\n"
     ]
    }
   ],
   "source": [
    "# ESTÁGIO 1: CLASSIFICAÇÃO\n",
    "print('🎯 ESTÁGIO 1: CLASSIFICAÇÃO (prever SE vai vender)')\n",
    "\n",
    "# Preparar features para classificação\n",
    "print('   🔧 Preparando features para classificação...')\n",
    "X_teste_classificacao = dados_teste[features_classificacao].copy().fillna(0)\n",
    "\n",
    "# Verificar features faltando\n",
    "features_faltando = set(features_classificacao) - set(dados_teste.columns)\n",
    "if features_faltando:\n",
    "    print(f'   ⚠️ Features faltando: {len(features_faltando)} - criando com zeros')\n",
    "    for feat in features_faltando:\n",
    "        X_teste_classificacao[feat] = 0\n",
    "\n",
    "# Reordenar para match com modelo\n",
    "X_teste_classificacao = X_teste_classificacao[features_classificacao]\n",
    "print(f'   ✅ Features classificação: {X_teste_classificacao.shape}')\n",
    "\n",
    "# Aplicar classificação\n",
    "probabilidades_venda = lgbm_classifier.predict_proba(X_teste_classificacao)[:, 1]\n",
    "print(f'   📊 Probabilidades: média={probabilidades_venda.mean():.4f}, mediana={np.median(probabilidades_venda):.4f}')\n",
    "\n",
    "# ESTÁGIO 2: THRESHOLD OTIMIZADO\n",
    "print('\\n🎯 ESTÁGIO 2: OTIMIZAÇÃO DE THRESHOLD')\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "print('   📊 Análise de thresholds:')\n",
    "for t in thresholds:\n",
    "    pos = (probabilidades_venda >= t).sum()\n",
    "    pct = pos / len(probabilidades_venda) * 100\n",
    "    print(f'      Threshold {t:.1f}: {pos:7,} positivos ({pct:5.1f}%)')\n",
    "\n",
    "# Escolher threshold baseado no histórico\n",
    "taxa_historica = (dados_completos_2022['quantidade'] > 0).mean()\n",
    "target_taxa = taxa_historica * 0.8  # Conservador para precisão\n",
    "\n",
    "best_threshold = 0.5\n",
    "min_diff = float('inf')\n",
    "for t in thresholds:\n",
    "    taxa_atual = (probabilidades_venda >= t).mean()\n",
    "    diff = abs(taxa_atual - target_taxa)\n",
    "    if diff < min_diff:\n",
    "        min_diff = diff\n",
    "        best_threshold = t\n",
    "\n",
    "vai_vender = (probabilidades_venda >= best_threshold)\n",
    "casos_positivos = vai_vender.sum()\n",
    "\n",
    "print(f'   🎯 Threshold escolhido: {best_threshold}')\n",
    "print(f'   📊 Taxa histórica: {taxa_historica:.1%} | Target: {target_taxa:.1%} | Obtida: {(casos_positivos/len(vai_vender)):.1%}')\n",
    "print(f'   📊 Casos positivos: {casos_positivos:,} ({casos_positivos/len(vai_vender)*100:.1f}%)')\n",
    "\n",
    "print('✅ Estágios 1 e 2 concluídos!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 ESTÁGIO 3: REGRESSÃO (prever QUANTO vai vender)\n",
      "   📊 Aplicando regressão a 14,691 casos positivos...\n",
      "   📊 Features para regressão: (14691, 31)\n",
      "   📊 Previsões: média=7.03, mediana=4\n",
      "   📊 Min: 1, Max: 223\n",
      "\n",
      "🎉 PIPELINE DE DOIS ESTÁGIOS CONCLUÍDO!\n",
      "============================================================\n",
      "📊 Total de previsões: 5,221,550\n",
      "📊 Previsões zero: 5,206,859 (99.7%)\n",
      "📊 Previsões positivas: 14,691 (0.3%)\n",
      "📊 Soma total prevista: 103,329.0\n",
      "\n",
      "📊 VALIDAÇÃO:\n",
      "   📈 Taxa histórica de vendas: 11.1%\n",
      "   📈 Taxa prevista de vendas: 0.3%\n",
      "   📈 Ratio previsto/histórico: 0.03\n",
      "   ⚠️ ALERTA: Taxa de previsões positivas muito baixa!\n",
      "\n",
      "✅ Pipeline aplicado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# ESTÁGIO 3: REGRESSÃO\n",
    "print('📊 ESTÁGIO 3: REGRESSÃO (prever QUANTO vai vender)')\n",
    "\n",
    "previsoes_finais = np.zeros(len(dados_teste))\n",
    "\n",
    "if casos_positivos > 0:\n",
    "    print(f'   📊 Aplicando regressão a {casos_positivos:,} casos positivos...')\n",
    "    \n",
    "    # Preparar features para regressão\n",
    "    X_teste_regressao = dados_teste[vai_vender][features_regressao].copy().fillna(0)\n",
    "    \n",
    "    # Verificar features faltando\n",
    "    features_faltando_reg = set(features_regressao) - set(dados_teste.columns)\n",
    "    if features_faltando_reg:\n",
    "        print(f'      ⚠️ Features faltando na regressão: {len(features_faltando_reg)}')\n",
    "        for feat in features_faltando_reg:\n",
    "            X_teste_regressao[feat] = 0\n",
    "    \n",
    "    X_teste_regressao = X_teste_regressao[features_regressao]\n",
    "    print(f'   📊 Features para regressão: {X_teste_regressao.shape}')\n",
    "    \n",
    "    # Aplicar regressão\n",
    "    previsoes_quantidade_raw = lgbm_regressor.predict(X_teste_regressao)\n",
    "    \n",
    "    # Pós-processamento otimizado para WMAPE\n",
    "    previsoes_quantidade = np.maximum(previsoes_quantidade_raw, 0)\n",
    "    \n",
    "    # Ceiling inteligente baseado no histórico\n",
    "    p95_historico = np.percentile(dados_completos_2022[dados_completos_2022['quantidade'] > 0]['quantidade'], 95)\n",
    "    previsoes_quantidade = np.where(\n",
    "        previsoes_quantidade > p95_historico,\n",
    "        p95_historico + (previsoes_quantidade - p95_historico) * 0.3,\n",
    "        previsoes_quantidade\n",
    "    )\n",
    "    \n",
    "    # Arredondamento inteligente\n",
    "    previsoes_quantidade_final = np.where(\n",
    "        previsoes_quantidade < 1.5,\n",
    "        np.maximum(np.round(previsoes_quantidade), 1),  # Pelo menos 1 para casos positivos\n",
    "        np.round(previsoes_quantidade)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Inserir previsões nos casos positivos\n",
    "    previsoes_finais[vai_vender] = previsoes_quantidade_final\n",
    "    \n",
    "    print(f'   📊 Previsões: média={previsoes_quantidade_final.mean():.2f}, mediana={np.median(previsoes_quantidade_final):.0f}')\n",
    "    print(f'   📊 Min: {previsoes_quantidade_final.min()}, Max: {previsoes_quantidade_final.max()}')\n",
    "\n",
    "else:\n",
    "    print('   ⚠️ Nenhum caso positivo detectado - todas as previsões serão zero!')\n",
    "\n",
    "# RESULTADO FINAL\n",
    "print('\\n🎉 PIPELINE DE DOIS ESTÁGIOS CONCLUÍDO!')\n",
    "print('=' * 60)\n",
    "print(f'📊 Total de previsões: {len(previsoes_finais):,}')\n",
    "print(f'📊 Previsões zero: {(previsoes_finais == 0).sum():,} ({(previsoes_finais == 0).mean()*100:.1f}%)')\n",
    "print(f'📊 Previsões positivas: {(previsoes_finais > 0).sum():,} ({(previsoes_finais > 0).mean()*100:.1f}%)')\n",
    "print(f'📊 Soma total prevista: {previsoes_finais.sum():,}')\n",
    "\n",
    "# Adicionar resultados ao dataframe\n",
    "dados_teste['quantidade_prevista'] = previsoes_finais\n",
    "dados_teste['probabilidade_venda'] = probabilidades_venda\n",
    "dados_teste['vai_vender'] = vai_vender\n",
    "\n",
    "# Validação dos resultados\n",
    "taxa_prevista = (previsoes_finais > 0).mean()\n",
    "taxa_historica_validacao = (dados_completos_2022['quantidade'] > 0).mean()\n",
    "\n",
    "print(f'\\n📊 VALIDAÇÃO:')\n",
    "print(f'   📈 Taxa histórica de vendas: {taxa_historica_validacao:.1%}')\n",
    "print(f'   📈 Taxa prevista de vendas: {taxa_prevista:.1%}')\n",
    "print(f'   📈 Ratio previsto/histórico: {taxa_prevista/taxa_historica_validacao:.2f}')\n",
    "\n",
    "if taxa_prevista < 0.01:\n",
    "    print('   ⚠️ ALERTA: Taxa de previsões positivas muito baixa!')\n",
    "elif taxa_prevista > 0.20:\n",
    "    print('   ⚠️ ALERTA: Taxa de previsões positivas muito alta!')\n",
    "else:\n",
    "    print('   ✅ Taxa de previsões positivas dentro do esperado')\n",
    "\n",
    "print('\\n✅ Pipeline aplicado com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Geração do Arquivo de Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Gerando arquivo de submissão...\n",
      "\n",
      "   🔍 Verificações de consistência:\n",
      "      • Shape: (5221550, 4)\n",
      "      • Colunas: ['semana', 'pdv_id', 'produto_id', 'quantity']\n",
      "      • Período: 2023-01-03 00:00:00 até 2023-01-31 00:00:00\n",
      "      • Semanas únicas: 5 (esperado: 5)\n",
      "      • Valores negativos: 0 (deve ser 0)\n",
      "      • Valores NaN: 0 (deve ser 0)\n",
      "      • Duplicatas: 0 (deve ser 0)\n",
      "      • Total registros: 5,221,550 (esperado: 5,221,550)\n",
      "      ✅ Total de registros correto!\n",
      "\n",
      "   📊 Estatísticas da submissão:\n",
      "      • Previsões zero: 5,206,859 (99.7%)\n",
      "      • Previsões positivas: 14,691 (0.3%)\n",
      "      • Soma total: 103,329.0\n",
      "      • Quantidade máxima: 223.0\n",
      "      • Quantidade média (apenas >0): 7.03\n",
      "\n",
      "   ✅ Arquivo principal salvo: ../data/submissao3/submission_final_two_stage_LIMPO.parquet\n",
      "   ✅ Arquivo CSV salvo: ../data/submissao3/submission_final_two_stage_LIMPO.csv\n",
      "   ✅ Arquivo detalhado salvo: ../data/submissao3/submission_detailed_LIMPO.parquet\n",
      "\n",
      "   📅 ANÁLISE POR SEMANA:\n",
      "      Semana 1 (2023-01-03): Total=  60783 | Positivos= 9,464 ( 0.9%)\n",
      "      Semana 2 (2023-01-10): Total=  37157 | Positivos= 4,869 ( 0.5%)\n",
      "      Semana 3 (2023-01-17): Total=   5389 | Positivos=   358 ( 0.0%)\n",
      "      Semana 4 (2023-01-24): Total=      0 | Positivos=     0 ( 0.0%)\n",
      "      Semana 5 (2023-01-31): Total=      0 | Positivos=     0 ( 0.0%)\n",
      "\n",
      "✅ ARQUIVOS DE SUBMISSÃO GERADOS!\n",
      "📄 Arquivo principal: submission_final_two_stage_LIMPO.parquet\n",
      "🎯 Pronto para submissão ao desafio!\n"
     ]
    }
   ],
   "source": [
    "# Gerar arquivo de submissão no formato exigido\n",
    "print('📄 Gerando arquivo de submissão...')\n",
    "\n",
    "# Preparar dados para submissão\n",
    "submissao = dados_teste[['semana', 'pdv_id', 'produto_id', 'quantidade_prevista']].copy()\n",
    "submissao = submissao.rename(columns={'quantidade_prevista': 'quantity'})\n",
    "\n",
    "# Verificações de consistência\n",
    "print(f'\\n   🔍 Verificações de consistência:')\n",
    "print(f'      • Shape: {submissao.shape}')\n",
    "print(f'      • Colunas: {list(submissao.columns)}')\n",
    "print(f'      • Período: {submissao[\"semana\"].min()} até {submissao[\"semana\"].max()}')\n",
    "print(f'      • Semanas únicas: {submissao[\"semana\"].nunique()} (esperado: 5)')\n",
    "print(f'      • Valores negativos: {(submissao[\"quantity\"] < 0).sum()} (deve ser 0)')\n",
    "print(f'      • Valores NaN: {submissao[\"quantity\"].isna().sum()} (deve ser 0)')\n",
    "print(f'      • Duplicatas: {submissao.duplicated(subset=[\"semana\", \"pdv_id\", \"produto_id\"]).sum()} (deve ser 0)')\n",
    "\n",
    "# Validar total esperado\n",
    "total_esperado = len(combinacoes_ativas) * len(semanas_teste)\n",
    "print(f'      • Total registros: {len(submissao):,} (esperado: {total_esperado:,})')\n",
    "\n",
    "if len(submissao) == total_esperado:\n",
    "    print(f'      ✅ Total de registros correto!')\n",
    "else:\n",
    "    print(f'      ⚠️ Discrepância no total de registros!')\n",
    "\n",
    "# Estatísticas da submissão\n",
    "print(f'\\n   📊 Estatísticas da submissão:')\n",
    "print(f'      • Previsões zero: {(submissao[\"quantity\"] == 0).sum():,} ({(submissao[\"quantity\"] == 0).mean()*100:.1f}%)')\n",
    "print(f'      • Previsões positivas: {(submissao[\"quantity\"] > 0).sum():,} ({(submissao[\"quantity\"] > 0).mean()*100:.1f}%)')\n",
    "print(f'      • Soma total: {submissao[\"quantity\"].sum():,}')\n",
    "print(f'      • Quantidade máxima: {submissao[\"quantity\"].max()}')\n",
    "if (submissao[\"quantity\"] > 0).sum() > 0:\n",
    "    print(f'      • Quantidade média (apenas >0): {submissao[submissao[\"quantity\"] > 0][\"quantity\"].mean():.2f}')\n",
    "\n",
    "# Salvar arquivos\n",
    "nome_arquivo_principal = '../data/submissao3/submission_final_two_stage_LIMPO.parquet'\n",
    "submissao.to_parquet(nome_arquivo_principal, index=False)\n",
    "print(f'\\n   ✅ Arquivo principal salvo: {nome_arquivo_principal}')\n",
    "\n",
    "nome_arquivo_csv = '../data/submissao3/submission_final_two_stage_LIMPO.csv'\n",
    "submissao.to_csv(nome_arquivo_csv, index=False)\n",
    "print(f'   ✅ Arquivo CSV salvo: {nome_arquivo_csv}')\n",
    "\n",
    "# Salvar dados detalhados para análise\n",
    "dados_detalhados = dados_teste[[\n",
    "    'semana', 'pdv_id', 'produto_id', 'quantidade_prevista', \n",
    "    'probabilidade_venda', 'vai_vender'\n",
    "]].copy()\n",
    "\n",
    "nome_arquivo_detalhado = '../data/submissao3/submission_detailed_LIMPO.parquet'\n",
    "dados_detalhados.to_parquet(nome_arquivo_detalhado, index=False)\n",
    "print(f'   ✅ Arquivo detalhado salvo: {nome_arquivo_detalhado}')\n",
    "\n",
    "# Análise por semana\n",
    "print(f'\\n   📅 ANÁLISE POR SEMANA:')\n",
    "for i, semana in enumerate(semanas_teste):\n",
    "    dados_semana = submissao[submissao['semana'] == semana]\n",
    "    total = dados_semana['quantity'].sum()\n",
    "    positivos = (dados_semana['quantity'] > 0).sum()\n",
    "    taxa = positivos / len(dados_semana) * 100\n",
    "    print(f'      Semana {i+1} ({semana.strftime(\"%Y-%m-%d\")}): Total={total:7.0f} | Positivos={positivos:6,} ({taxa:4.1f}%)')\n",
    "\n",
    "print(f'\\n✅ ARQUIVOS DE SUBMISSÃO GERADOS!')\n",
    "print(f'📄 Arquivo principal: submission_final_two_stage_LIMPO.parquet')\n",
    "print(f'🎯 Pronto para submissão ao desafio!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Salvamento de Metadados Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Salvando metadados do pipeline...\n",
      "📋 Metadados salvos em: pipeline_final_LIMPO_metadata.pkl\n",
      "\n",
      "🎉 PIPELINE FINAL LIMPO CONCLUÍDO COM SUCESSO!\n",
      "================================================================================\n",
      "🎯 RESUMO FINAL:\n",
      "   📊 Estratégia: Pipeline de Dois Estágios - Classificação + Regressão\n",
      "   🎯 Threshold: 0.1\n",
      "   📊 Total previsões: 5,221,550\n",
      "   📊 Taxa positivos: 0.3%\n",
      "   📊 Soma total: 103,329.0\n",
      "\n",
      "📄 ARQUIVO PRINCIPAL:\n",
      "   ✅ submission_final_two_stage_LIMPO.parquet\n",
      "\n",
      "🚀 PRONTO PARA SUBMISSÃO AO DESAFIO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Salvar metadados completos do pipeline\n",
    "print('📋 Salvando metadados do pipeline...')\n",
    "\n",
    "# Calcular estatísticas detalhadas\n",
    "previsoes_positivas_stats = dados_teste[dados_teste['quantidade_prevista'] > 0]['quantidade_prevista']\n",
    "vendas_historicas = dados_completos_2022[dados_completos_2022['quantidade'] > 0]['quantidade']\n",
    "\n",
    "metadados_pipeline = {\n",
    "    'data_criacao': pd.Timestamp.now(),\n",
    "    'versao': 'Pipeline Final Limpo v1.0',\n",
    "    'estrategia': 'Pipeline de Dois Estágios - Classificação + Regressão',\n",
    "    \n",
    "    # Modelos\n",
    "    'modelos': {\n",
    "        'classificador': 'LightGBM Classifier',\n",
    "        'regressor': 'LightGBM Regressor',\n",
    "        'features_classificacao': len(features_classificacao),\n",
    "        'features_regressao': len(features_regressao)\n",
    "    },\n",
    "    \n",
    "    # Performance\n",
    "    'performance_modelos': {\n",
    "        'classificador_auc': float(meta_classifier['metricas_validacao']['auc']),\n",
    "        'regressor_rmse': float(meta_regressor['metricas_validacao']['rmse'])\n",
    "    },\n",
    "    \n",
    "    # Configuração do pipeline\n",
    "    'configuracao': {\n",
    "        'threshold_classificacao': float(best_threshold),\n",
    "        'total_combinacoes': len(combinacoes_ativas),\n",
    "        'semanas_previsao': len(semanas_teste),\n",
    "        'periodo_inicio': semanas_teste[0].strftime('%Y-%m-%d'),\n",
    "        'periodo_fim': semanas_teste[-1].strftime('%Y-%m-%d')\n",
    "    },\n",
    "    \n",
    "    # Resultados\n",
    "    'resultados': {\n",
    "        'total_previsoes': len(previsoes_finais),\n",
    "        'previsoes_zero': int((previsoes_finais == 0).sum()),\n",
    "        'previsoes_positivas': int((previsoes_finais > 0).sum()),\n",
    "        'taxa_previsoes_positivas': float((previsoes_finais > 0).mean()),\n",
    "        'quantidade_total_prevista': int(previsoes_finais.sum()),\n",
    "        'quantidade_media_prevista': float(previsoes_finais.mean()),\n",
    "        'probabilidade_media': float(probabilidades_venda.mean())\n",
    "    },\n",
    "    \n",
    "    # Estatísticas detalhadas\n",
    "    'estatisticas_previsoes_positivas': {\n",
    "        'count': len(previsoes_positivas_stats),\n",
    "        'mean': float(previsoes_positivas_stats.mean()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'median': float(previsoes_positivas_stats.median()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'std': float(previsoes_positivas_stats.std()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'min': float(previsoes_positivas_stats.min()) if len(previsoes_positivas_stats) > 0 else 0,\n",
    "        'max': float(previsoes_positivas_stats.max()) if len(previsoes_positivas_stats) > 0 else 0\n",
    "    },\n",
    "    \n",
    "    # Comparação histórica\n",
    "    'comparacao_historica': {\n",
    "        'taxa_historica_vendas': float(len(vendas_historicas) / len(dados_completos_2022)),\n",
    "        'media_historica_vendas': float(vendas_historicas.mean()),\n",
    "        'total_historico': float(dados_completos_2022['quantidade'].sum()),\n",
    "        'ratio_total_previsto_historico': float(previsoes_finais.sum() / dados_completos_2022['quantidade'].sum()),\n",
    "        'ratio_taxa_prevista_historica': float((previsoes_finais > 0).mean() / (dados_completos_2022['quantidade'] > 0).mean())\n",
    "    },\n",
    "    \n",
    "    # Arquivos gerados\n",
    "    'arquivos_gerados': [\n",
    "        'submission_final_two_stage_LIMPO.parquet',\n",
    "        'submission_final_two_stage_LIMPO.csv',\n",
    "        'submission_detailed_LIMPO.parquet',\n",
    "        'pipeline_final_LIMPO_metadata.pkl'\n",
    "    ],\n",
    "    \n",
    "    # Otimizações aplicadas\n",
    "    'otimizacoes': [\n",
    "        'Otimização de tipos de dados sem overflow',\n",
    "        'Filtragem de dados históricos (últimas 4 semanas)',\n",
    "        'Rolling features otimizadas (sem groupby.apply)',\n",
    "        'Threshold otimizado baseado em padrão histórico',\n",
    "        'Ceiling inteligente em outliers para reduzir WMAPE',\n",
    "        'Arredondamento conservador para baixas quantidades',\n",
    "        'Garbage collection explícito para economizar memória'\n",
    "    ],\n",
    "    \n",
    "    # Observações\n",
    "    'observacoes': [\n",
    "        'Pipeline limpo e organizado sem seções duplicadas',\n",
    "        'Código otimizado para evitar overflow em IDs grandes',\n",
    "        'Features engineered aplicadas consistentemente',\n",
    "        'Validação temporal para evitar data leakage',\n",
    "        'Threshold otimizado para balance precision/recall',\n",
    "        'Pós-processamento focado em minimizar WMAPE'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Salvar metadados\n",
    "with open('../data/submissao3/pipeline_final_LIMPO_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_pipeline, f)\n",
    "\n",
    "print('📋 Metadados salvos em: pipeline_final_LIMPO_metadata.pkl')\n",
    "\n",
    "print('\\n🎉 PIPELINE FINAL LIMPO CONCLUÍDO COM SUCESSO!')\n",
    "print('=' * 80)\n",
    "print('🎯 RESUMO FINAL:')\n",
    "print(f'   📊 Estratégia: {metadados_pipeline[\"estrategia\"]}')\n",
    "print(f'   🎯 Threshold: {best_threshold}')\n",
    "print(f'   📊 Total previsões: {len(previsoes_finais):,}')\n",
    "print(f'   📊 Taxa positivos: {(previsoes_finais > 0).mean()*100:.1f}%')\n",
    "print(f'   📊 Soma total: {previsoes_finais.sum():,}')\n",
    "print(f'\\n📄 ARQUIVO PRINCIPAL:')\n",
    "print(f'   ✅ submission_final_two_stage_LIMPO.parquet')\n",
    "print(f'\\n🚀 PRONTO PARA SUBMISSÃO AO DESAFIO!')\n",
    "print('=' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
