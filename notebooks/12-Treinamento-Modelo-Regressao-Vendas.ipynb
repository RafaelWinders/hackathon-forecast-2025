{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Modelo de Regressão (Prever QUANTO vai vender)\n",
    "\n",
    "Este notebook treina o segundo estágio do modelo de dois estágios: **regressão**.\n",
    "\n",
    "## Objetivo:\n",
    "- Prever a quantidade de vendas **apenas para casos onde vendeu = 1**\n",
    "- Usar LightGBM Regressor especializado em prever volumes\n",
    "- Avaliar com métricas de regressão (RMSE, MAE, MAPE)\n",
    "- Otimizar para casos com vendas positivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Iniciando Treinamento do Modelo de Regressão\n",
      "🎯 Objetivo: Prever QUANTO vai vender (apenas onde vendeu = 1)\n",
      "📁 Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, \n",
    "    r2_score\n",
    ")\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('📊 Iniciando Treinamento do Modelo de Regressão')\n",
    "print('🎯 Objetivo: Prever QUANTO vai vender (apenas onde vendeu = 1)')\n",
    "\n",
    "# Criar pasta submissao3 se não existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('📁 Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados com Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados com features avançadas...\n",
      "🏋️ Dados de treino (completos): (50126880, 40)\n",
      "🔍 Dados de validação (completos): (5221550, 40)\n",
      "\n",
      "✂️ APLICANDO FILTRO CRÍTICO: apenas registros com vendas...\n",
      "🏋️ Dados de treino (apenas vendas): (5561122, 40)\n",
      "🔍 Dados de validação (apenas vendas): (572247, 40)\n",
      "\n",
      "📊 Estatísticas do target \"quantidade\" no treino:\n",
      "   • Média: 9.08\n",
      "   • Mediana: 2.00\n",
      "   • Desvio padrão: 87.26\n",
      "   • Min: 0\n",
      "   • Max: 94230\n",
      "\n",
      "📊 Estatísticas do target \"quantidade\" na validação:\n",
      "   • Média: 5.26\n",
      "   • Mediana: 2.00\n",
      "   • Desvio padrão: 15.81\n",
      "   • Min: 0\n",
      "   • Max: 2472\n",
      "✅ Dados filtrados para regressão\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com features avançadas\n",
    "print('📂 Carregando dados com features avançadas...')\n",
    "\n",
    "train_features = pd.read_parquet('../data/submissao3/train_features.parquet')\n",
    "validation_features = pd.read_parquet('../data/submissao3/validation_features.parquet')\n",
    "\n",
    "print(f'🏋️ Dados de treino (completos): {train_features.shape}')\n",
    "print(f'🔍 Dados de validação (completos): {validation_features.shape}')\n",
    "\n",
    "# FILTRO CRÍTICO: Manter apenas registros onde vendeu = 1\n",
    "print('\\n✂️ APLICANDO FILTRO CRÍTICO: apenas registros com vendas...')\n",
    "\n",
    "train_sales = train_features[train_features['vendeu'] == 1].copy()\n",
    "validation_sales = validation_features[validation_features['vendeu'] == 1].copy()\n",
    "\n",
    "print(f'🏋️ Dados de treino (apenas vendas): {train_sales.shape}')\n",
    "print(f'🔍 Dados de validação (apenas vendas): {validation_sales.shape}')\n",
    "\n",
    "# Verificar distribuição do target de regressão\n",
    "print(f'\\n📊 Estatísticas do target \"quantidade\" no treino:')\n",
    "print(f'   • Média: {train_sales[\"quantidade\"].mean():.2f}')\n",
    "print(f'   • Mediana: {train_sales[\"quantidade\"].median():.2f}')\n",
    "print(f'   • Desvio padrão: {train_sales[\"quantidade\"].std():.2f}')\n",
    "print(f'   • Min: {train_sales[\"quantidade\"].min():.0f}')\n",
    "print(f'   • Max: {train_sales[\"quantidade\"].max():.0f}')\n",
    "\n",
    "print(f'\\n📊 Estatísticas do target \"quantidade\" na validação:')\n",
    "print(f'   • Média: {validation_sales[\"quantidade\"].mean():.2f}')\n",
    "print(f'   • Mediana: {validation_sales[\"quantidade\"].median():.2f}')\n",
    "print(f'   • Desvio padrão: {validation_sales[\"quantidade\"].std():.2f}')\n",
    "print(f'   • Min: {validation_sales[\"quantidade\"].min():.0f}')\n",
    "print(f'   • Max: {validation_sales[\"quantidade\"].max():.0f}')\n",
    "\n",
    "print('✅ Dados filtrados para regressão')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação dos Dados para Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preparar features para regressão - VERSÃO OTIMIZADA\nprint('🔧 Preparando features para regressão - VERSÃO OTIMIZADA...')\n\n# Carregar lista de features seguras (sem leakage) da classificação\nwith open('../data/submissao3/classification_features.pkl', 'rb') as f:\n    features_classificacao = pickle.load(f)\n\n# Para REGRESSÃO, podemos usar algumas features \"atuais\" que foram removidas da classificação\n# Porque quando fazemos regressão, já sabemos que vendeu=1, então não há leakage\nfeatures_extras_para_regressao = []\n\n# Verificar se features \"atuais\" existem nos dados\ntodas_features = list(train_sales.columns)\nfeatures_atuais_disponiveis = [f for f in todas_features if 'atual' in f]\n\nprint(f'📊 Features da classificação (sem leakage): {len(features_classificacao)}')\nprint(f'📊 Features atuais disponíveis para regressão: {len(features_atuais_disponiveis)}')\n\nif features_atuais_disponiveis:\n    print('\\n✅ FEATURES EXTRAS PARA REGRESSÃO (com contexto atual):')\n    for feat in features_atuais_disponiveis:\n        print(f'   • {feat}')\n    features_extras_para_regressao = features_atuais_disponiveis\nelse:\n    print('\\n📝 Nenhuma feature \"atual\" encontrada - usando mesmo conjunto da classificação')\n\n# Combinar features: classificação (seguras) + extras para regressão\nfeatures_modelo_regressao = features_classificacao + features_extras_para_regressao\n\nprint(f'\\n📊 Total de features para regressão: {len(features_modelo_regressao)}')\nprint(f'   📊 Features básicas (da classificação): {len(features_classificacao)}')\nprint(f'   📊 Features extras (contexto atual): {len(features_extras_para_regressao)}')\n\n# Preparar datasets de regressão\nX_train_reg = train_sales[features_modelo_regressao].copy()\ny_train_reg = train_sales['quantidade'].copy()\n\nX_val_reg = validation_sales[features_modelo_regressao].copy()\ny_val_reg = validation_sales['quantidade'].copy()\n\nprint(f'\\n📊 Datasets de regressão preparados:')\nprint(f'   🏋️ X_train_reg: {X_train_reg.shape}, y_train_reg: {y_train_reg.shape}')\nprint(f'   🔍 X_val_reg: {X_val_reg.shape}, y_val_reg: {y_val_reg.shape}')\n\n# Verificar se há NAs\nnas_train = X_train_reg.isnull().sum().sum()\nnas_val = X_val_reg.isnull().sum().sum()\nprint(f'   🧹 NAs no treino: {nas_train}, NAs na validação: {nas_val}')\n\nif nas_train > 0 or nas_val > 0:\n    print('   ⚠️ Preenchendo NAs com 0...')\n    X_train_reg = X_train_reg.fillna(0)\n    X_val_reg = X_val_reg.fillna(0)\n\n# Verificar outliers extremos no target\nq99 = y_train_reg.quantile(0.99)\noutliers = (y_train_reg > q99).sum()\nprint(f'\\n📊 Análise de outliers:')\nprint(f'   • P99 da quantidade: {q99:.0f}')\nprint(f'   • Outliers acima P99: {outliers:,} ({outliers/len(y_train_reg)*100:.2f}%)')\n\n# Para regressão, manter outliers é importante para capturar vendas grandes\nprint('   💡 Mantendo outliers para preservar informação real de vendas grandes')\n\nprint('✅ Dados preparados para regressão com features otimizadas')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento do Modelo LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Treinando LightGBM Regressor...\n",
      "   📚 Iniciando treinamento...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 0.603397\tvalid_0's rmse: 5.12892\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l1: 0.603397\tvalid_0's rmse: 5.12892\n",
      "✅ Modelo treinado! Melhor iteração: 100\n",
      "📊 Score de validação: defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', np.float64(0.6033969145163481)), ('rmse', np.float64(5.128924904561696))])})\n"
     ]
    }
   ],
   "source": [
    "# Configurar e treinar LightGBM Regressor\n",
    "print('🚀 Treinando LightGBM Regressor...')\n",
    "\n",
    "# Parâmetros otimizados para regressão\n",
    "lgbm_reg_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 80,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Criar modelo\n",
    "lgbm_regressor = lgb.LGBMRegressor(**lgbm_reg_params)\n",
    "\n",
    "# Treinar modelo com callbacks para early stopping\n",
    "print('   📚 Iniciando treinamento...')\n",
    "lgbm_regressor.fit(\n",
    "    X_train_reg, y_train_reg,\n",
    "    eval_set=[(X_val_reg, y_val_reg)],\n",
    "    eval_metric=['rmse', 'mae'],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f'✅ Modelo treinado! Melhor iteração: {lgbm_regressor.best_iteration_}')\n",
    "print(f'📊 Score de validação: {lgbm_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões\n",
    "print('🔍 Avaliando modelo de regressão...')\n",
    "\n",
    "# Previsões\n",
    "y_pred_train_reg = lgbm_regressor.predict(X_train_reg)\n",
    "y_pred_val_reg = lgbm_regressor.predict(X_val_reg)\n",
    "\n",
    "# Garantir que previsões não sejam negativas\n",
    "y_pred_train_reg = np.maximum(y_pred_train_reg, 0)\n",
    "y_pred_val_reg = np.maximum(y_pred_val_reg, 0)\n",
    "\n",
    "# Função para calcular MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n",
    "\n",
    "# Métricas de treino\n",
    "print('\\n📊 MÉTRICAS DE TREINO:')\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_reg, y_pred_train_reg))\n",
    "mae_train = mean_absolute_error(y_train_reg, y_pred_train_reg)\n",
    "r2_train = r2_score(y_train_reg, y_pred_train_reg)\n",
    "mape_train = mean_absolute_percentage_error(y_train_reg, y_pred_train_reg)\n",
    "\n",
    "print(f'   📊 RMSE: {rmse_train:.4f}')\n",
    "print(f'   📊 MAE: {mae_train:.4f}')\n",
    "print(f'   📊 R²: {r2_train:.4f}')\n",
    "print(f'   📊 MAPE: {mape_train:.2f}%')\n",
    "\n",
    "# Métricas de validação\n",
    "print('\\n📊 MÉTRICAS DE VALIDAÇÃO:')\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val_reg, y_pred_val_reg))\n",
    "mae_val = mean_absolute_error(y_val_reg, y_pred_val_reg)\n",
    "r2_val = r2_score(y_val_reg, y_pred_val_reg)\n",
    "mape_val = mean_absolute_percentage_error(y_val_reg, y_pred_val_reg)\n",
    "\n",
    "print(f'   📊 RMSE: {rmse_val:.4f}')\n",
    "print(f'   📊 MAE: {mae_val:.4f}')\n",
    "print(f'   📊 R²: {r2_val:.4f}')\n",
    "print(f'   📊 MAPE: {mape_val:.2f}%')\n",
    "\n",
    "# Análise de resíduos\n",
    "print('\\n📊 ANÁLISE DE RESÍDUOS (Validação):')\n",
    "residuos = y_val_reg - y_pred_val_reg\n",
    "print(f'   📊 Média dos resíduos: {residuos.mean():.4f}')\n",
    "print(f'   📊 Desvio padrão dos resíduos: {residuos.std():.4f}')\n",
    "print(f'   📊 P25 dos resíduos: {residuos.quantile(0.25):.4f}')\n",
    "print(f'   📊 P75 dos resíduos: {residuos.quantile(0.75):.4f}')\n",
    "\n",
    "# Análise por faixas de quantidade\n",
    "print('\\n📊 PERFORMANCE POR FAIXA DE QUANTIDADE:')\n",
    "faixas = [(0, 5), (5, 10), (10, 20), (20, 50), (50, 1000)]\n",
    "for faixa_min, faixa_max in faixas:\n",
    "    mask = (y_val_reg >= faixa_min) & (y_val_reg < faixa_max)\n",
    "    if mask.sum() > 0:\n",
    "        mae_faixa = mean_absolute_error(y_val_reg[mask], y_pred_val_reg[mask])\n",
    "        count_faixa = mask.sum()\n",
    "        print(f'   Faixa [{faixa_min:3d}, {faixa_max:3d}): MAE={mae_faixa:6.2f}, Count={count_faixa:6,}')\n",
    "\n",
    "print('✅ Avaliação do modelo de regressão concluída')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análise de Importância das Features (Regressão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar importância das features para regressão\n",
    "print('📊 Analisando importância das features para regressão...')\n",
    "\n",
    "# Obter importâncias\n",
    "feature_importance_reg = lgbm_regressor.feature_importances_\n",
    "feature_names_reg = X_train_reg.columns\n",
    "\n",
    "# Criar DataFrame de importâncias\n",
    "importance_reg_df = pd.DataFrame({\n",
    "    'feature': feature_names_reg,\n",
    "    'importance': feature_importance_reg\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features mais importantes para regressão\n",
    "print('\\n🏆 TOP 20 FEATURES MAIS IMPORTANTES (REGRESSÃO):')\n",
    "for i, (_, row) in enumerate(importance_reg_df.head(20).iterrows()):\n",
    "    print(f'   {i+1:2d}. {row[\"feature\"]:30s} - {row[\"importance\"]:6.0f}')\n",
    "\n",
    "# Comparar com importâncias da classificação\n",
    "print('\\n📊 COMPARANDO COM CLASSIFICAÇÃO:')\n",
    "classification_importance = pd.read_csv('../data/submissao3/classification_feature_importance.csv')\n",
    "\n",
    "# Top 10 features de cada modelo\n",
    "top_classification = classification_importance.head(10)['feature'].tolist()\n",
    "top_regression = importance_reg_df.head(10)['feature'].tolist()\n",
    "\n",
    "features_em_comum = set(top_classification).intersection(set(top_regression))\n",
    "print(f'   📊 Features em comum no Top 10: {len(features_em_comum)}/10')\n",
    "print(f'   📊 Features comuns: {list(features_em_comum)}')\n",
    "\n",
    "# Análise por categoria de feature para regressão\n",
    "print('\\n📊 IMPORTÂNCIA POR CATEGORIA (REGRESSÃO):')\n",
    "categorias_feature = {\n",
    "    'Lag': [f for f in feature_names_reg if 'lag_' in f],\n",
    "    'Rolling': [f for f in feature_names_reg if any(x in f for x in ['media_4w', 'std_4w', 'max_4w'])],\n",
    "    'Preço': [f for f in feature_names_reg if 'preco' in f],\n",
    "    'Calendário': [f for f in feature_names_reg if any(x in f for x in ['mes', 'dia', 'inicio', 'fim'])],\n",
    "    'Tendência': [f for f in feature_names_reg if any(x in f for x in ['momentum', 'aceleracao'])],\n",
    "    'Hierarquia': [f for f in feature_names_reg if any(x in f for x in ['media_vendas', 'share'])],\n",
    "    'Hash': [f for f in feature_names_reg if 'hash' in f]\n",
    "}\n",
    "\n",
    "for categoria, features in categorias_feature.items():\n",
    "    if features:\n",
    "        importancia_categoria = importance_reg_df[importance_reg_df['feature'].isin(features)]['importance'].sum()\n",
    "        print(f'   {categoria:12s}: {importancia_categoria:8.0f} ({len(features)} features)')\n",
    "\n",
    "print('✅ Análise de importância para regressão concluída')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvamento do Modelo de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Salvar modelo de regressão OTIMIZADO\nprint('💾 Salvando modelo de regressão OTIMIZADO...')\n\n# Salvar modelo\nwith open('../data/submissao3/lgbm_regressor.pkl', 'wb') as f:\n    pickle.dump(lgbm_regressor, f)\n\n# IMPORTANTE: Salvar lista de features da regressão (pode ser diferente da classificação)\nwith open('../data/submissao3/regression_features.pkl', 'wb') as f:\n    pickle.dump(features_modelo_regressao, f)\n\n# Salvar importâncias da regressão\nimportance_reg_df.to_csv('../data/submissao3/regression_feature_importance.csv', index=False)\n\n# Salvar metadados do modelo de regressão OTIMIZADO\nmetadados_regressor = {\n    'data_criacao': pd.Timestamp.now(),\n    'modelo': 'LightGBM Regressor OTIMIZADO',\n    'objetivo': 'Regressão: prever quantidade de vendas (apenas onde vendeu = 1)',\n    'parametros': lgbm_reg_params,\n    'melhor_iteracao': lgbm_regressor.best_iteration_,\n    'melhor_score': lgbm_regressor.best_score_,\n    'total_features': len(features_modelo_regressao),\n    'features_usadas': features_modelo_regressao,\n    'features_da_classificacao': len(features_classificacao),\n    'features_extras_regressao': len(features_extras_para_regressao),\n    'strategy': {\n        'base_features': 'Mesmas da classificação (sem leakage)',\n        'extra_features': 'Features com contexto atual (permitidas na regressão)',\n        'reasoning': 'Regressão pode usar contexto atual pois já sabe que vendeu=1'\n    },\n    'metricas_validacao': {\n        'rmse': rmse_val,\n        'mae': mae_val,\n        'r2': r2_val,\n        'mape': mape_val\n    },\n    'shape_treino': X_train_reg.shape,\n    'shape_validacao': X_val_reg.shape,\n    'target_stats_treino': {\n        'mean': float(y_train_reg.mean()),\n        'median': float(y_train_reg.median()),\n        'std': float(y_train_reg.std()),\n        'min': float(y_train_reg.min()),\n        'max': float(y_train_reg.max())\n    },\n    'target_stats_validacao': {\n        'mean': float(y_val_reg.mean()),\n        'median': float(y_val_reg.median()),\n        'std': float(y_val_reg.std()),\n        'min': float(y_val_reg.min()),\n        'max': float(y_val_reg.max())\n    },\n    'observacoes': [\n        'Modelo treinado APENAS em registros com vendeu=1',\n        'Pode usar features \"atuais\" pois não há leakage na regressão',\n        'Previsões são clampadas para >= 0',\n        'Otimizado para casos com vendas positivas',\n        'Preserva outliers para capturar vendas grandes',\n        'Deve ser usado em conjunto com classificador'\n    ]\n}\n\nwith open('../data/submissao3/lgbm_regressor_metadata.pkl', 'wb') as f:\n    pickle.dump(metadados_regressor, f)\n\nprint('✅ Arquivos salvos:')\nprint('   • data/submissao3/lgbm_regressor.pkl')\nprint('   • data/submissao3/regression_features.pkl (SEPARADO da classificação)')\nprint('   • data/submissao3/regression_feature_importance.csv')\nprint('   • data/submissao3/lgbm_regressor_metadata.pkl')\n\nprint('\\n🎉 MODELO DE REGRESSÃO OTIMIZADO TREINADO E SALVO!')\nprint('=' * 70)\nprint('🎯 Resumo do Modelo de Regressão OTIMIZADO:')\nprint(f'   📊 RMSE na validação: {rmse_val:.4f}')\nprint(f'   📊 MAE na validação: {mae_val:.4f}')\nprint(f'   📊 R² na validação: {r2_val:.4f}')\nprint(f'   📊 MAPE na validação: {mape_val:.2f}%')\nprint(f'   📊 Features totais: {len(features_modelo_regressao)}')\nprint(f'     • Features base (sem leakage): {len(features_classificacao)}')\nprint(f'     • Features extras (contexto atual): {len(features_extras_para_regressao)}')\nprint(f'   📊 Registros de treino: {len(X_train_reg):,} (apenas com vendas)')\nprint(f'   📊 Registros de validação: {len(X_val_reg):,} (apenas com vendas)')\n\nprint('\\n💡 IMPORTANTE:')\nprint('   🔄 Classificação e Regressão usam features DIFERENTES')\nprint('   📊 Classificação: features seguras (sem leakage)')  \nprint('   📊 Regressão: features seguras + contexto atual')\nprint('   🎯 Isso é CORRETO e esperado!')\n\nprint('\\n💡 Próximo passo:')\nprint('   🔄 Atualizar pipeline final (Notebook 13) para usar features corretas')\nprint('   📊 Aplicar modelos nas 5 semanas de Janeiro 2023')\n\nprint('\\n🚀 Segundo estágio OTIMIZADO concluído!')\nprint('🎯 Ambos os modelos estão prontos e corrigidos!')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}