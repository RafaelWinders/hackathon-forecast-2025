{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Modelo de RegressÃ£o (Prever QUANTO vai vender)\n",
    "\n",
    "Este notebook treina o segundo estÃ¡gio do modelo de dois estÃ¡gios: **regressÃ£o**.\n",
    "\n",
    "## Objetivo:\n",
    "- Prever a quantidade de vendas **apenas para casos onde vendeu = 1**\n",
    "- Usar LightGBM Regressor especializado em prever volumes\n",
    "- Avaliar com mÃ©tricas de regressÃ£o (RMSE, MAE, MAPE)\n",
    "- Otimizar para casos com vendas positivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Iniciando Treinamento do Modelo de RegressÃ£o\n",
      "ğŸ¯ Objetivo: Prever QUANTO vai vender (apenas onde vendeu = 1)\n",
      "ğŸ“ Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, \n",
    "    r2_score\n",
    ")\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('ğŸ“Š Iniciando Treinamento do Modelo de RegressÃ£o')\n",
    "print('ğŸ¯ Objetivo: Prever QUANTO vai vender (apenas onde vendeu = 1)')\n",
    "\n",
    "# Criar pasta submissao3 se nÃ£o existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('ğŸ“ Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados com Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando dados com features avanÃ§adas...\n",
      "ğŸ‹ï¸ Dados de treino (completos): (50126880, 40)\n",
      "ğŸ” Dados de validaÃ§Ã£o (completos): (5221550, 40)\n",
      "\n",
      "âœ‚ï¸ APLICANDO FILTRO CRÃTICO: apenas registros com vendas...\n",
      "ğŸ‹ï¸ Dados de treino (apenas vendas): (5561122, 40)\n",
      "ğŸ” Dados de validaÃ§Ã£o (apenas vendas): (572247, 40)\n",
      "\n",
      "ğŸ“Š EstatÃ­sticas do target \"quantidade\" no treino:\n",
      "   â€¢ MÃ©dia: 9.08\n",
      "   â€¢ Mediana: 2.00\n",
      "   â€¢ Desvio padrÃ£o: 87.26\n",
      "   â€¢ Min: 0\n",
      "   â€¢ Max: 94230\n",
      "\n",
      "ğŸ“Š EstatÃ­sticas do target \"quantidade\" na validaÃ§Ã£o:\n",
      "   â€¢ MÃ©dia: 5.26\n",
      "   â€¢ Mediana: 2.00\n",
      "   â€¢ Desvio padrÃ£o: 15.81\n",
      "   â€¢ Min: 0\n",
      "   â€¢ Max: 2472\n",
      "âœ… Dados filtrados para regressÃ£o\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com features avanÃ§adas\n",
    "print('ğŸ“‚ Carregando dados com features avanÃ§adas...')\n",
    "\n",
    "train_features = pd.read_parquet('../data/submissao3/train_features.parquet')\n",
    "validation_features = pd.read_parquet('../data/submissao3/validation_features.parquet')\n",
    "\n",
    "print(f'ğŸ‹ï¸ Dados de treino (completos): {train_features.shape}')\n",
    "print(f'ğŸ” Dados de validaÃ§Ã£o (completos): {validation_features.shape}')\n",
    "\n",
    "# FILTRO CRÃTICO: Manter apenas registros onde vendeu = 1\n",
    "print('\\nâœ‚ï¸ APLICANDO FILTRO CRÃTICO: apenas registros com vendas...')\n",
    "\n",
    "train_sales = train_features[train_features['vendeu'] == 1].copy()\n",
    "validation_sales = validation_features[validation_features['vendeu'] == 1].copy()\n",
    "\n",
    "print(f'ğŸ‹ï¸ Dados de treino (apenas vendas): {train_sales.shape}')\n",
    "print(f'ğŸ” Dados de validaÃ§Ã£o (apenas vendas): {validation_sales.shape}')\n",
    "\n",
    "# Verificar distribuiÃ§Ã£o do target de regressÃ£o\n",
    "print(f'\\nğŸ“Š EstatÃ­sticas do target \"quantidade\" no treino:')\n",
    "print(f'   â€¢ MÃ©dia: {train_sales[\"quantidade\"].mean():.2f}')\n",
    "print(f'   â€¢ Mediana: {train_sales[\"quantidade\"].median():.2f}')\n",
    "print(f'   â€¢ Desvio padrÃ£o: {train_sales[\"quantidade\"].std():.2f}')\n",
    "print(f'   â€¢ Min: {train_sales[\"quantidade\"].min():.0f}')\n",
    "print(f'   â€¢ Max: {train_sales[\"quantidade\"].max():.0f}')\n",
    "\n",
    "print(f'\\nğŸ“Š EstatÃ­sticas do target \"quantidade\" na validaÃ§Ã£o:')\n",
    "print(f'   â€¢ MÃ©dia: {validation_sales[\"quantidade\"].mean():.2f}')\n",
    "print(f'   â€¢ Mediana: {validation_sales[\"quantidade\"].median():.2f}')\n",
    "print(f'   â€¢ Desvio padrÃ£o: {validation_sales[\"quantidade\"].std():.2f}')\n",
    "print(f'   â€¢ Min: {validation_sales[\"quantidade\"].min():.0f}')\n",
    "print(f'   â€¢ Max: {validation_sales[\"quantidade\"].max():.0f}')\n",
    "\n",
    "print('âœ… Dados filtrados para regressÃ£o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PreparaÃ§Ã£o dos Dados para RegressÃ£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preparar features para regressÃ£o - VERSÃƒO OTIMIZADA\nprint('ğŸ”§ Preparando features para regressÃ£o - VERSÃƒO OTIMIZADA...')\n\n# Carregar lista de features seguras (sem leakage) da classificaÃ§Ã£o\nwith open('../data/submissao3/classification_features.pkl', 'rb') as f:\n    features_classificacao = pickle.load(f)\n\n# Para REGRESSÃƒO, podemos usar algumas features \"atuais\" que foram removidas da classificaÃ§Ã£o\n# Porque quando fazemos regressÃ£o, jÃ¡ sabemos que vendeu=1, entÃ£o nÃ£o hÃ¡ leakage\nfeatures_extras_para_regressao = []\n\n# Verificar se features \"atuais\" existem nos dados\ntodas_features = list(train_sales.columns)\nfeatures_atuais_disponiveis = [f for f in todas_features if 'atual' in f]\n\nprint(f'ğŸ“Š Features da classificaÃ§Ã£o (sem leakage): {len(features_classificacao)}')\nprint(f'ğŸ“Š Features atuais disponÃ­veis para regressÃ£o: {len(features_atuais_disponiveis)}')\n\nif features_atuais_disponiveis:\n    print('\\nâœ… FEATURES EXTRAS PARA REGRESSÃƒO (com contexto atual):')\n    for feat in features_atuais_disponiveis:\n        print(f'   â€¢ {feat}')\n    features_extras_para_regressao = features_atuais_disponiveis\nelse:\n    print('\\nğŸ“ Nenhuma feature \"atual\" encontrada - usando mesmo conjunto da classificaÃ§Ã£o')\n\n# Combinar features: classificaÃ§Ã£o (seguras) + extras para regressÃ£o\nfeatures_modelo_regressao = features_classificacao + features_extras_para_regressao\n\nprint(f'\\nğŸ“Š Total de features para regressÃ£o: {len(features_modelo_regressao)}')\nprint(f'   ğŸ“Š Features bÃ¡sicas (da classificaÃ§Ã£o): {len(features_classificacao)}')\nprint(f'   ğŸ“Š Features extras (contexto atual): {len(features_extras_para_regressao)}')\n\n# Preparar datasets de regressÃ£o\nX_train_reg = train_sales[features_modelo_regressao].copy()\ny_train_reg = train_sales['quantidade'].copy()\n\nX_val_reg = validation_sales[features_modelo_regressao].copy()\ny_val_reg = validation_sales['quantidade'].copy()\n\nprint(f'\\nğŸ“Š Datasets de regressÃ£o preparados:')\nprint(f'   ğŸ‹ï¸ X_train_reg: {X_train_reg.shape}, y_train_reg: {y_train_reg.shape}')\nprint(f'   ğŸ” X_val_reg: {X_val_reg.shape}, y_val_reg: {y_val_reg.shape}')\n\n# Verificar se hÃ¡ NAs\nnas_train = X_train_reg.isnull().sum().sum()\nnas_val = X_val_reg.isnull().sum().sum()\nprint(f'   ğŸ§¹ NAs no treino: {nas_train}, NAs na validaÃ§Ã£o: {nas_val}')\n\nif nas_train > 0 or nas_val > 0:\n    print('   âš ï¸ Preenchendo NAs com 0...')\n    X_train_reg = X_train_reg.fillna(0)\n    X_val_reg = X_val_reg.fillna(0)\n\n# Verificar outliers extremos no target\nq99 = y_train_reg.quantile(0.99)\noutliers = (y_train_reg > q99).sum()\nprint(f'\\nğŸ“Š AnÃ¡lise de outliers:')\nprint(f'   â€¢ P99 da quantidade: {q99:.0f}')\nprint(f'   â€¢ Outliers acima P99: {outliers:,} ({outliers/len(y_train_reg)*100:.2f}%)')\n\n# Para regressÃ£o, manter outliers Ã© importante para capturar vendas grandes\nprint('   ğŸ’¡ Mantendo outliers para preservar informaÃ§Ã£o real de vendas grandes')\n\nprint('âœ… Dados preparados para regressÃ£o com features otimizadas')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento do Modelo LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Treinando LightGBM Regressor...\n",
      "   ğŸ“š Iniciando treinamento...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's l1: 0.603397\tvalid_0's rmse: 5.12892\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l1: 0.603397\tvalid_0's rmse: 5.12892\n",
      "âœ… Modelo treinado! Melhor iteraÃ§Ã£o: 100\n",
      "ğŸ“Š Score de validaÃ§Ã£o: defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', np.float64(0.6033969145163481)), ('rmse', np.float64(5.128924904561696))])})\n"
     ]
    }
   ],
   "source": [
    "# Configurar e treinar LightGBM Regressor\n",
    "print('ğŸš€ Treinando LightGBM Regressor...')\n",
    "\n",
    "# ParÃ¢metros otimizados para regressÃ£o\n",
    "lgbm_reg_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 80,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Criar modelo\n",
    "lgbm_regressor = lgb.LGBMRegressor(**lgbm_reg_params)\n",
    "\n",
    "# Treinar modelo com callbacks para early stopping\n",
    "print('   ğŸ“š Iniciando treinamento...')\n",
    "lgbm_regressor.fit(\n",
    "    X_train_reg, y_train_reg,\n",
    "    eval_set=[(X_val_reg, y_val_reg)],\n",
    "    eval_metric=['rmse', 'mae'],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f'âœ… Modelo treinado! Melhor iteraÃ§Ã£o: {lgbm_regressor.best_iteration_}')\n",
    "print(f'ğŸ“Š Score de validaÃ§Ã£o: {lgbm_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AvaliaÃ§Ã£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsÃµes\n",
    "print('ğŸ” Avaliando modelo de regressÃ£o...')\n",
    "\n",
    "# PrevisÃµes\n",
    "y_pred_train_reg = lgbm_regressor.predict(X_train_reg)\n",
    "y_pred_val_reg = lgbm_regressor.predict(X_val_reg)\n",
    "\n",
    "# Garantir que previsÃµes nÃ£o sejam negativas\n",
    "y_pred_train_reg = np.maximum(y_pred_train_reg, 0)\n",
    "y_pred_val_reg = np.maximum(y_pred_val_reg, 0)\n",
    "\n",
    "# FunÃ§Ã£o para calcular MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n",
    "\n",
    "# MÃ©tricas de treino\n",
    "print('\\nğŸ“Š MÃ‰TRICAS DE TREINO:')\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_reg, y_pred_train_reg))\n",
    "mae_train = mean_absolute_error(y_train_reg, y_pred_train_reg)\n",
    "r2_train = r2_score(y_train_reg, y_pred_train_reg)\n",
    "mape_train = mean_absolute_percentage_error(y_train_reg, y_pred_train_reg)\n",
    "\n",
    "print(f'   ğŸ“Š RMSE: {rmse_train:.4f}')\n",
    "print(f'   ğŸ“Š MAE: {mae_train:.4f}')\n",
    "print(f'   ğŸ“Š RÂ²: {r2_train:.4f}')\n",
    "print(f'   ğŸ“Š MAPE: {mape_train:.2f}%')\n",
    "\n",
    "# MÃ©tricas de validaÃ§Ã£o\n",
    "print('\\nğŸ“Š MÃ‰TRICAS DE VALIDAÃ‡ÃƒO:')\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val_reg, y_pred_val_reg))\n",
    "mae_val = mean_absolute_error(y_val_reg, y_pred_val_reg)\n",
    "r2_val = r2_score(y_val_reg, y_pred_val_reg)\n",
    "mape_val = mean_absolute_percentage_error(y_val_reg, y_pred_val_reg)\n",
    "\n",
    "print(f'   ğŸ“Š RMSE: {rmse_val:.4f}')\n",
    "print(f'   ğŸ“Š MAE: {mae_val:.4f}')\n",
    "print(f'   ğŸ“Š RÂ²: {r2_val:.4f}')\n",
    "print(f'   ğŸ“Š MAPE: {mape_val:.2f}%')\n",
    "\n",
    "# AnÃ¡lise de resÃ­duos\n",
    "print('\\nğŸ“Š ANÃLISE DE RESÃDUOS (ValidaÃ§Ã£o):')\n",
    "residuos = y_val_reg - y_pred_val_reg\n",
    "print(f'   ğŸ“Š MÃ©dia dos resÃ­duos: {residuos.mean():.4f}')\n",
    "print(f'   ğŸ“Š Desvio padrÃ£o dos resÃ­duos: {residuos.std():.4f}')\n",
    "print(f'   ğŸ“Š P25 dos resÃ­duos: {residuos.quantile(0.25):.4f}')\n",
    "print(f'   ğŸ“Š P75 dos resÃ­duos: {residuos.quantile(0.75):.4f}')\n",
    "\n",
    "# AnÃ¡lise por faixas de quantidade\n",
    "print('\\nğŸ“Š PERFORMANCE POR FAIXA DE QUANTIDADE:')\n",
    "faixas = [(0, 5), (5, 10), (10, 20), (20, 50), (50, 1000)]\n",
    "for faixa_min, faixa_max in faixas:\n",
    "    mask = (y_val_reg >= faixa_min) & (y_val_reg < faixa_max)\n",
    "    if mask.sum() > 0:\n",
    "        mae_faixa = mean_absolute_error(y_val_reg[mask], y_pred_val_reg[mask])\n",
    "        count_faixa = mask.sum()\n",
    "        print(f'   Faixa [{faixa_min:3d}, {faixa_max:3d}): MAE={mae_faixa:6.2f}, Count={count_faixa:6,}')\n",
    "\n",
    "print('âœ… AvaliaÃ§Ã£o do modelo de regressÃ£o concluÃ­da')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AnÃ¡lise de ImportÃ¢ncia das Features (RegressÃ£o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar importÃ¢ncia das features para regressÃ£o\n",
    "print('ğŸ“Š Analisando importÃ¢ncia das features para regressÃ£o...')\n",
    "\n",
    "# Obter importÃ¢ncias\n",
    "feature_importance_reg = lgbm_regressor.feature_importances_\n",
    "feature_names_reg = X_train_reg.columns\n",
    "\n",
    "# Criar DataFrame de importÃ¢ncias\n",
    "importance_reg_df = pd.DataFrame({\n",
    "    'feature': feature_names_reg,\n",
    "    'importance': feature_importance_reg\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features mais importantes para regressÃ£o\n",
    "print('\\nğŸ† TOP 20 FEATURES MAIS IMPORTANTES (REGRESSÃƒO):')\n",
    "for i, (_, row) in enumerate(importance_reg_df.head(20).iterrows()):\n",
    "    print(f'   {i+1:2d}. {row[\"feature\"]:30s} - {row[\"importance\"]:6.0f}')\n",
    "\n",
    "# Comparar com importÃ¢ncias da classificaÃ§Ã£o\n",
    "print('\\nğŸ“Š COMPARANDO COM CLASSIFICAÃ‡ÃƒO:')\n",
    "classification_importance = pd.read_csv('../data/submissao3/classification_feature_importance.csv')\n",
    "\n",
    "# Top 10 features de cada modelo\n",
    "top_classification = classification_importance.head(10)['feature'].tolist()\n",
    "top_regression = importance_reg_df.head(10)['feature'].tolist()\n",
    "\n",
    "features_em_comum = set(top_classification).intersection(set(top_regression))\n",
    "print(f'   ğŸ“Š Features em comum no Top 10: {len(features_em_comum)}/10')\n",
    "print(f'   ğŸ“Š Features comuns: {list(features_em_comum)}')\n",
    "\n",
    "# AnÃ¡lise por categoria de feature para regressÃ£o\n",
    "print('\\nğŸ“Š IMPORTÃ‚NCIA POR CATEGORIA (REGRESSÃƒO):')\n",
    "categorias_feature = {\n",
    "    'Lag': [f for f in feature_names_reg if 'lag_' in f],\n",
    "    'Rolling': [f for f in feature_names_reg if any(x in f for x in ['media_4w', 'std_4w', 'max_4w'])],\n",
    "    'PreÃ§o': [f for f in feature_names_reg if 'preco' in f],\n",
    "    'CalendÃ¡rio': [f for f in feature_names_reg if any(x in f for x in ['mes', 'dia', 'inicio', 'fim'])],\n",
    "    'TendÃªncia': [f for f in feature_names_reg if any(x in f for x in ['momentum', 'aceleracao'])],\n",
    "    'Hierarquia': [f for f in feature_names_reg if any(x in f for x in ['media_vendas', 'share'])],\n",
    "    'Hash': [f for f in feature_names_reg if 'hash' in f]\n",
    "}\n",
    "\n",
    "for categoria, features in categorias_feature.items():\n",
    "    if features:\n",
    "        importancia_categoria = importance_reg_df[importance_reg_df['feature'].isin(features)]['importance'].sum()\n",
    "        print(f'   {categoria:12s}: {importancia_categoria:8.0f} ({len(features)} features)')\n",
    "\n",
    "print('âœ… AnÃ¡lise de importÃ¢ncia para regressÃ£o concluÃ­da')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvamento do Modelo de RegressÃ£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Salvar modelo de regressÃ£o OTIMIZADO\nprint('ğŸ’¾ Salvando modelo de regressÃ£o OTIMIZADO...')\n\n# Salvar modelo\nwith open('../data/submissao3/lgbm_regressor.pkl', 'wb') as f:\n    pickle.dump(lgbm_regressor, f)\n\n# IMPORTANTE: Salvar lista de features da regressÃ£o (pode ser diferente da classificaÃ§Ã£o)\nwith open('../data/submissao3/regression_features.pkl', 'wb') as f:\n    pickle.dump(features_modelo_regressao, f)\n\n# Salvar importÃ¢ncias da regressÃ£o\nimportance_reg_df.to_csv('../data/submissao3/regression_feature_importance.csv', index=False)\n\n# Salvar metadados do modelo de regressÃ£o OTIMIZADO\nmetadados_regressor = {\n    'data_criacao': pd.Timestamp.now(),\n    'modelo': 'LightGBM Regressor OTIMIZADO',\n    'objetivo': 'RegressÃ£o: prever quantidade de vendas (apenas onde vendeu = 1)',\n    'parametros': lgbm_reg_params,\n    'melhor_iteracao': lgbm_regressor.best_iteration_,\n    'melhor_score': lgbm_regressor.best_score_,\n    'total_features': len(features_modelo_regressao),\n    'features_usadas': features_modelo_regressao,\n    'features_da_classificacao': len(features_classificacao),\n    'features_extras_regressao': len(features_extras_para_regressao),\n    'strategy': {\n        'base_features': 'Mesmas da classificaÃ§Ã£o (sem leakage)',\n        'extra_features': 'Features com contexto atual (permitidas na regressÃ£o)',\n        'reasoning': 'RegressÃ£o pode usar contexto atual pois jÃ¡ sabe que vendeu=1'\n    },\n    'metricas_validacao': {\n        'rmse': rmse_val,\n        'mae': mae_val,\n        'r2': r2_val,\n        'mape': mape_val\n    },\n    'shape_treino': X_train_reg.shape,\n    'shape_validacao': X_val_reg.shape,\n    'target_stats_treino': {\n        'mean': float(y_train_reg.mean()),\n        'median': float(y_train_reg.median()),\n        'std': float(y_train_reg.std()),\n        'min': float(y_train_reg.min()),\n        'max': float(y_train_reg.max())\n    },\n    'target_stats_validacao': {\n        'mean': float(y_val_reg.mean()),\n        'median': float(y_val_reg.median()),\n        'std': float(y_val_reg.std()),\n        'min': float(y_val_reg.min()),\n        'max': float(y_val_reg.max())\n    },\n    'observacoes': [\n        'Modelo treinado APENAS em registros com vendeu=1',\n        'Pode usar features \"atuais\" pois nÃ£o hÃ¡ leakage na regressÃ£o',\n        'PrevisÃµes sÃ£o clampadas para >= 0',\n        'Otimizado para casos com vendas positivas',\n        'Preserva outliers para capturar vendas grandes',\n        'Deve ser usado em conjunto com classificador'\n    ]\n}\n\nwith open('../data/submissao3/lgbm_regressor_metadata.pkl', 'wb') as f:\n    pickle.dump(metadados_regressor, f)\n\nprint('âœ… Arquivos salvos:')\nprint('   â€¢ data/submissao3/lgbm_regressor.pkl')\nprint('   â€¢ data/submissao3/regression_features.pkl (SEPARADO da classificaÃ§Ã£o)')\nprint('   â€¢ data/submissao3/regression_feature_importance.csv')\nprint('   â€¢ data/submissao3/lgbm_regressor_metadata.pkl')\n\nprint('\\nğŸ‰ MODELO DE REGRESSÃƒO OTIMIZADO TREINADO E SALVO!')\nprint('=' * 70)\nprint('ğŸ¯ Resumo do Modelo de RegressÃ£o OTIMIZADO:')\nprint(f'   ğŸ“Š RMSE na validaÃ§Ã£o: {rmse_val:.4f}')\nprint(f'   ğŸ“Š MAE na validaÃ§Ã£o: {mae_val:.4f}')\nprint(f'   ğŸ“Š RÂ² na validaÃ§Ã£o: {r2_val:.4f}')\nprint(f'   ğŸ“Š MAPE na validaÃ§Ã£o: {mape_val:.2f}%')\nprint(f'   ğŸ“Š Features totais: {len(features_modelo_regressao)}')\nprint(f'     â€¢ Features base (sem leakage): {len(features_classificacao)}')\nprint(f'     â€¢ Features extras (contexto atual): {len(features_extras_para_regressao)}')\nprint(f'   ğŸ“Š Registros de treino: {len(X_train_reg):,} (apenas com vendas)')\nprint(f'   ğŸ“Š Registros de validaÃ§Ã£o: {len(X_val_reg):,} (apenas com vendas)')\n\nprint('\\nğŸ’¡ IMPORTANTE:')\nprint('   ğŸ”„ ClassificaÃ§Ã£o e RegressÃ£o usam features DIFERENTES')\nprint('   ğŸ“Š ClassificaÃ§Ã£o: features seguras (sem leakage)')  \nprint('   ğŸ“Š RegressÃ£o: features seguras + contexto atual')\nprint('   ğŸ¯ Isso Ã© CORRETO e esperado!')\n\nprint('\\nğŸ’¡ PrÃ³ximo passo:')\nprint('   ğŸ”„ Atualizar pipeline final (Notebook 13) para usar features corretas')\nprint('   ğŸ“Š Aplicar modelos nas 5 semanas de Janeiro 2023')\n\nprint('\\nğŸš€ Segundo estÃ¡gio OTIMIZADO concluÃ­do!')\nprint('ğŸ¯ Ambos os modelos estÃ£o prontos e corrigidos!')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}