{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Modelo de Regressão (Prever QUANTO vai vender)\n",
    "\n",
    "Este notebook treina o segundo estágio do modelo de dois estágios: **regressão**.\n",
    "\n",
    "## Objetivo:\n",
    "- Prever a quantidade de vendas **apenas para casos onde vendeu = 1**\n",
    "- Usar LightGBM Regressor especializado em prever volumes\n",
    "- Avaliar com métricas de regressão (RMSE, MAE, MAPE)\n",
    "- Otimizar para casos com vendas positivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Iniciando Treinamento do Modelo de Regressão\n",
      "🎯 Objetivo: Prever QUANTO vai vender (apenas onde vendeu = 1)\n",
      "📁 Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, \n",
    "    r2_score\n",
    ")\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('📊 Iniciando Treinamento do Modelo de Regressão')\n",
    "print('🎯 Objetivo: Prever QUANTO vai vender (apenas onde vendeu = 1)')\n",
    "\n",
    "# Criar pasta submissao3 se não existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('📁 Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados com Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados com features avançadas...\n",
      "🏋️ Dados de treino (completos): (50126880, 54)\n",
      "🔍 Dados de validação (completos): (5221550, 54)\n",
      "\n",
      "✂️ APLICANDO FILTRO CRÍTICO: apenas registros com vendas...\n",
      "🏋️ Dados de treino (apenas vendas): (5543608, 54)\n",
      "🔍 Dados de validação (apenas vendas): (571729, 54)\n",
      "\n",
      "📊 Estatísticas do target \"quantidade\" no treino:\n",
      "   • Média: 9.10\n",
      "   • Mediana: 2.00\n",
      "   • Desvio padrão: 87.39\n",
      "   • Min: 1\n",
      "   • Max: 94230\n",
      "\n",
      "📊 Estatísticas do target \"quantidade\" na validação:\n",
      "   • Média: 5.26\n",
      "   • Mediana: 2.00\n",
      "   • Desvio padrão: 15.82\n",
      "   • Min: 1\n",
      "   • Max: 2472\n",
      "✅ Dados filtrados para regressão\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com features avançadas\n",
    "print('📂 Carregando dados com features avançadas...')\n",
    "\n",
    "train_features = pd.read_parquet('../data/submissao3/train_features.parquet')\n",
    "validation_features = pd.read_parquet('../data/submissao3/validation_features.parquet')\n",
    "\n",
    "print(f'🏋️ Dados de treino (completos): {train_features.shape}')\n",
    "print(f'🔍 Dados de validação (completos): {validation_features.shape}')\n",
    "\n",
    "# FILTRO CRÍTICO: Manter apenas registros onde vendeu = 1\n",
    "print('\\n✂️ APLICANDO FILTRO CRÍTICO: apenas registros com vendas...')\n",
    "\n",
    "train_sales = train_features[train_features['vendeu'] == 1].copy()\n",
    "validation_sales = validation_features[validation_features['vendeu'] == 1].copy()\n",
    "\n",
    "print(f'🏋️ Dados de treino (apenas vendas): {train_sales.shape}')\n",
    "print(f'🔍 Dados de validação (apenas vendas): {validation_sales.shape}')\n",
    "\n",
    "# Verificar distribuição do target de regressão\n",
    "print(f'\\n📊 Estatísticas do target \"quantidade\" no treino:')\n",
    "print(f'   • Média: {train_sales[\"quantidade\"].mean():.2f}')\n",
    "print(f'   • Mediana: {train_sales[\"quantidade\"].median():.2f}')\n",
    "print(f'   • Desvio padrão: {train_sales[\"quantidade\"].std():.2f}')\n",
    "print(f'   • Min: {train_sales[\"quantidade\"].min():.0f}')\n",
    "print(f'   • Max: {train_sales[\"quantidade\"].max():.0f}')\n",
    "\n",
    "print(f'\\n📊 Estatísticas do target \"quantidade\" na validação:')\n",
    "print(f'   • Média: {validation_sales[\"quantidade\"].mean():.2f}')\n",
    "print(f'   • Mediana: {validation_sales[\"quantidade\"].median():.2f}')\n",
    "print(f'   • Desvio padrão: {validation_sales[\"quantidade\"].std():.2f}')\n",
    "print(f'   • Min: {validation_sales[\"quantidade\"].min():.0f}')\n",
    "print(f'   • Max: {validation_sales[\"quantidade\"].max():.0f}')\n",
    "\n",
    "print('✅ Dados filtrados para regressão')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação dos Dados para Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Preparando features para regressão - VERSÃO OTIMIZADA...\n",
      "📊 Features da classificação (sem leakage): 41\n",
      "📊 Features atuais disponíveis para regressão: 4\n",
      "\n",
      "✅ FEATURES EXTRAS PARA REGRESSÃO (com contexto atual):\n",
      "   • preco_unitario_atual\n",
      "   • preco_medio_semanal_sku_atual\n",
      "   • media_vendas_categoria_pdv_atual\n",
      "   • share_vendas_sku_categoria_atual\n",
      "\n",
      "📊 Total de features para regressão: 45\n",
      "   📊 Features básicas (da classificação): 41\n",
      "   📊 Features extras (contexto atual): 4\n",
      "\n",
      "📊 Datasets de regressão preparados:\n",
      "   🏋️ X_train_reg: (5543608, 45), y_train_reg: (5543608,)\n",
      "   🔍 X_val_reg: (571729, 45), y_val_reg: (571729,)\n",
      "   🧹 NAs no treino: 0, NAs na validação: 0\n",
      "\n",
      "📊 Análise de outliers:\n",
      "   • P99 da quantidade: 132\n",
      "   • Outliers acima P99: 53,858 (0.97%)\n",
      "   💡 Mantendo outliers para preservar informação real de vendas grandes\n",
      "✅ Dados preparados para regressão com features otimizadas\n"
     ]
    }
   ],
   "source": [
    "# Preparar features para regressão - VERSÃO OTIMIZADA\n",
    "print('🔧 Preparando features para regressão - VERSÃO OTIMIZADA...')\n",
    "\n",
    "# Carregar lista de features seguras (sem leakage) da classificação\n",
    "with open('../data/submissao3/classification_features.pkl', 'rb') as f:\n",
    "    features_classificacao = pickle.load(f)\n",
    "\n",
    "# Para REGRESSÃO, podemos usar algumas features \"atuais\" que foram removidas da classificação\n",
    "# Porque quando fazemos regressão, já sabemos que vendeu=1, então não há leakage\n",
    "features_extras_para_regressao = []\n",
    "\n",
    "# Verificar se features \"atuais\" existem nos dados\n",
    "todas_features = list(train_sales.columns)\n",
    "features_atuais_disponiveis = [f for f in todas_features if 'atual' in f]\n",
    "\n",
    "print(f'📊 Features da classificação (sem leakage): {len(features_classificacao)}')\n",
    "print(f'📊 Features atuais disponíveis para regressão: {len(features_atuais_disponiveis)}')\n",
    "\n",
    "if features_atuais_disponiveis:\n",
    "    print('\\n✅ FEATURES EXTRAS PARA REGRESSÃO (com contexto atual):')\n",
    "    for feat in features_atuais_disponiveis:\n",
    "        print(f'   • {feat}')\n",
    "    features_extras_para_regressao = features_atuais_disponiveis\n",
    "else:\n",
    "    print('\\n📝 Nenhuma feature \"atual\" encontrada - usando mesmo conjunto da classificação')\n",
    "\n",
    "# Combinar features: classificação (seguras) + extras para regressão\n",
    "features_modelo_regressao = features_classificacao + features_extras_para_regressao\n",
    "\n",
    "print(f'\\n📊 Total de features para regressão: {len(features_modelo_regressao)}')\n",
    "print(f'   📊 Features básicas (da classificação): {len(features_classificacao)}')\n",
    "print(f'   📊 Features extras (contexto atual): {len(features_extras_para_regressao)}')\n",
    "\n",
    "# Preparar datasets de regressão\n",
    "X_train_reg = train_sales[features_modelo_regressao].copy()\n",
    "y_train_reg = train_sales['quantidade'].copy()\n",
    "\n",
    "X_val_reg = validation_sales[features_modelo_regressao].copy()\n",
    "y_val_reg = validation_sales['quantidade'].copy()\n",
    "\n",
    "print(f'\\n📊 Datasets de regressão preparados:')\n",
    "print(f'   🏋️ X_train_reg: {X_train_reg.shape}, y_train_reg: {y_train_reg.shape}')\n",
    "print(f'   🔍 X_val_reg: {X_val_reg.shape}, y_val_reg: {y_val_reg.shape}')\n",
    "\n",
    "# Verificar se há NAs\n",
    "nas_train = X_train_reg.isnull().sum().sum()\n",
    "nas_val = X_val_reg.isnull().sum().sum()\n",
    "print(f'   🧹 NAs no treino: {nas_train}, NAs na validação: {nas_val}')\n",
    "\n",
    "if nas_train > 0 or nas_val > 0:\n",
    "    print('   ⚠️ Preenchendo NAs com 0...')\n",
    "    X_train_reg = X_train_reg.fillna(0)\n",
    "    X_val_reg = X_val_reg.fillna(0)\n",
    "\n",
    "# Verificar outliers extremos no target\n",
    "q99 = y_train_reg.quantile(0.99)\n",
    "outliers = (y_train_reg > q99).sum()\n",
    "print(f'\\n📊 Análise de outliers:')\n",
    "print(f'   • P99 da quantidade: {q99:.0f}')\n",
    "print(f'   • Outliers acima P99: {outliers:,} ({outliers/len(y_train_reg)*100:.2f}%)')\n",
    "\n",
    "# Para regressão, manter outliers é importante para capturar vendas grandes\n",
    "print('   💡 Mantendo outliers para preservar informação real de vendas grandes')\n",
    "\n",
    "print('✅ Dados preparados para regressão com features otimizadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento do Modelo LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Treinando LightGBM Regressor...\n",
      "   📚 Iniciando treinamento...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 4.78741\tvalid_0's rmse: 13.1673\n",
      "✅ Modelo treinado! Melhor iteração: 18\n",
      "📊 Score de validação: defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', np.float64(4.787410554597628)), ('rmse', np.float64(13.167331659008351))])})\n"
     ]
    }
   ],
   "source": [
    "# Configurar e treinar LightGBM Regressor\n",
    "print('🚀 Treinando LightGBM Regressor...')\n",
    "\n",
    "# Parâmetros otimizados para regressão\n",
    "lgbm_reg_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 80,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 50,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Criar modelo\n",
    "lgbm_regressor = lgb.LGBMRegressor(**lgbm_reg_params)\n",
    "\n",
    "# Treinar modelo com callbacks para early stopping\n",
    "print('   📚 Iniciando treinamento...')\n",
    "lgbm_regressor.fit(\n",
    "    X_train_reg, y_train_reg,\n",
    "    eval_set=[(X_val_reg, y_val_reg)],\n",
    "    eval_metric=['rmse', 'mae'],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f'✅ Modelo treinado! Melhor iteração: {lgbm_regressor.best_iteration_}')\n",
    "print(f'📊 Score de validação: {lgbm_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Avaliando modelo de regressão...\n",
      "\n",
      "📊 MÉTRICAS DE TREINO:\n",
      "   📊 RMSE: 68.8696\n",
      "   📊 MAE: 6.9484\n",
      "   📊 R²: 0.3790\n",
      "   📊 MAPE: 240.43%\n",
      "\n",
      "📊 MÉTRICAS DE VALIDAÇÃO:\n",
      "   📊 RMSE: 13.1673\n",
      "   📊 MAE: 4.7874\n",
      "   📊 R²: 0.3072\n",
      "   📊 MAPE: 230.63%\n",
      "\n",
      "📊 ANÁLISE DE RESÍDUOS (Validação):\n",
      "   📊 Média dos resíduos: -1.0409\n",
      "   📊 Desvio padrão dos resíduos: 13.1261\n",
      "   📊 P25 dos resíduos: -4.1151\n",
      "   📊 P75 dos resíduos: -1.5627\n",
      "\n",
      "📊 PERFORMANCE POR FAIXA DE QUANTIDADE:\n",
      "   Faixa [  0,   5): MAE=  3.69, Count=426,941\n",
      "   Faixa [  5,  10): MAE=  1.28, Count=67,118\n",
      "   Faixa [ 10,  20): MAE=  4.00, Count=49,800\n",
      "   Faixa [ 20,  50): MAE= 16.16, Count=21,294\n",
      "   Faixa [ 50, 1000): MAE= 79.99, Count= 6,563\n",
      "✅ Avaliação do modelo de regressão concluída\n"
     ]
    }
   ],
   "source": [
    "# Fazer previsões\n",
    "print('🔍 Avaliando modelo de regressão...')\n",
    "\n",
    "# Previsões\n",
    "y_pred_train_reg = lgbm_regressor.predict(X_train_reg)\n",
    "y_pred_val_reg = lgbm_regressor.predict(X_val_reg)\n",
    "\n",
    "# Garantir que previsões não sejam negativas\n",
    "y_pred_train_reg = np.maximum(y_pred_train_reg, 0)\n",
    "y_pred_val_reg = np.maximum(y_pred_val_reg, 0)\n",
    "\n",
    "# Função para calcular MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n",
    "\n",
    "# Métricas de treino\n",
    "print('\\n📊 MÉTRICAS DE TREINO:')\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_reg, y_pred_train_reg))\n",
    "mae_train = mean_absolute_error(y_train_reg, y_pred_train_reg)\n",
    "r2_train = r2_score(y_train_reg, y_pred_train_reg)\n",
    "mape_train = mean_absolute_percentage_error(y_train_reg, y_pred_train_reg)\n",
    "\n",
    "print(f'   📊 RMSE: {rmse_train:.4f}')\n",
    "print(f'   📊 MAE: {mae_train:.4f}')\n",
    "print(f'   📊 R²: {r2_train:.4f}')\n",
    "print(f'   📊 MAPE: {mape_train:.2f}%')\n",
    "\n",
    "# Métricas de validação\n",
    "print('\\n📊 MÉTRICAS DE VALIDAÇÃO:')\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val_reg, y_pred_val_reg))\n",
    "mae_val = mean_absolute_error(y_val_reg, y_pred_val_reg)\n",
    "r2_val = r2_score(y_val_reg, y_pred_val_reg)\n",
    "mape_val = mean_absolute_percentage_error(y_val_reg, y_pred_val_reg)\n",
    "\n",
    "print(f'   📊 RMSE: {rmse_val:.4f}')\n",
    "print(f'   📊 MAE: {mae_val:.4f}')\n",
    "print(f'   📊 R²: {r2_val:.4f}')\n",
    "print(f'   📊 MAPE: {mape_val:.2f}%')\n",
    "\n",
    "# Análise de resíduos\n",
    "print('\\n📊 ANÁLISE DE RESÍDUOS (Validação):')\n",
    "residuos = y_val_reg - y_pred_val_reg\n",
    "print(f'   📊 Média dos resíduos: {residuos.mean():.4f}')\n",
    "print(f'   📊 Desvio padrão dos resíduos: {residuos.std():.4f}')\n",
    "print(f'   📊 P25 dos resíduos: {residuos.quantile(0.25):.4f}')\n",
    "print(f'   📊 P75 dos resíduos: {residuos.quantile(0.75):.4f}')\n",
    "\n",
    "# Análise por faixas de quantidade\n",
    "print('\\n📊 PERFORMANCE POR FAIXA DE QUANTIDADE:')\n",
    "faixas = [(0, 5), (5, 10), (10, 20), (20, 50), (50, 1000)]\n",
    "for faixa_min, faixa_max in faixas:\n",
    "    mask = (y_val_reg >= faixa_min) & (y_val_reg < faixa_max)\n",
    "    if mask.sum() > 0:\n",
    "        mae_faixa = mean_absolute_error(y_val_reg[mask], y_pred_val_reg[mask])\n",
    "        count_faixa = mask.sum()\n",
    "        print(f'   Faixa [{faixa_min:3d}, {faixa_max:3d}): MAE={mae_faixa:6.2f}, Count={count_faixa:6,}')\n",
    "\n",
    "print('✅ Avaliação do modelo de regressão concluída')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análise de Importância das Features (Regressão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Analisando importância das features para regressão...\n",
      "\n",
      "🏆 TOP 20 FEATURES MAIS IMPORTANTES (REGRESSÃO):\n",
      "    1. share_vendas_sku_categoria_atual -    342\n",
      "    2. produto_hash                   -    175\n",
      "    3. media_vendas_categoria_pdv_atual -    159\n",
      "    4. preco_medio_semanal_sku_atual  -    133\n",
      "    5. preco_relativo_pdv             -     93\n",
      "    6. media_vendas_categoria_pdv_lag_1 -     82\n",
      "    7. preco_lag_1                    -     73\n",
      "    8. preco_relativo_categoria       -     60\n",
      "    9. categoria_zipcode_hash         -     54\n",
      "   10. categoria_hash                 -     37\n",
      "   11. zipcode_hash                   -     33\n",
      "   12. preco_ewma_4w                  -     32\n",
      "   13. pdv_hash                       -     30\n",
      "   14. preco_unitario_atual           -     24\n",
      "   15. quantidade_ewma_8w             -     21\n",
      "   16. preco_lag_2                    -     16\n",
      "   17. pdv_produto_hash               -     14\n",
      "   18. quantidade_max_4w              -     12\n",
      "   19. quantidade_std_4w              -     10\n",
      "   20. quantidade_ewma_4w             -      7\n",
      "\n",
      "📊 COMPARANDO COM CLASSIFICAÇÃO:\n",
      "   📊 Features em comum no Top 10: 0/10\n",
      "   📊 Features comuns: []\n",
      "\n",
      "📊 IMPORTÂNCIA POR CATEGORIA (REGRESSÃO):\n",
      "   Lag         :      172 (8 features)\n",
      "   Rolling     :       22 (3 features)\n",
      "   Preço       :      431 (9 features)\n",
      "   Calendário  :      246 (11 features)\n",
      "   Tendência   :        0 (3 features)\n",
      "   Hierarquia  :      583 (4 features)\n",
      "   Hash        :      343 (6 features)\n",
      "✅ Análise de importância para regressão concluída\n"
     ]
    }
   ],
   "source": [
    "# Analisar importância das features para regressão\n",
    "print('📊 Analisando importância das features para regressão...')\n",
    "\n",
    "# Obter importâncias\n",
    "feature_importance_reg = lgbm_regressor.feature_importances_\n",
    "feature_names_reg = X_train_reg.columns\n",
    "\n",
    "# Criar DataFrame de importâncias\n",
    "importance_reg_df = pd.DataFrame({\n",
    "    'feature': feature_names_reg,\n",
    "    'importance': feature_importance_reg\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features mais importantes para regressão\n",
    "print('\\n🏆 TOP 20 FEATURES MAIS IMPORTANTES (REGRESSÃO):')\n",
    "for i, (_, row) in enumerate(importance_reg_df.head(20).iterrows()):\n",
    "    print(f'   {i+1:2d}. {row[\"feature\"]:30s} - {row[\"importance\"]:6.0f}')\n",
    "\n",
    "# Comparar com importâncias da classificação\n",
    "print('\\n📊 COMPARANDO COM CLASSIFICAÇÃO:')\n",
    "classification_importance = pd.read_csv('../data/submissao3/classification_feature_importance.csv')\n",
    "\n",
    "# Top 10 features de cada modelo\n",
    "top_classification = classification_importance.head(10)['feature'].tolist()\n",
    "top_regression = importance_reg_df.head(10)['feature'].tolist()\n",
    "\n",
    "features_em_comum = set(top_classification).intersection(set(top_regression))\n",
    "print(f'   📊 Features em comum no Top 10: {len(features_em_comum)}/10')\n",
    "print(f'   📊 Features comuns: {list(features_em_comum)}')\n",
    "\n",
    "# Análise por categoria de feature para regressão\n",
    "print('\\n📊 IMPORTÂNCIA POR CATEGORIA (REGRESSÃO):')\n",
    "categorias_feature = {\n",
    "    'Lag': [f for f in feature_names_reg if 'lag_' in f],\n",
    "    'Rolling': [f for f in feature_names_reg if any(x in f for x in ['media_4w', 'std_4w', 'max_4w'])],\n",
    "    'Preço': [f for f in feature_names_reg if 'preco' in f],\n",
    "    'Calendário': [f for f in feature_names_reg if any(x in f for x in ['mes', 'dia', 'inicio', 'fim'])],\n",
    "    'Tendência': [f for f in feature_names_reg if any(x in f for x in ['momentum', 'aceleracao'])],\n",
    "    'Hierarquia': [f for f in feature_names_reg if any(x in f for x in ['media_vendas', 'share'])],\n",
    "    'Hash': [f for f in feature_names_reg if 'hash' in f]\n",
    "}\n",
    "\n",
    "for categoria, features in categorias_feature.items():\n",
    "    if features:\n",
    "        importancia_categoria = importance_reg_df[importance_reg_df['feature'].isin(features)]['importance'].sum()\n",
    "        print(f'   {categoria:12s}: {importancia_categoria:8.0f} ({len(features)} features)')\n",
    "\n",
    "print('✅ Análise de importância para regressão concluída')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvamento do Modelo de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Salvando modelo de regressão OTIMIZADO...\n",
      "✅ Arquivos salvos:\n",
      "   • data/submissao3/lgbm_regressor.pkl\n",
      "   • data/submissao3/regression_features.pkl (SEPARADO da classificação)\n",
      "   • data/submissao3/regression_feature_importance.csv\n",
      "   • data/submissao3/lgbm_regressor_metadata.pkl\n",
      "\n",
      "🎉 MODELO DE REGRESSÃO OTIMIZADO TREINADO E SALVO!\n",
      "======================================================================\n",
      "🎯 Resumo do Modelo de Regressão OTIMIZADO:\n",
      "   📊 RMSE na validação: 13.1673\n",
      "   📊 MAE na validação: 4.7874\n",
      "   📊 R² na validação: 0.3072\n",
      "   📊 MAPE na validação: 230.63%\n",
      "   📊 Features totais: 45\n",
      "     • Features base (sem leakage): 41\n",
      "     • Features extras (contexto atual): 4\n",
      "   📊 Registros de treino: 5,543,608 (apenas com vendas)\n",
      "   📊 Registros de validação: 571,729 (apenas com vendas)\n",
      "\n",
      "💡 IMPORTANTE:\n",
      "   🔄 Classificação e Regressão usam features DIFERENTES\n",
      "   📊 Classificação: features seguras (sem leakage)\n",
      "   📊 Regressão: features seguras + contexto atual\n",
      "   🎯 Isso é CORRETO e esperado!\n",
      "\n",
      "💡 Próximo passo:\n",
      "   🔄 Atualizar pipeline final (Notebook 13) para usar features corretas\n",
      "   📊 Aplicar modelos nas 5 semanas de Janeiro 2023\n",
      "\n",
      "🚀 Segundo estágio OTIMIZADO concluído!\n",
      "🎯 Ambos os modelos estão prontos e corrigidos!\n"
     ]
    }
   ],
   "source": [
    "# Salvar modelo de regressão OTIMIZADO\n",
    "print('💾 Salvando modelo de regressão OTIMIZADO...')\n",
    "\n",
    "# Salvar modelo\n",
    "with open('../data/submissao3/lgbm_regressor.pkl', 'wb') as f:\n",
    "    pickle.dump(lgbm_regressor, f)\n",
    "\n",
    "# IMPORTANTE: Salvar lista de features da regressão (pode ser diferente da classificação)\n",
    "with open('../data/submissao3/regression_features.pkl', 'wb') as f:\n",
    "    pickle.dump(features_modelo_regressao, f)\n",
    "\n",
    "# Salvar importâncias da regressão\n",
    "importance_reg_df.to_csv('../data/submissao3/regression_feature_importance.csv', index=False)\n",
    "\n",
    "# Salvar metadados do modelo de regressão OTIMIZADO\n",
    "metadados_regressor = {\n",
    "    'data_criacao': pd.Timestamp.now(),\n",
    "    'modelo': 'LightGBM Regressor OTIMIZADO',\n",
    "    'objetivo': 'Regressão: prever quantidade de vendas (apenas onde vendeu = 1)',\n",
    "    'parametros': lgbm_reg_params,\n",
    "    'melhor_iteracao': lgbm_regressor.best_iteration_,\n",
    "    'melhor_score': lgbm_regressor.best_score_,\n",
    "    'total_features': len(features_modelo_regressao),\n",
    "    'features_usadas': features_modelo_regressao,\n",
    "    'features_da_classificacao': len(features_classificacao),\n",
    "    'features_extras_regressao': len(features_extras_para_regressao),\n",
    "    'strategy': {\n",
    "        'base_features': 'Mesmas da classificação (sem leakage)',\n",
    "        'extra_features': 'Features com contexto atual (permitidas na regressão)',\n",
    "        'reasoning': 'Regressão pode usar contexto atual pois já sabe que vendeu=1'\n",
    "    },\n",
    "    'metricas_validacao': {\n",
    "        'rmse': rmse_val,\n",
    "        'mae': mae_val,\n",
    "        'r2': r2_val,\n",
    "        'mape': mape_val\n",
    "    },\n",
    "    'shape_treino': X_train_reg.shape,\n",
    "    'shape_validacao': X_val_reg.shape,\n",
    "    'target_stats_treino': {\n",
    "        'mean': float(y_train_reg.mean()),\n",
    "        'median': float(y_train_reg.median()),\n",
    "        'std': float(y_train_reg.std()),\n",
    "        'min': float(y_train_reg.min()),\n",
    "        'max': float(y_train_reg.max())\n",
    "    },\n",
    "    'target_stats_validacao': {\n",
    "        'mean': float(y_val_reg.mean()),\n",
    "        'median': float(y_val_reg.median()),\n",
    "        'std': float(y_val_reg.std()),\n",
    "        'min': float(y_val_reg.min()),\n",
    "        'max': float(y_val_reg.max())\n",
    "    },\n",
    "    'observacoes': [\n",
    "        'Modelo treinado APENAS em registros com vendeu=1',\n",
    "        'Pode usar features \"atuais\" pois não há leakage na regressão',\n",
    "        'Previsões são clampadas para >= 0',\n",
    "        'Otimizado para casos com vendas positivas',\n",
    "        'Preserva outliers para capturar vendas grandes',\n",
    "        'Deve ser usado em conjunto com classificador'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../data/submissao3/lgbm_regressor_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_regressor, f)\n",
    "\n",
    "print('✅ Arquivos salvos:')\n",
    "print('   • data/submissao3/lgbm_regressor.pkl')\n",
    "print('   • data/submissao3/regression_features.pkl (SEPARADO da classificação)')\n",
    "print('   • data/submissao3/regression_feature_importance.csv')\n",
    "print('   • data/submissao3/lgbm_regressor_metadata.pkl')\n",
    "\n",
    "print('\\n🎉 MODELO DE REGRESSÃO OTIMIZADO TREINADO E SALVO!')\n",
    "print('=' * 70)\n",
    "print('🎯 Resumo do Modelo de Regressão OTIMIZADO:')\n",
    "print(f'   📊 RMSE na validação: {rmse_val:.4f}')\n",
    "print(f'   📊 MAE na validação: {mae_val:.4f}')\n",
    "print(f'   📊 R² na validação: {r2_val:.4f}')\n",
    "print(f'   📊 MAPE na validação: {mape_val:.2f}%')\n",
    "print(f'   📊 Features totais: {len(features_modelo_regressao)}')\n",
    "print(f'     • Features base (sem leakage): {len(features_classificacao)}')\n",
    "print(f'     • Features extras (contexto atual): {len(features_extras_para_regressao)}')\n",
    "print(f'   📊 Registros de treino: {len(X_train_reg):,} (apenas com vendas)')\n",
    "print(f'   📊 Registros de validação: {len(X_val_reg):,} (apenas com vendas)')\n",
    "\n",
    "print('\\n💡 IMPORTANTE:')\n",
    "print('   🔄 Classificação e Regressão usam features DIFERENTES')\n",
    "print('   📊 Classificação: features seguras (sem leakage)')  \n",
    "print('   📊 Regressão: features seguras + contexto atual')\n",
    "print('   🎯 Isso é CORRETO e esperado!')\n",
    "\n",
    "print('\\n💡 Próximo passo:')\n",
    "print('   🔄 Atualizar pipeline final (Notebook 13) para usar features corretas')\n",
    "print('   📊 Aplicar modelos nas 5 semanas de Janeiro 2023')\n",
    "\n",
    "print('\\n🚀 Segundo estágio OTIMIZADO concluído!')\n",
    "print('🎯 Ambos os modelos estão prontos e corrigidos!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
