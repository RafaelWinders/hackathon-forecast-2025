{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - OtimizaÃ§Ã£o de HiperparÃ¢metros com Optuna\n",
    "\n",
    "**ğŸ¯ PROPÃ“SITO DESTE NOTEBOOK:**\n",
    "Este notebook implementa otimizaÃ§Ã£o automÃ¡tica de hiperparÃ¢metros usando **Optuna** com validaÃ§Ã£o cruzada temporal robusta. O objetivo Ã© melhorar significativamente o WMAPE atravÃ©s de uma busca inteligente no espaÃ§o de hiperparÃ¢metros.\n",
    "\n",
    "**ğŸ“Š ESTRATÃ‰GIA TÃ‰CNICA:**\n",
    "- **Optuna**: Framework de otimizaÃ§Ã£o bayesiana para busca eficiente de hiperparÃ¢metros\n",
    "- **TimeSeriesSplit**: ValidaÃ§Ã£o cruzada que respeita a natureza temporal dos dados\n",
    "- **WMAPE como objetivo**: MÃ©trica oficial do challenge como funÃ§Ã£o objetivo\n",
    "- **MÃºltiplos folds**: 3 cortes temporais para validaÃ§Ã£o robusta\n",
    "\n",
    "**ğŸš€ EXPECTATIVA DE RESULTADO:**\n",
    "Com mais de 90% de certeza, esta implementaÃ§Ã£o deve reduzir o WMAPE de ~15.25% para **menos de 14%**, representando uma melhoria significativa no pipeline de forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos da OtimizaÃ§Ã£o:\n",
    "1. **InstalaÃ§Ã£o e Setup**: Configurar Optuna e dependÃªncias\n",
    "2. **PreparaÃ§Ã£o dos Dados**: Carregar dados processados com otimizaÃ§Ã£o de memÃ³ria\n",
    "3. **FunÃ§Ã£o Objetivo**: Implementar funÃ§Ã£o que o Optuna irÃ¡ otimizar\n",
    "4. **ValidaÃ§Ã£o Temporal**: Usar TimeSeriesSplit para validaÃ§Ã£o robusta\n",
    "5. **ExecuÃ§Ã£o do Estudo**: Executar otimizaÃ§Ã£o com 30+ trials\n",
    "6. **AnÃ¡lise dos Resultados**: Comparar performance otimizada vs vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from optuna) (1.16.5)\n",
      "Requirement already satisfied: colorlog in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from optuna) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from optuna) (2.0.43)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\rafae\\onedrive\\documentos\\dev\\hackathon-forecast-2025\\venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "âœ… Optuna e TimeSeriesSplit importados com sucesso!\n",
      "ğŸ¯ Iniciando fase de OtimizaÃ§Ã£o de HiperparÃ¢metros\n"
     ]
    }
   ],
   "source": [
    "# InstalaÃ§Ã£o do Optuna (se ainda nÃ£o tiver)\n",
    "!pip install optuna\n",
    "\n",
    "# ImportaÃ§Ãµes essenciais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Novas importaÃ§Ãµes para otimizaÃ§Ã£o\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('âœ… Optuna e TimeSeriesSplit importados com sucesso!')\n",
    "print('ğŸ¯ Iniciando fase de OtimizaÃ§Ã£o de HiperparÃ¢metros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando dados processados...\n",
      "âœ… Todos os arquivos necessÃ¡rios encontrados\n",
      "ğŸ“Š Carregando dataset (parquet)...\n",
      "\n",
      "ğŸ“Š Dados carregados com sucesso:\n",
      "   â€¢ Shape: (51171190, 26)\n",
      "   â€¢ PerÃ­odo: 2022-01-25 00:00:00 atÃ© 2022-12-27 00:00:00\n",
      "   â€¢ Features disponÃ­veis: 26\n",
      "   â€¢ MemÃ³ria: 16045.8 MB\n",
      "   â€¢ EstratÃ©gia: Grid Inteligente com Dask + Polars - Big Data Optimized\n",
      "\n",
      "ğŸ” Metadados do processamento:\n",
      "   â€¢ total_registros: 51171190\n",
      "   â€¢ total_features: 26\n",
      "   â€¢ combinacoes_pdv_produto: 1044310\n",
      "   â€¢ semanas_cobertas: 49\n",
      "   â€¢ periodo_treino: 2022-01-25 00:00:00 a 2022-12-27 00:00:00\n",
      "   â€¢ estrategia: Grid Inteligente com Dask + Polars - Big Data Optimized\n",
      "   â€¢ tecnologia: Dask + Polars for Maximum Performance\n",
      "   â€¢ memoria_otimizada: 9974.253155708313 MB\n",
      "\n",
      "âœ… Pronto para otimizaÃ§Ã£o!\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com features processadas\n",
    "print('ğŸ“‚ Carregando dados processados...')\n",
    "\n",
    "# Verificar se os arquivos essenciais existem\n",
    "import os\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',  # Usar parquet (mais rÃ¡pido)\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print('âŒ Arquivos nÃ£o encontrados:')\n",
    "    for f in missing_files:\n",
    "        print(f'   â€¢ {f}')\n",
    "    print('\\nğŸ”„ Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\n",
    "else:\n",
    "    print('âœ… Todos os arquivos necessÃ¡rios encontrados')\n",
    "    \n",
    "    # Carregar dados principais (usar parquet para velocidade)\n",
    "    print('ğŸ“Š Carregando dataset (parquet)...')\n",
    "    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "    \n",
    "    # Carregar metadados\n",
    "    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    print(f'\\nğŸ“Š Dados carregados com sucesso:')\n",
    "    print(f'   â€¢ Shape: {dados.shape}')\n",
    "    print(f'   â€¢ PerÃ­odo: {dados[\"semana\"].min()} atÃ© {dados[\"semana\"].max()}')\n",
    "    print(f'   â€¢ Features disponÃ­veis: {len(dados.columns)}')\n",
    "    print(f'   â€¢ MemÃ³ria: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   â€¢ EstratÃ©gia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n",
    "    \n",
    "    print(f'\\nğŸ” Metadados do processamento:')\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n",
    "            print(f'   â€¢ {key}: {value}')\n",
    "    \n",
    "    print(f'\\nâœ… Pronto para otimizaÃ§Ã£o!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PreparaÃ§Ã£o dos Dados para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ PreparaÃ§Ã£o dos dados para otimizaÃ§Ã£o:\n",
      "   â€¢ Target: quantidade\n",
      "   â€¢ Features disponÃ­veis: 20\n",
      "   â€¢ Features excluÃ­das: 6\n",
      "\n",
      "âš ï¸ Features com valores missing:\n",
      "   â€¢ distributor_id: 45,202,572 (88.3%)\n",
      "\n",
      "ğŸ§  EstratÃ©gia de Tratamento Inteligente:\n",
      "   â€¢ distributor_id (categÃ³rica): NaN â†’ -1 (venda direta)\n",
      "   â€¢ Features numÃ©ricas: NaN â†’ 0 (ausÃªncia = zero)\n",
      "   â€¢ LightGBM aprenderÃ¡ padrÃµes especÃ­ficos para valores -1/0\n",
      "\n",
      "ğŸ“‹ Features finais para otimizaÃ§Ã£o: 20\n",
      "ğŸ’¡ Missing values serÃ£o tratados como informaÃ§Ã£o, nÃ£o removidos\n"
     ]
    }
   ],
   "source": [
    "# Definir variÃ¡vel target e features\n",
    "target = 'quantidade'\n",
    "\n",
    "# Features a excluir (nÃ£o devem ser usadas para prediÃ§Ã£o)\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana',  # IDs e data\n",
    "    'quantidade',  # Target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informaÃ§Ã£o do futuro\n",
    "]\n",
    "\n",
    "# Identificar features disponÃ­veis\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'ğŸ¯ PreparaÃ§Ã£o dos dados para otimizaÃ§Ã£o:')\n",
    "print(f'   â€¢ Target: {target}')\n",
    "print(f'   â€¢ Features disponÃ­veis: {len(all_features)}')\n",
    "print(f'   â€¢ Features excluÃ­das: {len(exclude_features)}')\n",
    "\n",
    "# Verificar missing values nas features\n",
    "missing_features = dados[all_features].isnull().sum()\n",
    "missing_features = missing_features[missing_features > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\nâš ï¸ Features com valores missing:')\n",
    "    for feature, count in missing_features.head(10).items():\n",
    "        pct = (count / len(dados)) * 100\n",
    "        print(f'   â€¢ {feature}: {count:,} ({pct:.1f}%)')\n",
    "    \n",
    "    print(f'\\nğŸ§  EstratÃ©gia de Tratamento Inteligente:')\n",
    "    print('   â€¢ distributor_id (categÃ³rica): NaN â†’ -1 (venda direta)')\n",
    "    print('   â€¢ Features numÃ©ricas: NaN â†’ 0 (ausÃªncia = zero)')\n",
    "    print('   â€¢ LightGBM aprenderÃ¡ padrÃµes especÃ­ficos para valores -1/0')\n",
    "else:\n",
    "    print('\\nâœ… Nenhum valor missing nas features')\n",
    "\n",
    "print(f'\\nğŸ“‹ Features finais para otimizaÃ§Ã£o: {len(all_features)}')\n",
    "print('ğŸ’¡ Missing values serÃ£o tratados como informaÃ§Ã£o, nÃ£o removidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… OtimizaÃ§Ã£o de MemÃ³ria + PreparaÃ§Ã£o para Optuna\n",
      "ğŸ§  EstratÃ©gia: Downcasting em vez de amostragem (preserva sÃ©ries temporais)\n",
      "\n",
      "ğŸ” ANTES da otimizaÃ§Ã£o:\n",
      "ğŸ’¾ MemÃ³ria total: 15.67 GB\n",
      "\n",
      "ğŸš€ Aplicando Downcasting...\n",
      "   â€¢ quantidade: float64 â†’ float32\n",
      "   â€¢ num_transacoes: float64 â†’ float32\n",
      "   â€¢ mes_sin: float64 â†’ float32\n",
      "   â€¢ mes_cos: float64 â†’ float32\n",
      "   â€¢ quantidade_lag_1: float64 â†’ float32\n",
      "   â€¢ quantidade_lag_2: float64 â†’ float32\n",
      "   â€¢ quantidade_lag_3: float64 â†’ float32\n",
      "   â€¢ quantidade_lag_4: float64 â†’ float32\n",
      "   â€¢ quantidade_media_4w: float64 â†’ float32\n",
      "   â€¢ quantidade_max_4w: float64 â†’ float32\n",
      "   â€¢ quantidade_min_4w: float64 â†’ float32\n",
      "   â€¢ pdv_hash: uint64 â†’ int8\n",
      "   â€¢ produto_hash: uint64 â†’ int8\n",
      "   â€¢ pdv_produto_hash: uint64 â†’ int16\n",
      "   â€¢ hist_mean: float64 â†’ float32\n",
      "   â€¢ hist_std: float64 â†’ float32\n",
      "   â€¢ hist_max: float64 â†’ float32\n",
      "   â€¢ hist_count: uint32 â†’ int8\n",
      "   â€¢ pdv_id: object â†’ category\n",
      "   â€¢ produto_id: object â†’ category\n",
      "   â€¢ distributor_id: object â†’ category\n",
      "âœ… Downcasting concluÃ­do!\n",
      "\n",
      "ğŸ“Š DEPOIS da otimizaÃ§Ã£o:\n",
      "ğŸ’¾ MemÃ³ria total: 4.39 GB\n",
      "ğŸ¯ ReduÃ§Ã£o: 72.0% (11.28 GB economizados)\n",
      "\n",
      "ğŸ“… OrdenaÃ§Ã£o temporal e tratamento de missing values...\n",
      "\n",
      "ğŸ§  Tratamento inteligente de missing values...\n",
      "   â€¢ distributor_id: 45,202,572 NaN â†’ -1 (venda direta)\n",
      "\n",
      "ğŸ¯ Preparando dados para otimizaÃ§Ã£o Optuna...\n",
      "âœ… Dados preparados com sucesso:\n",
      "   â€¢ X shape: (51171190, 20)\n",
      "   â€¢ y shape: (51171190,)\n",
      "   â€¢ MemÃ³ria X: 3513.6 MB\n",
      "   â€¢ Missing values: 0 (deve ser 0)\n",
      "\n",
      "ğŸ‰ DADOS PRONTOS PARA OPTUNA!\n",
      "   âœ… Downcasting: 72.0% menos memÃ³ria\n",
      "   âœ… Missing values tratados\n",
      "   âœ… SÃ©ries temporais preservadas\n",
      "   âœ… Pronto para TimeSeriesSplit\n"
     ]
    }
   ],
   "source": [
    "# OTIMIZAÃ‡ÃƒO DE MEMÃ“RIA + PREPARAÃ‡ÃƒO DOS DADOS\n",
    "print('ğŸ“… OtimizaÃ§Ã£o de MemÃ³ria + PreparaÃ§Ã£o para Optuna')\n",
    "print('ğŸ§  EstratÃ©gia: Downcasting em vez de amostragem (preserva sÃ©ries temporais)')\n",
    "\n",
    "# PASSO 1: Inspecionar uso de memÃ³ria atual\n",
    "print(f'\\nğŸ” ANTES da otimizaÃ§Ã£o:')\n",
    "memory_before = dados.memory_usage(deep=True).sum() / (1024**3)\n",
    "print(f'ğŸ’¾ MemÃ³ria total: {memory_before:.2f} GB')\n",
    "\n",
    "# PASSO 2: Aplicar Downcasting Inteligente\n",
    "print(f'\\nğŸš€ Aplicando Downcasting...')\n",
    "\n",
    "# Fazer uma cÃ³pia para otimizaÃ§Ã£o\n",
    "dados_sorted = dados.copy()\n",
    "\n",
    "# Otimizar colunas numÃ©ricas (inteiros e floats)\n",
    "for col in dados_sorted.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_sorted[col].dtype\n",
    "    \n",
    "    if dados_sorted[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='float')\n",
    "    \n",
    "    new_dtype = dados_sorted[col].dtype\n",
    "    if original_dtype != new_dtype:\n",
    "        print(f'   â€¢ {col}: {original_dtype} â†’ {new_dtype}')\n",
    "\n",
    "# Otimizar colunas categÃ³ricas\n",
    "for col in dados_sorted.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_sorted[col].nunique()\n",
    "        total_rows = len(dados_sorted)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores Ãºnicos, usar category\n",
    "            dados_sorted[col] = dados_sorted[col].astype('category')\n",
    "            print(f'   â€¢ {col}: object â†’ category')\n",
    "\n",
    "print(f'âœ… Downcasting concluÃ­do!')\n",
    "\n",
    "# PASSO 3: Verificar resultado da otimizaÃ§Ã£o\n",
    "memory_after = dados_sorted.memory_usage(deep=True).sum() / (1024**3)\n",
    "memory_reduction = (memory_before - memory_after) / memory_before * 100\n",
    "print(f'\\nğŸ“Š DEPOIS da otimizaÃ§Ã£o:')\n",
    "print(f'ğŸ’¾ MemÃ³ria total: {memory_after:.2f} GB')\n",
    "print(f'ğŸ¯ ReduÃ§Ã£o: {memory_reduction:.1f}% ({memory_before-memory_after:.2f} GB economizados)')\n",
    "\n",
    "# PASSO 4: Ordenar por semana e tratar missing values\n",
    "print(f'\\nğŸ“… OrdenaÃ§Ã£o temporal e tratamento de missing values...')\n",
    "\n",
    "# Ordenar por semana\n",
    "dados_sorted = dados_sorted.sort_values('semana')\n",
    "\n",
    "# Tratamento inteligente de missing values\n",
    "print(f'\\nğŸ§  Tratamento inteligente de missing values...')\n",
    "\n",
    "for col in all_features:\n",
    "    missing_count = dados_sorted[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if col == 'distributor_id':\n",
    "            # Adicionar -1 ao \"menu\" de categorias primeiro\n",
    "            if dados_sorted[col].dtype.name == 'category':\n",
    "                if -1 not in dados_sorted[col].cat.categories:\n",
    "                    dados_sorted[col] = dados_sorted[col].cat.add_categories([-1])\n",
    "            \n",
    "            # Agora pode preencher com -1 sem erro\n",
    "            dados_sorted[col] = dados_sorted[col].fillna(-1)\n",
    "            print(f'   â€¢ {col}: {missing_count:,} NaN â†’ -1 (venda direta)')\n",
    "            \n",
    "        elif dados_sorted[col].dtype.kind in ['i', 'u', 'f']:\n",
    "            # NumÃ©ricas: fillna funciona diretamente\n",
    "            dados_sorted[col] = dados_sorted[col].fillna(0)\n",
    "            print(f'   â€¢ {col}: {missing_count:,} NaN â†’ 0 (ausÃªncia)')\n",
    "\n",
    "# PASSO 5: Preparar dados para Optuna\n",
    "print(f'\\nğŸ¯ Preparando dados para otimizaÃ§Ã£o Optuna...')\n",
    "\n",
    "# Separar os dados em features (X) e alvo (y)\n",
    "X = dados_sorted[all_features]\n",
    "y = dados_sorted[target]\n",
    "\n",
    "print(f'âœ… Dados preparados com sucesso:')\n",
    "print(f'   â€¢ X shape: {X.shape}')\n",
    "print(f'   â€¢ y shape: {y.shape}')\n",
    "print(f'   â€¢ MemÃ³ria X: {X.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "print(f'   â€¢ Missing values: {X.isnull().sum().sum()} (deve ser 0)')\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(f'\\nğŸ‰ DADOS PRONTOS PARA OPTUNA!')\n",
    "print(f'   âœ… Downcasting: {memory_reduction:.1f}% menos memÃ³ria')\n",
    "print(f'   âœ… Missing values tratados')\n",
    "print(f'   âœ… SÃ©ries temporais preservadas')\n",
    "print(f'   âœ… Pronto para TimeSeriesSplit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FunÃ§Ã£o Objetivo do Optuna com TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FunÃ§Ã£o objetivo implementada!\n",
      "ğŸ¯ Esta funÃ§Ã£o irÃ¡ treinar LightGBM com diferentes hiperparÃ¢metros\n",
      "ğŸ“Š TimeSeriesSplit com 3 folds garante validaÃ§Ã£o temporal robusta\n",
      "ğŸ” WMAPE como mÃ©trica objetivo (oficial do challenge)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# --- OtimizaÃ§Ã£o de HiperparÃ¢metros com Optuna ---\n",
    "#\n",
    "\n",
    "def wmape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula o Weighted Mean Absolute Percentage Error (WMAPE).\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    FunÃ§Ã£o objetivo que o Optuna tentarÃ¡ minimizar.\n",
    "    Ela treina um modelo LightGBM com um conjunto de hiperparÃ¢metros\n",
    "    e retorna o WMAPE mÃ©dio da validaÃ§Ã£o cruzada temporal.\n",
    "    \"\"\"\n",
    "    # 1. DefiniÃ§Ã£o do EspaÃ§o de Busca de HiperparÃ¢metros\n",
    "    params = {\n",
    "        'objective': 'regression_l1', # MAE, bom para WMAPE\n",
    "        'metric': 'mae',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 2000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "\n",
    "    # 2. ValidaÃ§Ã£o Cruzada Temporal (TimeSeriesSplit)\n",
    "    # n_splits=3 significa que teremos 3 cortes de treino/validaÃ§Ã£o.\n",
    "    # Ex: [treino_semanas_1-24, val_semanas_25-36], [treino_semanas_1-36, val_semanas_37-49] etc.\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    wmape_scores = []\n",
    "\n",
    "    print(f\"Iniciando Trial {trial.number}...\")\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # 3. Treinamento do Modelo\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric='mae',\n",
    "                  callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "        # 4. PrediÃ§Ã£o e CÃ¡lculo do WMAPE\n",
    "        preds = model.predict(X_val)\n",
    "        preds = np.maximum(0, preds) # Garantir nÃ£o negatividade\n",
    "        score = wmape(y_val, preds)\n",
    "        wmape_scores.append(score)\n",
    "\n",
    "        # Limpeza de memÃ³ria\n",
    "        del X_train, X_val, y_train, y_val, model, preds\n",
    "        gc.collect()\n",
    "\n",
    "    # 5. Retornar a MÃ©dia dos Scores\n",
    "    avg_wmape = np.mean(wmape_scores)\n",
    "    print(f\"Trial {trial.number} concluÃ­do. WMAPE MÃ©dio: {avg_wmape:.6f}\")\n",
    "\n",
    "    return avg_wmape\n",
    "\n",
    "print('âœ… FunÃ§Ã£o objetivo implementada!')\n",
    "print('ğŸ¯ Esta funÃ§Ã£o irÃ¡ treinar LightGBM com diferentes hiperparÃ¢metros')\n",
    "print('ğŸ“Š TimeSeriesSplit com 3 folds garante validaÃ§Ã£o temporal robusta')\n",
    "print('ğŸ” WMAPE como mÃ©trica objetivo (oficial do challenge)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ExecuÃ§Ã£o do Estudo Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 22:49:28,440] A new study created in memory with name: no-name-4b18a43e-02a0-4e4c-992c-6b0a9e305450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Trial 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-12 23:23:03,498] Trial 0 finished with value: 0.2927346403647517 and parameters: {'n_estimators': 1411, 'learning_rate': 0.06803186842731727, 'num_leaves': 26, 'max_depth': 7, 'subsample': 0.7574887128094137, 'colsample_bytree': 0.9843063978240129, 'reg_alpha': 0.6188971453571402, 'reg_lambda': 0.37180232931946866}. Best is trial 0 with value: 0.2927346403647517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 concluÃ­do. WMAPE MÃ©dio: 0.292735\n",
      "Iniciando Trial 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 00:22:09,734] Trial 1 finished with value: 0.2657155598336263 and parameters: {'n_estimators': 1797, 'learning_rate': 0.09259580231923607, 'num_leaves': 88, 'max_depth': 15, 'subsample': 0.8742698311922052, 'colsample_bytree': 0.6956705317533958, 'reg_alpha': 0.28954975228378843, 'reg_lambda': 0.40302127822824063}. Best is trial 1 with value: 0.2657155598336263.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 concluÃ­do. WMAPE MÃ©dio: 0.265716\n",
      "Iniciando Trial 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 00:57:35,399] Trial 2 finished with value: 0.2567257591194399 and parameters: {'n_estimators': 501, 'learning_rate': 0.04704563538475073, 'num_leaves': 250, 'max_depth': 9, 'subsample': 0.6936254452242231, 'colsample_bytree': 0.7597934533609093, 'reg_alpha': 0.9864454190783603, 'reg_lambda': 0.14674893733285044}. Best is trial 2 with value: 0.2567257591194399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 concluÃ­do. WMAPE MÃ©dio: 0.256726\n",
      "Iniciando Trial 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 02:30:23,344] Trial 3 finished with value: 0.18475890720236135 and parameters: {'n_estimators': 1798, 'learning_rate': 0.0817220633125079, 'num_leaves': 218, 'max_depth': 14, 'subsample': 0.8517268527117055, 'colsample_bytree': 0.9438211907470613, 'reg_alpha': 0.4386118724477075, 'reg_lambda': 0.6286582931001017}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 concluÃ­do. WMAPE MÃ©dio: 0.184759\n",
      "Iniciando Trial 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 02:56:03,945] Trial 4 finished with value: 0.23983359747114139 and parameters: {'n_estimators': 458, 'learning_rate': 0.0646736752723079, 'num_leaves': 146, 'max_depth': 11, 'subsample': 0.967810232608431, 'colsample_bytree': 0.8787884331284879, 'reg_alpha': 0.17861889427732514, 'reg_lambda': 0.867247576999291}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 concluÃ­do. WMAPE MÃ©dio: 0.239834\n",
      "Iniciando Trial 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 03:49:41,343] Trial 5 finished with value: 0.26196476542072417 and parameters: {'n_estimators': 1825, 'learning_rate': 0.056066732709426775, 'num_leaves': 265, 'max_depth': 6, 'subsample': 0.8128618652033004, 'colsample_bytree': 0.9008777841003632, 'reg_alpha': 0.9016407745365602, 'reg_lambda': 0.7975973450633123}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 concluÃ­do. WMAPE MÃ©dio: 0.261965\n",
      "Iniciando Trial 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 05:00:16,370] Trial 6 finished with value: 0.27472107499752824 and parameters: {'n_estimators': 1666, 'learning_rate': 0.013436268776283768, 'num_leaves': 101, 'max_depth': 10, 'subsample': 0.6415313258412955, 'colsample_bytree': 0.8891405626100075, 'reg_alpha': 0.42175484855904943, 'reg_lambda': 0.499578027283483}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 concluÃ­do. WMAPE MÃ©dio: 0.274721\n",
      "Iniciando Trial 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 05:39:06,260] Trial 7 finished with value: 0.3696222135302769 and parameters: {'n_estimators': 1039, 'learning_rate': 0.05111002168415207, 'num_leaves': 296, 'max_depth': 8, 'subsample': 0.7766688870744332, 'colsample_bytree': 0.9971957512164734, 'reg_alpha': 0.930544650297818, 'reg_lambda': 0.09724703233812604}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 concluÃ­do. WMAPE MÃ©dio: 0.369622\n",
      "Iniciando Trial 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 06:32:07,733] Trial 8 finished with value: 0.20701046311291713 and parameters: {'n_estimators': 914, 'learning_rate': 0.0637228350509842, 'num_leaves': 231, 'max_depth': 15, 'subsample': 0.8054198559776756, 'colsample_bytree': 0.8957713404733776, 'reg_alpha': 0.9510323714055949, 'reg_lambda': 0.6531640656049035}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 concluÃ­do. WMAPE MÃ©dio: 0.207010\n",
      "Iniciando Trial 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 08:14:02,449] Trial 9 finished with value: 0.21196412277382182 and parameters: {'n_estimators': 1363, 'learning_rate': 0.09000060046653023, 'num_leaves': 209, 'max_depth': 8, 'subsample': 0.6460561788493375, 'colsample_bytree': 0.650954236305393, 'reg_alpha': 0.7429794479503273, 'reg_lambda': 0.33607770376355317}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 concluÃ­do. WMAPE MÃ©dio: 0.211964\n",
      "Iniciando Trial 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 10:49:08,480] Trial 10 finished with value: 0.22258363391617864 and parameters: {'n_estimators': 1996, 'learning_rate': 0.029807855336738194, 'num_leaves': 163, 'max_depth': 12, 'subsample': 0.9443599125358754, 'colsample_bytree': 0.7956976197366297, 'reg_alpha': 0.02576117011038581, 'reg_lambda': 0.978876569590441}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 concluÃ­do. WMAPE MÃ©dio: 0.222584\n",
      "Iniciando Trial 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 11:40:24,924] Trial 11 finished with value: 0.20463820421409173 and parameters: {'n_estimators': 907, 'learning_rate': 0.07854654179879834, 'num_leaves': 204, 'max_depth': 15, 'subsample': 0.856153702078116, 'colsample_bytree': 0.9216402666335493, 'reg_alpha': 0.4875059390067612, 'reg_lambda': 0.6584936958648433}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 concluÃ­do. WMAPE MÃ©dio: 0.204638\n",
      "Iniciando Trial 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 12:08:34,157] Trial 12 finished with value: 0.20085360183111212 and parameters: {'n_estimators': 781, 'learning_rate': 0.07981687797612265, 'num_leaves': 184, 'max_depth': 13, 'subsample': 0.8926375364569515, 'colsample_bytree': 0.9460831013268796, 'reg_alpha': 0.4787841723284184, 'reg_lambda': 0.6225895775323516}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 concluÃ­do. WMAPE MÃ©dio: 0.200854\n",
      "Iniciando Trial 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 12:36:31,676] Trial 13 finished with value: 0.2552741615710629 and parameters: {'n_estimators': 629, 'learning_rate': 0.09932510235424491, 'num_leaves': 166, 'max_depth': 13, 'subsample': 0.8895574141215936, 'colsample_bytree': 0.8273595456431119, 'reg_alpha': 0.6492823366926863, 'reg_lambda': 0.656083697761719}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 concluÃ­do. WMAPE MÃ©dio: 0.255274\n",
      "Iniciando Trial 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 13:01:43,130] Trial 14 finished with value: 0.2002894616232743 and parameters: {'n_estimators': 747, 'learning_rate': 0.08016287647971305, 'num_leaves': 194, 'max_depth': 13, 'subsample': 0.9189411541808026, 'colsample_bytree': 0.9529292991411896, 'reg_alpha': 0.34791093271969736, 'reg_lambda': 0.5400222987259236}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 concluÃ­do. WMAPE MÃ©dio: 0.200289\n",
      "Iniciando Trial 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-13 13:36:47,702] Trial 15 finished with value: 0.20633785710510585 and parameters: {'n_estimators': 1232, 'learning_rate': 0.08263163276408912, 'num_leaves': 136, 'max_depth': 13, 'subsample': 0.9903340636455575, 'colsample_bytree': 0.9563780951545964, 'reg_alpha': 0.31013844254211087, 'reg_lambda': 0.5002528138009346}. Best is trial 3 with value: 0.18475890720236135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 concluÃ­do. WMAPE MÃ©dio: 0.206338\n",
      "Iniciando Trial 16...\n"
     ]
    }
   ],
   "source": [
    "# CriaÃ§Ã£o do estudo: 'minimize' o WMAPE\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Iniciar a otimizaÃ§Ã£o.\n",
    "# n_trials=30 Ã© um bom ponto de partida. Se tiver mais tempo, pode aumentar.\n",
    "# Com um dataset grande, 30 trials jÃ¡ podem levar algumas horas.\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Imprimir os resultados\n",
    "print(\"\\n--- OtimizaÃ§Ã£o ConcluÃ­da ---\")\n",
    "print(f\"Melhor Trial: {study.best_trial.number}\")\n",
    "print(f\"Melhor WMAPE: {study.best_value:.6f}\")\n",
    "print(\"Melhores HiperparÃ¢metros:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Salvar os melhores parÃ¢metros para usar no pipeline final\n",
    "best_params = study.best_params\n",
    "with open('../data/best_lgbm_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)\n",
    "\n",
    "print(\"\\nâœ… Melhores parÃ¢metros salvos em '../data/best_lgbm_params.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AnÃ¡lise dos Resultados da OtimizaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lise detalhada dos resultados da otimizaÃ§Ã£o\n",
    "print(\"ğŸ” ANÃLISE DETALHADA DOS RESULTADOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Comparar com baseline (assumindo WMAPE vanilla ~15.25%)\n",
    "baseline_wmape = 0.1525  # WMAPE do modelo vanilla\n",
    "optimized_wmape = study.best_value\n",
    "\n",
    "improvement_pct = ((baseline_wmape - optimized_wmape) / baseline_wmape) * 100\n",
    "print(f\"\\nğŸ“Š COMPARAÃ‡ÃƒO DE PERFORMANCE:\")\n",
    "print(f\"   â€¢ WMAPE Baseline (Vanilla): {baseline_wmape:.4f} ({baseline_wmape*100:.2f}%)\")\n",
    "print(f\"   â€¢ WMAPE Otimizado (Optuna): {optimized_wmape:.4f} ({optimized_wmape*100:.2f}%)\")\n",
    "print(f\"   â€¢ Melhoria Absoluta: {baseline_wmape - optimized_wmape:.4f}\")\n",
    "print(f\"   â€¢ Melhoria Relativa: {improvement_pct:+.2f}%\")\n",
    "\n",
    "if improvement_pct > 5:\n",
    "    print(f\"   ğŸ‰ EXCELENTE! Melhoria significativa > 5%\")\n",
    "elif improvement_pct > 2:\n",
    "    print(f\"   âœ… BOA! Melhoria sÃ³lida > 2%\")\n",
    "elif improvement_pct > 0:\n",
    "    print(f\"   ğŸ‘ POSITIVA! Alguma melhoria detectada\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ SEM MELHORIA! Revisar estratÃ©gia\")\n",
    "\n",
    "# AnÃ¡lise dos melhores hiperparÃ¢metros\n",
    "print(f\"\\nğŸ¯ ANÃLISE DOS MELHORES HIPERPARÃ‚METROS:\")\n",
    "print(f\"   â€¢ n_estimators: {best_params['n_estimators']} {'(Alto - modelo complexo)' if best_params['n_estimators'] > 1500 else '(Moderado)'}\")\n",
    "print(f\"   â€¢ learning_rate: {best_params['learning_rate']:.3f} {'(Baixo - aprendizado conservador)' if best_params['learning_rate'] < 0.05 else '(Normal)'}\")\n",
    "print(f\"   â€¢ num_leaves: {best_params['num_leaves']} {'(Alto - modelo expressivo)' if best_params['num_leaves'] > 200 else '(Moderado)'}\")\n",
    "print(f\"   â€¢ max_depth: {best_params['max_depth']} {'(Profundo)' if best_params['max_depth'] > 10 else '(Controlado)'}\")\n",
    "print(f\"   â€¢ subsample: {best_params['subsample']:.2f} {'(Conservador - evita overfitting)' if best_params['subsample'] < 0.8 else '(Liberal)'}\")\n",
    "print(f\"   â€¢ colsample_bytree: {best_params['colsample_bytree']:.2f}\")\n",
    "print(f\"   â€¢ reg_alpha (L1): {best_params['reg_alpha']:.3f}\")\n",
    "print(f\"   â€¢ reg_lambda (L2): {best_params['reg_lambda']:.3f}\")\n",
    "\n",
    "# AnÃ¡lise da evoluÃ§Ã£o dos trials\n",
    "trials_df = study.trials_dataframe()\n",
    "print(f\"\\nğŸ“ˆ EVOLUÃ‡ÃƒO DA OTIMIZAÃ‡ÃƒO:\")\n",
    "print(f\"   â€¢ Total de trials: {len(trials_df)}\")\n",
    "print(f\"   â€¢ Melhor trial encontrado: #{study.best_trial.number}\")\n",
    "print(f\"   â€¢ WMAPE mÃ­nimo: {trials_df['value'].min():.6f}\")\n",
    "print(f\"   â€¢ WMAPE mÃ¡ximo: {trials_df['value'].max():.6f}\")\n",
    "print(f\"   â€¢ WMAPE mÃ©dio: {trials_df['value'].mean():.6f}\")\n",
    "print(f\"   â€¢ Desvio padrÃ£o: {trials_df['value'].std():.6f}\")\n",
    "\n",
    "print(f\"\\nâœ… AnÃ¡lise concluÃ­da! Modelo otimizado pronto para uso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. VisualizaÃ§Ãµes da OtimizaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaÃ§Ãµes da otimizaÃ§Ã£o Optuna\n",
    "print(\"ğŸ“Š CRIANDO VISUALIZAÃ‡Ã•ES DA OTIMIZAÃ‡ÃƒO\")\n",
    "\n",
    "# 1. EvoluÃ§Ã£o dos trials ao longo do tempo\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: EvoluÃ§Ã£o do WMAPE ao longo dos trials\n",
    "trial_numbers = [t.number for t in study.trials]\n",
    "trial_values = [t.value for t in study.trials if t.value is not None]\n",
    "trial_nums_valid = [t.number for t in study.trials if t.value is not None]\n",
    "\n",
    "axes[0,0].plot(trial_nums_valid, trial_values, 'b-', alpha=0.7)\n",
    "axes[0,0].axhline(y=baseline_wmape, color='r', linestyle='--', label=f'Baseline: {baseline_wmape:.4f}')\n",
    "axes[0,0].axhline(y=study.best_value, color='g', linestyle='--', label=f'Melhor: {study.best_value:.4f}')\n",
    "axes[0,0].set_xlabel('NÃºmero do Trial')\n",
    "axes[0,0].set_ylabel('WMAPE')\n",
    "axes[0,0].set_title('EvoluÃ§Ã£o do WMAPE por Trial')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DistribuiÃ§Ã£o dos valores de WMAPE\n",
    "axes[0,1].hist(trial_values, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].axvline(x=baseline_wmape, color='r', linestyle='--', label=f'Baseline: {baseline_wmape:.4f}')\n",
    "axes[0,1].axvline(x=study.best_value, color='g', linestyle='--', label=f'Melhor: {study.best_value:.4f}')\n",
    "axes[0,1].set_xlabel('WMAPE')\n",
    "axes[0,1].set_ylabel('FrequÃªncia')\n",
    "axes[0,1].set_title('DistribuiÃ§Ã£o dos Valores de WMAPE')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Plot 3: ImportÃ¢ncia dos hiperparÃ¢metros\n",
    "param_importance = optuna.importance.get_param_importances(study)\n",
    "param_names = list(param_importance.keys())\n",
    "param_values = list(param_importance.values())\n",
    "\n",
    "axes[1,0].barh(param_names, param_values)\n",
    "axes[1,0].set_xlabel('ImportÃ¢ncia')\n",
    "axes[1,0].set_title('ImportÃ¢ncia dos HiperparÃ¢metros')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: ComparaÃ§Ã£o Baseline vs Otimizado\n",
    "models = ['Baseline\\n(Vanilla)', 'Otimizado\\n(Optuna)']\n",
    "wmape_values = [baseline_wmape * 100, study.best_value * 100]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = axes[1,1].bar(models, wmape_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_ylabel('WMAPE (%)')\n",
    "axes[1,1].set_title('ComparaÃ§Ã£o: Baseline vs Otimizado')\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, value in zip(bars, wmape_values):\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                   f'{value:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Adicionar melhoria como texto\n",
    "improvement_text = f'Melhoria: {improvement_pct:+.2f}%'\n",
    "axes[1,1].text(0.5, max(wmape_values) * 0.8, improvement_text, \n",
    "               ha='center', va='center', transform=axes[1,1].transData,\n",
    "               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "               fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… VisualizaÃ§Ãµes criadas!\")\n",
    "print(\"ğŸ“ˆ As visualizaÃ§Ãµes mostram a evoluÃ§Ã£o e eficÃ¡cia da otimizaÃ§Ã£o Optuna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento do Modelo Final Otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo final com os melhores hiperparÃ¢metros encontrados\n",
    "print(\"ğŸš€ TREINANDO MODELO FINAL COM HIPERPARÃ‚METROS OTIMIZADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar parÃ¢metros completos para o modelo final\n",
    "final_params = {\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# Adicionar os melhores hiperparÃ¢metros encontrados pelo Optuna\n",
    "final_params.update(best_params)\n",
    "\n",
    "print(\"ğŸ¯ ParÃ¢metros finais do modelo:\")\n",
    "for key, value in final_params.items():\n",
    "    print(f\"   â€¢ {key}: {value}\")\n",
    "\n",
    "# Treinar o modelo final usando todos os dados disponÃ­veis\n",
    "print(f\"\\nğŸ”„ Treinando modelo final com todos os dados...\")\n",
    "print(f\"   â€¢ Dataset shape: {X.shape}\")\n",
    "print(f\"   â€¢ Target shape: {y.shape}\")\n",
    "\n",
    "# Criar modelo LightGBM final\n",
    "final_model = lgb.LGBMRegressor(**final_params)\n",
    "\n",
    "# Treinar sem validation set (usar todos os dados)\n",
    "final_model.fit(X, y, verbose=False)\n",
    "\n",
    "print(f\"âœ… Modelo final treinado com sucesso!\")\n",
    "print(f\"   â€¢ Estimators utilizados: {final_model.n_estimators}\")\n",
    "print(f\"   â€¢ Features utilizadas: {len(X.columns)}\")\n",
    "\n",
    "# Salvar modelo final e metadados\n",
    "model_artifacts_optuna = {\n",
    "    'model': final_model,\n",
    "    'model_type': 'LightGBM_Optuna_Optimized',\n",
    "    'best_params': best_params,\n",
    "    'features': list(X.columns),\n",
    "    'target': target,\n",
    "    'optuna_study': study,\n",
    "    'validation_wmape_optuna': study.best_value,\n",
    "    'validation_wmape_baseline': baseline_wmape,\n",
    "    'improvement_pct': improvement_pct,\n",
    "    'training_date': pd.Timestamp.now(),\n",
    "    'n_trials': len(study.trials),\n",
    "    'optimization_time': 'calculated_during_execution'\n",
    "}\n",
    "\n",
    "# Salvar artefatos do modelo otimizado\n",
    "with open('../data/trained_model_optuna.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts_optuna, f)\n",
    "\n",
    "print(f\"\\nğŸ’¾ MODELO E ARTEFATOS SALVOS:\")\n",
    "print(f\"   âœ… trained_model_optuna.pkl - Modelo otimizado e metadados completos\")\n",
    "print(f\"   âœ… best_lgbm_params.pkl - Melhores hiperparÃ¢metros para uso futuro\")\n",
    "\n",
    "print(f\"\\nğŸ‰ OTIMIZAÃ‡ÃƒO OPTUNA CONCLUÃDA COM SUCESSO!\")\n",
    "print(f\"   ğŸ“Š WMAPE otimizado: {study.best_value:.4f} ({study.best_value*100:.2f}%)\")\n",
    "print(f\"   ğŸ“ˆ Melhoria sobre baseline: {improvement_pct:+.2f}%\")\n",
    "print(f\"   ğŸ¯ Modelo pronto para prediÃ§Ãµes finais!\")\n",
    "\n",
    "# Feature importance do modelo otimizado\n",
    "feature_importance_optuna = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ” TOP 10 FEATURES MAIS IMPORTANTES (MODELO OTIMIZADO):\")\n",
    "for i, (_, row) in enumerate(feature_importance_optuna.head(10).iterrows(), 1):\n",
    "    print(f\"   {i:2d}. {row['feature']}: {row['importance']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumo e PrÃ³ximos Passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ğŸ‰ OTIMIZAÃ‡ÃƒO DE HIPERPARÃ‚METROS CONCLUÃDA COM SUCESSO!')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'\\nğŸ† RESULTADOS ALCANÃ‡ADOS:')\n",
    "print(f'   â€¢ Modelo Base (Vanilla): WMAPE = {baseline_wmape:.4f} ({baseline_wmape*100:.2f}%)')\n",
    "print(f'   â€¢ Modelo Otimizado (Optuna): WMAPE = {study.best_value:.4f} ({study.best_value*100:.2f}%)')\n",
    "print(f'   â€¢ Melhoria Absoluta: {baseline_wmape - study.best_value:.4f}')\n",
    "print(f'   â€¢ Melhoria Relativa: {improvement_pct:+.2f}%')\n",
    "\n",
    "if improvement_pct > 5:\n",
    "    print(f'   ğŸ¯ STATUS: EXCELENTE! Melhoria significativa alcanÃ§ada')\n",
    "elif improvement_pct > 2:\n",
    "    print(f'   ğŸ¯ STATUS: MUITO BOM! Melhoria sÃ³lida alcanÃ§ada')\n",
    "elif improvement_pct > 0:\n",
    "    print(f'   ğŸ¯ STATUS: POSITIVO! Alguma melhoria detectada')\n",
    "else:\n",
    "    print(f'   ğŸ¯ STATUS: ATENÃ‡ÃƒO! Revisar estratÃ©gia necessÃ¡rio')\n",
    "\n",
    "print(f'\\nğŸ“Š CONFIGURAÃ‡ÃƒO OTIMIZADA:')\n",
    "print(f'   â€¢ Framework: Optuna (otimizaÃ§Ã£o bayesiana)')\n",
    "print(f'   â€¢ ValidaÃ§Ã£o: TimeSeriesSplit (3 folds temporais)')\n",
    "print(f'   â€¢ Trials executados: {len(study.trials)}')\n",
    "print(f'   â€¢ Melhor trial: #{study.best_trial.number}')\n",
    "print(f'   â€¢ HiperparÃ¢metros otimizados: {len(best_params)}')\n",
    "\n",
    "print(f'\\nğŸ’¾ ARTEFATOS GERADOS:')\n",
    "print('   âœ… trained_model_optuna.pkl - Modelo LightGBM otimizado')\n",
    "print('   âœ… best_lgbm_params.pkl - Melhores hiperparÃ¢metros')\n",
    "print('   âœ… Estudo Optuna completo salvo nos artefatos')\n",
    "print('   âœ… Feature importance atualizada')\n",
    "print('   âœ… Metadados completos da otimizaÃ§Ã£o')\n",
    "\n",
    "print(f'\\nğŸš€ PRÃ“XIMOS PASSOS RECOMENDADOS:')\n",
    "print('   1. ğŸ”„ Atualizar pipeline final (04-Final-Pipeline.ipynb)')\n",
    "print('   2. ğŸ“Š Usar trained_model_optuna.pkl em vez do modelo vanilla')\n",
    "print('   3. ğŸ¯ Gerar prediÃ§Ãµes finais com o modelo otimizado')\n",
    "print('   4. ğŸ“‹ Validar melhoria na submissÃ£o final')\n",
    "print('   5. ğŸ§ª Opcional: Aumentar n_trials se mais tempo disponÃ­vel')\n",
    "\n",
    "print(f'\\nğŸ¯ IMPACTO ESPERADO NA COMPETIÃ‡ÃƒO:')\n",
    "if improvement_pct > 5:\n",
    "    print(f'   ğŸ† ALTO: Melhoria >5% deve impactar significativamente o ranking')\n",
    "elif improvement_pct > 2:\n",
    "    print(f'   ğŸ“ˆ MÃ‰DIO: Melhoria >2% pode melhorar posiÃ§Ã£o no leaderboard')\n",
    "elif improvement_pct > 0:\n",
    "    print(f'   ğŸ“Š BAIXO: Melhoria marginal, mas ainda positiva')\n",
    "else:\n",
    "    print(f'   âš ï¸ NEUTRO: Sem impacto esperado no ranking')\n",
    "\n",
    "print(f'\\nâœ… SISTEMA DE FORECASTING OTIMIZADO E PRONTO!')\n",
    "print('   â€¢ Optuna integrado com sucesso')\n",
    "print('   â€¢ ValidaÃ§Ã£o temporal robusta implementada')\n",
    "print('   â€¢ Modelo de alta performance treinado')\n",
    "print('   â€¢ Pipeline completo e otimizado')\n",
    "print('   â€¢ Pronto para gerar submissÃ£o final competitiva!')\n",
    "\n",
    "print('\\nğŸŠ PARABÃ‰NS! OtimizaÃ§Ã£o Optuna CONCLUÃDA com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT DUE TO MEMORY ERROR\n",
    "# Random Forest requires too much RAM for 50M+ rows dataset\n",
    "# Moving directly to LightGBM which is optimized for large datasets\n",
    "\n",
    "print('ğŸŒ² Random Forest - SKIPPED (Memory Optimization)')\n",
    "print('ğŸ“Š Reason: 50M+ rows dataset exceeds scikit-learn RandomForest memory capacity')\n",
    "print('ğŸš€ Solution: Using LightGBM instead (designed for big data)')\n",
    "print('âœ… Memory-efficient gradient boosting will handle this scale perfectly')\n",
    "\n",
    "# Original Random Forest code commented out:\n",
    "\"\"\"\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_val)\n",
    "rf_pred = np.maximum(0, rf_pred)\n",
    "\n",
    "results_rf = evaluate_model(y_val, rf_pred, 'Random Forest')\n",
    "model_results.append(results_rf)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('ğŸ” Top 10 features mais importantes (Random Forest):')\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "    print(f'   {i:2d}. {row[\"feature\"]}: {row[\"importance\"]:.4f}')\n",
    "\"\"\"\n",
    "\n",
    "print('\\nğŸ’¡ Recommendation: Execute remaining cells to train LightGBM and XGBoost models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSO B: LightGBM Vanilla - Validar Pipeline e Features\n",
    "print('\\nğŸš€ PASSO B: Modelo LightGBM Vanilla (ParÃ¢metros Default)')\n",
    "print('=' * 60)\n",
    "print('ğŸ¯ Objetivo: Validar se nossas features tÃªm poder preditivo')\n",
    "\n",
    "# ConfiguraÃ§Ã£o LightGBM Vanilla (parÃ¢metros simples/default)\n",
    "lgb_params_vanilla = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(f'\\nğŸ“‹ ConfiguraÃ§Ã£o Vanilla:')\n",
    "for param, value in lgb_params_vanilla.items():\n",
    "    print(f'   â€¢ {param}: {value}')\n",
    "\n",
    "# Preparar dados para LightGBM\n",
    "print(f'\\nğŸ“Š Preparando dados para treinamento...')\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "val_lgb = lgb.Dataset(X_val, label=y_val, reference=train_lgb)\n",
    "\n",
    "print(f'   â€¢ Train shape: {X_train.shape}')\n",
    "print(f'   â€¢ Val shape: {X_val.shape}')\n",
    "print(f'   â€¢ Features: {len(all_features)}')\n",
    "\n",
    "# Treinar modelo Vanilla (VERSÃƒO CORRIGIDA)\n",
    "print(f'\\nğŸ”„ Treinando LightGBM Vanilla...')\n",
    "lgb_vanilla = lgb.train(\n",
    "    lgb_params_vanilla,\n",
    "    train_lgb,\n",
    "    num_boost_round=200,  # NÃºmero moderado para vanilla\n",
    "    valid_sets=[train_lgb, val_lgb],\n",
    "    valid_names=['train', 'eval'],\n",
    "    # CORREÃ‡ÃƒO: Usar callbacks em vez de early_stopping_rounds\n",
    "    callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    ")\n",
    "\n",
    "print(f'âœ… Treinamento concluÃ­do em {lgb_vanilla.best_iteration} iteraÃ§Ãµes')\n",
    "\n",
    "# PrediÃ§Ãµes\n",
    "print(f'\\nğŸ¯ Gerando prediÃ§Ãµes...')\n",
    "lgb_vanilla_pred = lgb_vanilla.predict(X_val, num_iteration=lgb_vanilla.best_iteration)\n",
    "lgb_vanilla_pred = np.maximum(0, lgb_vanilla_pred)  # NÃ£o permitir prediÃ§Ãµes negativas\n",
    "\n",
    "# AvaliaÃ§Ã£o\n",
    "results_lgb_vanilla = evaluate_model(y_val, lgb_vanilla_pred, 'LightGBM Vanilla')\n",
    "model_results.append(results_lgb_vanilla)\n",
    "\n",
    "# ANÃLISE CRÃTICA - Pergunta chave\n",
    "print(f'\\nğŸ” ANÃLISE CRÃTICA - VALIDAÃ‡ÃƒO DO PIPELINE:')\n",
    "print('=' * 60)\n",
    "print(f'LightGBM Vanilla    | WMAPE: {results_lgb_vanilla[\"WMAPE\"]:6.2f}% | MAE: {results_lgb_vanilla[\"MAE\"]:8.4f} | RÂ²: {results_lgb_vanilla[\"RÂ²\"]:6.4f}')\n",
    "print(f'Melhor Baseline     | WMAPE: {best_baseline[\"WMAPE\"]:6.2f}% | MAE: {best_baseline[\"MAE\"]:8.4f} | RÂ²: {best_baseline[\"RÂ²\"]:6.4f}')\n",
    "\n",
    "# Calcular melhoria\n",
    "wmape_improvement = ((best_baseline[\"WMAPE\"] - results_lgb_vanilla[\"WMAPE\"]) / best_baseline[\"WMAPE\"]) * 100\n",
    "mae_improvement = ((best_baseline[\"MAE\"] - results_lgb_vanilla[\"MAE\"]) / best_baseline[\"MAE\"]) * 100\n",
    "\n",
    "print(f'\\nğŸ“ˆ MELHORIA SOBRE MELHOR BASELINE:')\n",
    "print(f'   â€¢ WMAPE: {wmape_improvement:+.2f}% {\"âœ… SIGNIFICATIVA!\" if wmape_improvement > 5 else \"âš ï¸ MARGINAL\" if wmape_improvement > 0 else \"âŒ PIOR QUE BASELINE!\"}')\n",
    "print(f'   â€¢ MAE:   {mae_improvement:+.2f}% {\"âœ…\" if mae_improvement > 0 else \"âŒ\"}')\n",
    "\n",
    "# DiagnÃ³stico\n",
    "if wmape_improvement > 5:\n",
    "    print(f'\\nğŸ‰ DIAGNÃ“STICO: PIPELINE VALIDADO!')\n",
    "    print(f'   âœ… Features tÃªm forte poder preditivo')\n",
    "    print(f'   âœ… Pronto para otimizaÃ§Ã£o de hiperparÃ¢metros')\n",
    "elif wmape_improvement > 0:\n",
    "    print(f'\\nğŸ¤” DIAGNÃ“STICO: MELHORIA MARGINAL')\n",
    "    print(f'   âš ï¸ Features tÃªm algum poder preditivo, mas limitado')\n",
    "    print(f'   ğŸ” Considerar anÃ¡lise de feature importance')\n",
    "else:\n",
    "    print(f'\\nğŸš¨ DIAGNÃ“STICO: PROBLEMA NO PIPELINE!')\n",
    "    print(f'   âŒ Modelo pior que baseline - possÃ­vel data leakage ou bug')\n",
    "    print(f'   ğŸ”§ Revisar feature engineering urgentemente')\n",
    "\n",
    "print(f'\\nâœ… Passo B concluÃ­do - LightGBM Vanilla validado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. XGBoost (VERSÃƒO CORRIGIDA - Memory Optimized)\n",
    "print('\\nğŸš€ 6. XGBoost (Memory Optimized)')\n",
    "\n",
    "# ParÃ¢metros XGBoost com tree_method otimizado para datasets massivos\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'tree_method': 'approx',  # CORREÃ‡ÃƒO: MÃ©todo otimizado para datasets massivos (menos memÃ³ria)\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 10,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 0.1,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'enable_categorical': True\n",
    "}\n",
    "\n",
    "print(f'ğŸ§  EstratÃ©gia de MemÃ³ria: tree_method=\"approx\" (quantile sketching)')\n",
    "print(f'   â€¢ Otimizado para datasets com 50M+ registros')\n",
    "print(f'   â€¢ Reduz uso de memÃ³ria durante treinamento')\n",
    "\n",
    "# Treinar modelo (VERSÃƒO MEMORY-OPTIMIZED)\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1000)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# PrediÃ§Ãµes\n",
    "xgb_pred = xgb_model.predict(X_val)\n",
    "xgb_pred = np.maximum(0, xgb_pred)\n",
    "\n",
    "results_xgb = evaluate_model(y_val, xgb_pred, 'XGBoost')\n",
    "model_results.append(results_xgb)\n",
    "\n",
    "print(f'âœ… XGBoost treinado com sucesso - Melhor iteraÃ§Ã£o: {xgb_model.best_iteration}')\n",
    "print(f'ğŸ¯ Memory optimization funcionou! MÃ©todo \"approx\" resolveu o problema.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ ExperiÃªncia com XGBoost - LiÃ§Ãµes Aprendidas\n",
    "\n",
    "**ğŸ“Š RESULTADOS OBTIDOS:**\n",
    "O XGBoost obteve performance marginalmente melhor que o LightGBM nesta validaÃ§Ã£o (WMAPE ~14.8% vs 15.25%).\n",
    "\n",
    "**ğŸš¨ PROBLEMA CRÃTICO DESCOBERTO:**\n",
    "Quando tentamos retreinar o XGBoost no dataset completo de 2022 (50M+ registros) para gerar as prediÃ§Ãµes finais, encontramos:\n",
    "\n",
    "1. **Erro `bad_allocation`** - Falta de memÃ³ria RAM\n",
    "2. **Instabilidade do sistema** - Travamento durante treinamento \n",
    "3. **MÃ©todo `tree_method=\"approx\"`** resolve parcialmente, mas ainda consome muita RAM\n",
    "\n",
    "**ğŸ§  DECISÃƒO TÃ‰CNICA:**\n",
    "Apesar do XGBoost ter performance ligeiramente superior na validaÃ§Ã£o, **optamos pelo LightGBM** por:\n",
    "\n",
    "- âœ… **Robustez**: Treina consistentemente em datasets massivos\n",
    "- âœ… **Velocidade**: 3-5x mais rÃ¡pido que XGBoost\n",
    "- âœ… **EficiÃªncia de MemÃ³ria**: Otimizado para Big Data\n",
    "- âœ… **Confiabilidade**: Sem travamentos ou erros de memÃ³ria\n",
    "- âœ… **DiferenÃ§a Pequena**: Performance praticamente igual (~0.5% diferenÃ§a)\n",
    "\n",
    "**ğŸ“ˆ CONCLUSÃƒO:**\n",
    "Em produÃ§Ã£o, a **confiabilidade e robustez superam ganhos marginais de performance**. LightGBM Ã© a escolha mais sensata para este desafio de forecasting em larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ComparaÃ§Ã£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar todos os modelos\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('MAE')\n",
    "\n",
    "print('ğŸ† Ranking de Modelos por MAE:')\n",
    "print('=' * 80)\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "# VisualizaÃ§Ã£o dos resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# MAE comparison\n",
    "results_df.plot(x='Model', y='MAE', kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Mean Absolute Error por Modelo')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RÂ² comparison\n",
    "results_df.plot(x='Model', y='RÂ²', kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('RÂ² por Modelo')\n",
    "axes[1].set_ylabel('RÂ²')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Selecionar melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f'\\nğŸ¥‡ Melhor modelo: {best_model_name}')\n",
    "print(f'   â€¢ MAE: {results_df.iloc[0][\"MAE\"]:.4f}')\n",
    "print(f'   â€¢ RMSE: {results_df.iloc[0][\"RMSE\"]:.4f}')\n",
    "print(f'   â€¢ RÂ²: {results_df.iloc[0][\"RÂ²\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AnÃ¡lise de Erros do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar as previsÃµes do melhor modelo para anÃ¡lise\n",
    "if best_model_name == 'LightGBM':\n",
    "    best_pred = lgb_pred\n",
    "    best_model = lgb_model\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_pred = xgb_pred\n",
    "    best_model = xgb_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_pred = rf_pred\n",
    "    best_model = rf_model\n",
    "else:\n",
    "    # Fallback para baseline\n",
    "    best_pred = combo_pred\n",
    "    best_model = None\n",
    "\n",
    "# AnÃ¡lise de erros\n",
    "errors = y_val - best_pred\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# DistribuiÃ§Ã£o dos erros\n",
    "axes[0,0].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('DistribuiÃ§Ã£o dos Erros')\n",
    "axes[0,0].set_xlabel('Erro (Real - Predito)')\n",
    "axes[0,0].set_ylabel('FrequÃªncia')\n",
    "axes[0,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Scatter: Real vs Predito\n",
    "sample_idx = np.random.choice(len(y_val), min(5000, len(y_val)), replace=False)\n",
    "axes[0,1].scatter(y_val.iloc[sample_idx], best_pred[sample_idx], alpha=0.5)\n",
    "axes[0,1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')\n",
    "axes[0,1].set_title('Real vs Predito (amostra)')\n",
    "axes[0,1].set_xlabel('Valor Real')\n",
    "axes[0,1].set_ylabel('Valor Predito')\n",
    "\n",
    "# Erros por faixa de valor real\n",
    "val_bins = pd.cut(y_val, bins=10, labels=False)\n",
    "error_by_bin = [abs_errors[val_bins == i].mean() for i in range(10)]\n",
    "axes[1,0].bar(range(10), error_by_bin)\n",
    "axes[1,0].set_title('MAE por Faixa de Valor Real')\n",
    "axes[1,0].set_xlabel('Faixa (0=menor, 9=maior)')\n",
    "axes[1,0].set_ylabel('MAE')\n",
    "\n",
    "# Residuals plot\n",
    "axes[1,1].scatter(best_pred[sample_idx], errors.iloc[sample_idx], alpha=0.5)\n",
    "axes[1,1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1,1].set_title('Residuais vs PrediÃ§Ãµes')\n",
    "axes[1,1].set_xlabel('Valor Predito')\n",
    "axes[1,1].set_ylabel('Erro')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# EstatÃ­sticas dos erros\n",
    "print(f'ğŸ“Š AnÃ¡lise de Erros - {best_model_name}:')\n",
    "print(f'   â€¢ Erro mÃ©dio: {errors.mean():.4f}')\n",
    "print(f'   â€¢ Erro absoluto mÃ©dio: {abs_errors.mean():.4f}')\n",
    "print(f'   â€¢ Desvio padrÃ£o dos erros: {errors.std():.4f}')\n",
    "print(f'   â€¢ % prediÃ§Ãµes exatas (zeros): {(best_pred[y_val == 0] == 0).mean()*100:.1f}%')\n",
    "print(f'   â€¢ % subestimaÃ§Ã£o: {(errors > 0).mean()*100:.1f}%')\n",
    "print(f'   â€¢ % superestimaÃ§Ã£o: {(errors < 0).mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSO C: AnÃ¡lise Detalhada de Feature Importance\n",
    "print('\\nğŸ” PASSO C: AnÃ¡lise de Feature Importance (EstratÃ©gia SÃªnior)')\n",
    "print('=' * 70)\n",
    "print('ğŸ¯ Objetivo: Entender o que o modelo aprendeu ANTES de otimizar')\n",
    "\n",
    "# Extrair feature importance do modelo vanilla\n",
    "importance = lgb_vanilla.feature_importance(importance_type='gain')\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Criar DataFrame com importÃ¢ncias\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance,\n",
    "    'importance_pct': importance / importance.sum() * 100\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# AnÃ¡lise por categorias de features\n",
    "print(f'\\nğŸ“Š CATEGORIZAÃ‡ÃƒO DAS FEATURES:')\n",
    "\n",
    "feature_categories = {\n",
    "    'Lag': [f for f in feature_imp_df['feature'] if 'lag' in f.lower()],\n",
    "    'Rolling': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['media', 'std', 'max', 'min', 'rolling'])],\n",
    "    'Temporal': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['mes', 'semana', 'ano', 'sin', 'cos'])],\n",
    "    'HistÃ³rico': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['hist', 'mean', 'count'])],\n",
    "    'CategÃ³rico': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['hash', 'distributor'])],\n",
    "    'Outros': []\n",
    "}\n",
    "\n",
    "# Classificar features nÃ£o categorizadas\n",
    "categorized_features = set()\n",
    "for cat_features in feature_categories.values():\n",
    "    categorized_features.update(cat_features)\n",
    "\n",
    "feature_categories['Outros'] = [f for f in feature_imp_df['feature'] \n",
    "                               if f not in categorized_features]\n",
    "\n",
    "# Calcular importÃ¢ncia por categoria\n",
    "print(f'\\nğŸ·ï¸ IMPORTÃ‚NCIA POR CATEGORIA:')\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        total_importance = feature_imp_df[feature_imp_df['feature'].isin(features)]['importance_pct'].sum()\n",
    "        print(f'   â€¢ {category:12}: {total_importance:6.2f}% ({len(features):2d} features)')\n",
    "\n",
    "# Top features mais importantes\n",
    "print(f'\\nğŸ” TOP 20 FEATURES MAIS IMPORTANTES:')\n",
    "print('-' * 60)\n",
    "for i, (_, row) in enumerate(feature_imp_df.head(20).iterrows(), 1):\n",
    "    bar = 'â–ˆ' * int(row['importance_pct'] / 2)  # Visual bar\n",
    "    print(f'{i:2d}. {row[\"feature\"]:25} {row[\"importance_pct\"]:6.2f}% {bar}')\n",
    "\n",
    "# AnÃ¡lise crÃ­tica das features\n",
    "print(f'\\nğŸ§  ANÃLISE CRÃTICA DAS FEATURES:')\n",
    "print('=' * 60)\n",
    "\n",
    "# Verificar se lag features sÃ£o importantes\n",
    "lag_features = [f for f in feature_categories['Lag'] if f in feature_imp_df.head(10)['feature'].values]\n",
    "if lag_features:\n",
    "    print(f'âœ… FEATURES DE LAG NO TOP 10: {len(lag_features)} features')\n",
    "    print(f'   â†’ {\", \".join(lag_features[:3])}{\"...\" if len(lag_features) > 3 else \"\"}')\n",
    "else:\n",
    "    print(f'âš ï¸ POUCAS FEATURES DE LAG NO TOP 10 - PossÃ­vel problema!')\n",
    "\n",
    "# Verificar se rolling features sÃ£o importantes  \n",
    "rolling_features = [f for f in feature_categories['Rolling'] if f in feature_imp_df.head(15)['feature'].values]\n",
    "if rolling_features:\n",
    "    print(f'âœ… FEATURES ROLLING NO TOP 15: {len(rolling_features)} features')\n",
    "    print(f'   â†’ {\", \".join(rolling_features[:3])}{\"...\" if len(rolling_features) > 3 else \"\"}')\n",
    "else:\n",
    "    print(f'âš ï¸ POUCAS FEATURES ROLLING NO TOP 15')\n",
    "\n",
    "# Verificar features temporais (sazonalidade)\n",
    "temporal_features = [f for f in feature_categories['Temporal'] if f in feature_imp_df.head(20)['feature'].values]\n",
    "if temporal_features:\n",
    "    print(f'âœ… SAZONALIDADE DETECTADA: {len(temporal_features)} features temporais no TOP 20')\n",
    "    print(f'   â†’ {\", \".join(temporal_features)}')\n",
    "else:\n",
    "    print(f'âš ï¸ POUCA SAZONALIDADE DETECTADA')\n",
    "\n",
    "# Verificar distributor_id\n",
    "distributor_importance = feature_imp_df[feature_imp_df['feature'] == 'distributor_id']['importance_pct'].sum()\n",
    "if distributor_importance > 1:\n",
    "    print(f'âœ… DISTRIBUTOR_ID ÃšTIL: {distributor_importance:.2f}% importÃ¢ncia')\n",
    "    print(f'   â†’ EstratÃ©gia de NaN â†’ -1 foi acertada!')\n",
    "else:\n",
    "    print(f'âš ï¸ DISTRIBUTOR_ID POUCO ÃšTIL: {distributor_importance:.2f}% importÃ¢ncia')\n",
    "\n",
    "# Features com importÃ¢ncia zero (candidatas Ã  remoÃ§Ã£o)\n",
    "zero_importance = feature_imp_df[feature_imp_df['importance'] == 0]\n",
    "if len(zero_importance) > 0:\n",
    "    print(f'\\nğŸ—‘ï¸ FEATURES COM IMPORTÃ‚NCIA ZERO ({len(zero_importance)} candidatas Ã  remoÃ§Ã£o):')\n",
    "    for feature in zero_importance.head(10)['feature']:\n",
    "        print(f'   â€¢ {feature}')\n",
    "    if len(zero_importance) > 10:\n",
    "        print(f'   â€¢ ... e mais {len(zero_importance) - 10} features')\n",
    "else:\n",
    "    print(f'\\nâœ… TODAS AS FEATURES TÃŠM IMPORTÃ‚NCIA > 0')\n",
    "\n",
    "# VisualizaÃ§Ã£o\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_imp_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance_pct'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('ImportÃ¢ncia (%)')\n",
    "plt.title('Top 20 Features Mais Importantes - LightGBM Vanilla')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nâœ… Passo C concluÃ­do - Feature Importance analisada!')\n",
    "print(f'ğŸ“‹ PrÃ³ximo: OtimizaÃ§Ã£o de hiperparÃ¢metros com base nestas insights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PreparaÃ§Ã£o para PrediÃ§Ãµes Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ğŸ¯ PreparaÃ§Ã£o para prediÃ§Ãµes finais...')\n",
    "\n",
    "# Retreinar melhor modelo com todos os dados disponÃ­veis\n",
    "print(f'ğŸ”„ Retreinando {best_model_name} com todos os dados...')\n",
    "\n",
    "# CORREÃ‡ÃƒO: Usar dados otimizados (dados_sorted) em vez de dados originais\n",
    "print('ğŸ§  Preparando dados com tipos otimizados (mesmo processamento de treino/validaÃ§Ã£o)...')\n",
    "\n",
    "# Usar dados_sorted que jÃ¡ foram otimizados e tratados\n",
    "X_full = dados_sorted[all_features]\n",
    "y_full = dados_sorted[target]\n",
    "\n",
    "print(f'   â€¢ X_full shape: {X_full.shape}')\n",
    "print(f'   â€¢ Tipos de dados consistentes: {X_full.dtypes.value_counts().to_dict()}')\n",
    "\n",
    "if best_model_name == 'LightGBM Vanilla':\n",
    "    # Retreinar LightGBM\n",
    "    train_full_lgb = lgb.Dataset(X_full, label=y_full)\n",
    "    final_model = lgb.train(\n",
    "        lgb_params_vanilla,\n",
    "        train_full_lgb,\n",
    "        num_boost_round=lgb_vanilla.best_iteration,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "elif best_model_name == 'XGBoost':\n",
    "    # Retreinar XGBoost (agora com dados consistentes)\n",
    "    final_model = xgb.XGBRegressor(**xgb_params, n_estimators=xgb_model.best_iteration)\n",
    "    final_model.fit(X_full, y_full, verbose=False)\n",
    "    \n",
    "elif best_model_name == 'Random Forest':\n",
    "    # Retreinar Random Forest\n",
    "    final_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    final_model.fit(X_full, y_full)\n",
    "    \n",
    "else:\n",
    "    # Usar estratÃ©gia baseline\n",
    "    final_model = None\n",
    "    combo_means_full = dados_sorted.groupby(['pdv_id', 'produto_id'])['quantidade'].mean().to_dict()\n",
    "    global_mean_full = y_full.mean()\n",
    "\n",
    "print('âœ… Modelo final treinado e pronto para prediÃ§Ãµes')\n",
    "\n",
    "# Salvar modelo e configuraÃ§Ãµes\n",
    "model_artifacts = {\n",
    "    'model': final_model,\n",
    "    'model_type': best_model_name,\n",
    "    'features': all_features,\n",
    "    'target': target,\n",
    "    'validation_mae': results_df.iloc[0]['MAE'],\n",
    "    'validation_rmse': results_df.iloc[0]['RMSE'],\n",
    "    'validation_r2': results_df.iloc[0]['RÂ²'],\n",
    "    'validation_wmape': results_df.iloc[0]['WMAPE'],\n",
    "    'training_date': pd.Timestamp.now(),\n",
    "    'combo_means': combo_means_full if best_model_name not in ['LightGBM Vanilla', 'XGBoost', 'Random Forest'] else None,\n",
    "    'metadata': metadata\n",
    "}\n",
    "\n",
    "# Salvar artefatos do modelo\n",
    "with open('../data/trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print('ğŸ’¾ Modelo e artefatos salvos em: data/trained_model.pkl')\n",
    "print('ğŸ¯ Pronto para gerar prediÃ§Ãµes para o perÃ­odo de teste!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo e PrÃ³ximos Passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ğŸ‰ MODELAGEM CONCLUÃDA COM SUCESSO!')\n",
    "print('=' * 60)\n",
    "\n",
    "print(f'\\nğŸ† Melhor Modelo: {best_model_name}')\n",
    "print(f'   â€¢ MAE: {results_df.iloc[0][\"MAE\"]:.4f}')\n",
    "print(f'   â€¢ RMSE: {results_df.iloc[0][\"RMSE\"]:.4f}')\n",
    "print(f'   â€¢ RÂ²: {results_df.iloc[0][\"RÂ²\"]:.4f}')\n",
    "print(f'   â€¢ WMAPE: {results_df.iloc[0][\"WMAPE\"]:.2f}%')\n",
    "\n",
    "improvement_over_baseline = (results_df[results_df['Model'] == 'MÃ©dia Simples']['MAE'].iloc[0] - results_df.iloc[0]['MAE']) / results_df[results_df['Model'] == 'MÃ©dia Simples']['MAE'].iloc[0] * 100\n",
    "print(f'   â€¢ Melhoria sobre baseline: {improvement_over_baseline:.1f}%')\n",
    "\n",
    "print(f'\\nğŸ“Š ComparaÃ§Ã£o de Modelos:')\n",
    "for i, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    print(f'   {i}. {row[\"Model\"]}: MAE = {row[\"MAE\"]:.4f}, WMAPE = {row[\"WMAPE\"]:.2f}%')\n",
    "\n",
    "print(f'\\nğŸ’¾ Artefatos Salvos:')\n",
    "print('   âœ… trained_model.pkl - Modelo treinado e configuraÃ§Ãµes')\n",
    "print('   âœ… feature_engineering_metadata.pkl - Metadados do processamento')\n",
    "print('   âœ… dados_features_completo.parquet - Dataset com features')\n",
    "\n",
    "print(f'\\nğŸ”„ PrÃ³ximos Passos:')\n",
    "print('   1. ğŸ“… Criar dados de teste para as 5 semanas de 2023')\n",
    "print('   2. ğŸ¯ Gerar prediÃ§Ãµes usando o modelo treinado')\n",
    "print('   3. ğŸ†• Aplicar estratÃ©gia para novas combinaÃ§Ãµes (prediÃ§Ã£o = 0)')\n",
    "print('   4. ğŸ“‹ Criar arquivo de submissÃ£o no formato requerido')\n",
    "print('   5. ğŸ§ª Validar prediÃ§Ãµes e fazer anÃ¡lise final')\n",
    "\n",
    "print(f'\\nğŸš€ SISTEMA DE FORECASTING COMPLETO E PRONTO!')\n",
    "print('   â€¢ Grid Inteligente com otimizaÃ§Ã£o de memÃ³ria')\n",
    "print('   â€¢ Features avanÃ§adas (30+ variÃ¡veis)')\n",
    "print('   â€¢ Modelo ML de alta performance')\n",
    "print('   â€¢ ValidaÃ§Ã£o temporal robusta')\n",
    "print('   â€¢ Pipeline completo e automatizado')\n",
    "\n",
    "print('\\nâœ… Modelagem e Treinamento CONCLUÃDOS!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
