{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria de Dados (EDA) - Hackathon Forecast 2025\n",
    "\n",
    "Este notebook realiza a an√°lise explorat√≥ria completa dos dados do hackathon, incluindo:\n",
    "1. Carregamento e inspe√ß√£o dos tr√™s arquivos Parquet\n",
    "2. An√°lise das transa√ß√µes como s√©rie temporal\n",
    "3. Identifica√ß√£o de tend√™ncias e sazonalidade\n",
    "4. An√°lise de distribui√ß√µes\n",
    "5. **An√°lise dos cadastros (PDVs e Produtos)**\n",
    "6. **Long Tail Analysis**\n",
    "7. **Cruzamento de dados (Merge)**\n",
    "8. **An√°lises por categoria de PDV**\n",
    "9. **An√°lises geogr√°ficas (Zipcode)**\n",
    "10. **Insights e recomenda√ß√µes para modelagem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Bibliotecas carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Inspe√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os tr√™s arquivos Parquet\n",
    "data_path = '../data/'\n",
    "\n",
    "# Lista dos arquivos parquet dispon√≠veis\n",
    "import os\n",
    "parquet_files = [f for f in os.listdir(data_path) if f.endswith('.parquet')]\n",
    "print(f\"Arquivos encontrados: {len(parquet_files)}\")\n",
    "for file in parquet_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Carregar cada arquivo e identificar sua natureza pelos dados\n",
    "dataframes = {}\n",
    "for i, file in enumerate(parquet_files):\n",
    "    df = pd.read_parquet(os.path.join(data_path, file))\n",
    "    dataframes[f'df_{i+1}'] = df\n",
    "    print(f\"\\n{file}:\")\n",
    "    print(f\"  Forma: {df.shape}\")\n",
    "    print(f\"  Colunas: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar qual dataset √© qual baseado nas colunas\n",
    "transacoes_df = None\n",
    "produtos_df = None\n",
    "pdvs_df = None\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    # Identificar transa√ß√µes (maior dataset, com quantity/transaction_date)\n",
    "    if len(df) > 100000:\n",
    "        transacoes_df = df\n",
    "        print(f\"\\n{key} identificado como TRANSA√á√ïES\")\n",
    "        \n",
    "    # Identificar produtos (tem produto, categoria, marca)\n",
    "    elif any('produto' in col.lower() for col in cols) or any('categoria' in col.lower() for col in cols):\n",
    "        produtos_df = df\n",
    "        print(f\"\\n{key} identificado como PRODUTOS\")\n",
    "        \n",
    "    # Identificar PDVs (tem pdv, premise)\n",
    "    elif any('pdv' in col.lower() for col in cols) or any('premise' in col.lower() for col in cols):\n",
    "        pdvs_df = df\n",
    "        print(f\"\\n{key} identificado como PDVs\")\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets identificados:\")\n",
    "print(f\"  - Transa√ß√µes: {transacoes_df.shape if transacoes_df is not None else 'N√£o encontrado'}\")\n",
    "print(f\"  - Produtos: {produtos_df.shape if produtos_df is not None else 'N√£o encontrado'}\")\n",
    "print(f\"  - PDVs: {pdvs_df.shape if pdvs_df is not None else 'N√£o encontrado'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspe√ß√£o Detalhada dos Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspecionar_dataset(df, nome):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"AN√ÅLISE DO DATASET: {nome}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(f\"\\nüìä INFORMA√á√ïES GERAIS:\")\n",
    "    print(f\"   Forma: {df.shape}\")\n",
    "    print(f\"   Colunas: {list(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nüîç TIPOS DE DADOS E VALORES NULOS:\")\n",
    "    info_df = pd.DataFrame({\n",
    "        'Tipo': df.dtypes,\n",
    "        'Nulos': df.isnull().sum(),\n",
    "        '% Nulos': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        '√önicos': df.nunique()\n",
    "    })\n",
    "    print(info_df)\n",
    "    \n",
    "    print(f\"\\nüìà ESTAT√çSTICAS DESCRITIVAS (Colunas Num√©ricas):\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(df[numeric_cols].describe())\n",
    "    else:\n",
    "        print(\"   Nenhuma coluna num√©rica encontrada\")\n",
    "    \n",
    "    print(f\"\\nüëÄ PRIMEIRAS 5 LINHAS:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar cada dataset\n",
    "if transacoes_df is not None:\n",
    "    transacoes_info = inspecionar_dataset(transacoes_df, \"TRANSA√á√ïES\")\n",
    "\n",
    "if produtos_df is not None:\n",
    "    produtos_info = inspecionar_dataset(produtos_df, \"PRODUTOS\")\n",
    "\n",
    "if pdvs_df is not None:\n",
    "    pdvs_info = inspecionar_dataset(pdvs_df, \"PDVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise da S√©rie Temporal de Transa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados de transa√ß√µes para an√°lise temporal\n",
    "if transacoes_df is not None:\n",
    "    # Identificar coluna de data\n",
    "    date_cols = [col for col in transacoes_df.columns if 'date' in col.lower()]\n",
    "    \n",
    "    print(f\"Colunas de data identificadas: {date_cols}\")\n",
    "    \n",
    "    if date_cols:\n",
    "        date_col = 'transaction_date'  # Usar a coluna de transa√ß√£o\n",
    "        # Converter para datetime se necess√°rio\n",
    "        if transacoes_df[date_col].dtype != 'datetime64[ns]':\n",
    "            transacoes_df[date_col] = pd.to_datetime(transacoes_df[date_col], errors='coerce')\n",
    "        \n",
    "        print(f\"\\nUsando coluna de data: {date_col}\")\n",
    "        print(f\"Per√≠odo dos dados: {transacoes_df[date_col].min()} at√© {transacoes_df[date_col].max()}\")\n",
    "        print(f\"Total de dias: {(transacoes_df[date_col].max() - transacoes_df[date_col].min()).days + 1}\")\n",
    "    else:\n",
    "        print(\"‚ùå N√£o foi poss√≠vel identificar coluna de data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas de quantidade e faturamento\n",
    "if transacoes_df is not None and date_cols:\n",
    "    # Definir colunas baseado no que vimos\n",
    "    qty_col = 'quantity'\n",
    "    revenue_col = 'gross_value'\n",
    "    \n",
    "    print(f\"Colunas identificadas:\")\n",
    "    print(f\"  Data: {date_col}\")\n",
    "    print(f\"  Quantidade: {qty_col}\")\n",
    "    print(f\"  Faturamento: {revenue_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar dados por dia\n",
    "if transacoes_df is not None and date_cols:\n",
    "    daily_data = transacoes_df.groupby(date_col).agg({\n",
    "        qty_col: ['sum', 'count'],\n",
    "        revenue_col: 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Simplificar nomes das colunas\n",
    "    daily_data.columns = ['Quantidade_Total', 'Num_Transacoes', 'Faturamento_Total']\n",
    "    daily_data = daily_data.reset_index()\n",
    "    \n",
    "    print(f\"\\nüìä RESUMO DA S√âRIE TEMPORAL DI√ÅRIA:\")\n",
    "    print(f\"   Total de dias com dados: {len(daily_data)}\")\n",
    "    print(f\"   Quantidade total vendida: {daily_data['Quantidade_Total'].sum():,.0f}\")\n",
    "    print(f\"   Faturamento total: R$ {daily_data['Faturamento_Total'].sum():,.2f}\")\n",
    "    print(f\"   N√∫mero total de transa√ß√µes: {daily_data['Num_Transacoes'].sum():,.0f}\")\n",
    "    \n",
    "    print(f\"\\nüìà ESTAT√çSTICAS DI√ÅRIAS:\")\n",
    "    display(daily_data[['Quantidade_Total', 'Num_Transacoes', 'Faturamento_Total']].describe())\n",
    "    \n",
    "    display(daily_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualiza√ß√µes da S√©rie Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar s√©rie temporal completa\n",
    "if 'daily_data' in locals():\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Gr√°fico 1: Quantidade Total por Dia\n",
    "    axes[0].plot(daily_data[date_col], daily_data['Quantidade_Total'], linewidth=1, alpha=0.7, color='blue')\n",
    "    axes[0].set_title('üì¶ Quantidade Total Vendida por Dia', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Quantidade')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Gr√°fico 2: Faturamento por Dia\n",
    "    axes[1].plot(daily_data[date_col], daily_data['Faturamento_Total'], linewidth=1, alpha=0.7, color='green')\n",
    "    axes[1].set_title('üí∞ Faturamento Total por Dia', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Faturamento (R$)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Gr√°fico 3: N√∫mero de Transa√ß√µes por Dia\n",
    "    axes[2].plot(daily_data[date_col], daily_data['Num_Transacoes'], linewidth=1, alpha=0.7, color='orange')\n",
    "    axes[2].set_title('üõí N√∫mero de Transa√ß√µes por Dia', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('N√∫mero de Transa√ß√µes')\n",
    "    axes[2].set_xlabel('Data')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Adicionar m√©dias m√≥veis para identificar tend√™ncias\n",
    "    daily_data['Quantidade_MA7'] = daily_data['Quantidade_Total'].rolling(window=7).mean()\n",
    "    daily_data['Quantidade_MA30'] = daily_data['Quantidade_Total'].rolling(window=30).mean()\n",
    "    daily_data['Faturamento_MA7'] = daily_data['Faturamento_Total'].rolling(window=7).mean()\n",
    "    daily_data['Faturamento_MA30'] = daily_data['Faturamento_Total'].rolling(window=30).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise de Sazonalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de sazonalidade\n",
    "if 'daily_data' in locals():\n",
    "    # Extrair componentes de data\n",
    "    daily_data['Ano'] = daily_data[date_col].dt.year\n",
    "    daily_data['Mes'] = daily_data[date_col].dt.month\n",
    "    daily_data['Dia_Semana'] = daily_data[date_col].dt.dayofweek  # 0=Segunda, 6=Domingo\n",
    "    daily_data['Nome_Dia_Semana'] = daily_data[date_col].dt.day_name()\n",
    "    daily_data['Nome_Mes'] = daily_data[date_col].dt.month_name()\n",
    "    daily_data['Dia_Mes'] = daily_data[date_col].dt.day\n",
    "    daily_data['Semana_Ano'] = daily_data[date_col].dt.isocalendar().week\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Vendas por m√™s\n",
    "    monthly_sales = daily_data.groupby('Nome_Mes')['Quantidade_Total'].mean()\n",
    "    month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "                   'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    monthly_sales = monthly_sales.reindex([m for m in month_order if m in monthly_sales.index])\n",
    "    \n",
    "    monthly_sales.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('üìÖ Quantidade M√©dia Vendida por M√™s', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Quantidade M√©dia')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Vendas por dia da semana\n",
    "    weekday_sales = daily_data.groupby('Nome_Dia_Semana')['Quantidade_Total'].mean()\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekday_sales = weekday_sales.reindex([d for d in weekday_order if d in weekday_sales.index])\n",
    "    \n",
    "    weekday_sales.plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "    axes[0,1].set_title('üìä Quantidade M√©dia Vendida por Dia da Semana', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('Quantidade M√©dia')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Faturamento por m√™s\n",
    "    monthly_revenue = daily_data.groupby('Nome_Mes')['Faturamento_Total'].mean()\n",
    "    monthly_revenue = monthly_revenue.reindex([m for m in month_order if m in monthly_revenue.index])\n",
    "    \n",
    "    monthly_revenue.plot(kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "    axes[1,0].set_title('üí∞ Faturamento M√©dio por M√™s', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Faturamento M√©dio (R$)')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Faturamento por dia da semana\n",
    "    weekday_revenue = daily_data.groupby('Nome_Dia_Semana')['Faturamento_Total'].mean()\n",
    "    weekday_revenue = weekday_revenue.reindex([d for d in weekday_order if d in weekday_revenue.index])\n",
    "    \n",
    "    weekday_revenue.plot(kind='bar', ax=axes[1,1], color='gold')\n",
    "    axes[1,1].set_title('üí∏ Faturamento M√©dio por Dia da Semana', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Faturamento M√©dio (R$)')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agrega√ß√£o para Dados Semanais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como o objetivo √© prever vendas semanais, vamos agregar os dados\n",
    "if 'daily_data' in locals():\n",
    "    # Criar dados semanais\n",
    "    daily_data['Semana'] = daily_data[date_col].dt.to_period('W').dt.start_time\n",
    "    \n",
    "    weekly_data = daily_data.groupby('Semana').agg({\n",
    "        'Quantidade_Total': 'sum',\n",
    "        'Faturamento_Total': 'sum',\n",
    "        'Num_Transacoes': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\"üìÖ DADOS SEMANAIS AGREGADOS\")\n",
    "    print(f\"=\" * 40)\n",
    "    print(f\"Total de semanas: {len(weekly_data)}\")\n",
    "    print(f\"Per√≠odo: {weekly_data['Semana'].min().strftime('%Y-%m-%d')} at√© {weekly_data['Semana'].max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS SEMANAIS:\")\n",
    "    display(weekly_data[['Quantidade_Total', 'Faturamento_Total', 'Num_Transacoes']].describe())\n",
    "    \n",
    "    # Visualizar dados semanais\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    axes[0].plot(weekly_data['Semana'], weekly_data['Quantidade_Total'], marker='o', linewidth=2, markersize=4)\n",
    "    axes[0].set_title('üì¶ Quantidade Total Vendida por Semana', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Quantidade Total')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(weekly_data['Semana'], weekly_data['Faturamento_Total'], marker='o', linewidth=2, markersize=4, color='green')\n",
    "    axes[1].set_title('üí∞ Faturamento Total por Semana', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Faturamento Total (R$)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(weekly_data['Semana'], weekly_data['Num_Transacoes'], marker='o', linewidth=2, markersize=4, color='orange')\n",
    "    axes[2].set_title('üõí N√∫mero Total de Transa√ß√µes por Semana', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('N√∫mero de Transa√ß√µes')\n",
    "    axes[2].set_xlabel('Semana')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar algumas semanas\n",
    "    print(f\"\\nüëÄ PRIMEIRAS 10 SEMANAS:\")\n",
    "    display(weekly_data.head(10))\n",
    "    \n",
    "    print(f\"\\nüëÄ √öLTIMAS 10 SEMANAS:\")\n",
    "    display(weekly_data.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise dos Cadastros (Enriquecimento)\n",
    "\n",
    "Agora vamos analisar os dados cadastrais de PDVs e Produtos para entender melhor o neg√≥cio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada dos PDVs\n",
    "print(\"üè™ AN√ÅLISE DOS PONTOS DE VENDA (PDVs)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if pdvs_df is not None:\n",
    "    print(f\"\\nüìä RESUMO GERAL:\")\n",
    "    print(f\"   ‚Ä¢ Total de PDVs: {len(pdvs_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Colunas: {list(pdvs_df.columns)}\")\n",
    "    \n",
    "    # Distribui√ß√£o por Premise (On/Off)\n",
    "    print(f\"\\nüè¢ DISTRIBUI√á√ÉO POR TIPO DE PREMISE:\")\n",
    "    premise_dist = pdvs_df['premise'].value_counts()\n",
    "    premise_pct = pdvs_df['premise'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for premise, count in premise_dist.items():\n",
    "        print(f\"   ‚Ä¢ {premise}: {count:,} ({premise_pct[premise]:.1f}%)\")\n",
    "    \n",
    "    # Distribui√ß√£o por Categoria de PDV\n",
    "    print(f\"\\nüè∑Ô∏è DISTRIBUI√á√ÉO POR CATEGORIA DE PDV:\")\n",
    "    categoria_dist = pdvs_df['categoria_pdv'].value_counts().head(15)  # Top 15\n",
    "    categoria_pct = pdvs_df['categoria_pdv'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for i, (categoria, count) in enumerate(categoria_dist.items()):\n",
    "        print(f\"   {i+1:2d}. {categoria}: {count:,} ({categoria_pct[categoria]:.1f}%)\")\n",
    "    \n",
    "    # Distribui√ß√£o geogr√°fica (Zipcode)\n",
    "    print(f\"\\nüó∫Ô∏è DISTRIBUI√á√ÉO GEOGR√ÅFICA:\")\n",
    "    zipcode_stats = pdvs_df['zipcode'].describe()\n",
    "    print(f\"   ‚Ä¢ Zipcodes √∫nicos: {pdvs_df['zipcode'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Zipcode m√©dio: {zipcode_stats['mean']:.0f}\")\n",
    "    print(f\"   ‚Ä¢ Range: {zipcode_stats['min']:.0f} - {zipcode_stats['max']:.0f}\")\n",
    "    \n",
    "    # Top zipcodes com mais PDVs\n",
    "    print(f\"\\nüèÜ TOP 10 ZIPCODES COM MAIS PDVs:\")\n",
    "    top_zipcodes = pdvs_df['zipcode'].value_counts().head(10)\n",
    "    for zipcode, count in top_zipcodes.items():\n",
    "        print(f\"   ‚Ä¢ {zipcode}: {count:,} PDVs\")\n",
    "\n",
    "    # Visualiza√ß√µes\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Gr√°fico 1: Distribui√ß√£o por Premise\n",
    "    premise_dist.plot(kind='pie', ax=axes[0,0], autopct='%1.1f%%', startangle=90)\n",
    "    axes[0,0].set_title('üè¢ Distribui√ß√£o de PDVs por Tipo de Premise', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('')\n",
    "    \n",
    "    # Gr√°fico 2: Top 15 Categorias de PDV\n",
    "    categoria_dist.head(15).plot(kind='barh', ax=axes[0,1], color='skyblue')\n",
    "    axes[0,1].set_title('üè∑Ô∏è Top 15 Categorias de PDV', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('N√∫mero de PDVs')\n",
    "    \n",
    "    # Gr√°fico 3: Distribui√ß√£o de Zipcodes\n",
    "    axes[1,0].hist(pdvs_df['zipcode'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1,0].set_title('üó∫Ô∏è Distribui√ß√£o de Zipcodes', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Zipcode')\n",
    "    axes[1,0].set_ylabel('N√∫mero de PDVs')\n",
    "    \n",
    "    # Gr√°fico 4: Top 10 Zipcodes\n",
    "    top_zipcodes.plot(kind='bar', ax=axes[1,1], color='lightgreen')\n",
    "    axes[1,1].set_title('üèÜ Top 10 Zipcodes com Mais PDVs', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('N√∫mero de PDVs')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise detalhada dos Produtos\n",
    "print(\"\\nüç∫ AN√ÅLISE DOS PRODUTOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if produtos_df is not None:\n",
    "    print(f\"\\nüìä RESUMO GERAL:\")\n",
    "    print(f\"   ‚Ä¢ Total de produtos: {len(produtos_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Colunas: {list(produtos_df.columns)}\")\n",
    "    \n",
    "    # Distribui√ß√£o por Categoria\n",
    "    print(f\"\\nüè∑Ô∏è DISTRIBUI√á√ÉO POR CATEGORIA:\")\n",
    "    categoria_prod_dist = produtos_df['categoria'].value_counts()\n",
    "    categoria_prod_pct = produtos_df['categoria'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for categoria, count in categoria_prod_dist.items():\n",
    "        print(f\"   ‚Ä¢ {categoria}: {count:,} ({categoria_prod_pct[categoria]:.1f}%)\")\n",
    "    \n",
    "    # Distribui√ß√£o por Subcategoria (Top 15)\n",
    "    print(f\"\\nüìÇ TOP 15 SUBCATEGORIAS:\")\n",
    "    subcategoria_dist = produtos_df['subcategoria'].value_counts().head(15)\n",
    "    subcategoria_pct = produtos_df['subcategoria'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for i, (subcat, count) in enumerate(subcategoria_dist.items()):\n",
    "        if pd.notna(subcat):  # Ignorar valores NaN\n",
    "            print(f\"   {i+1:2d}. {subcat}: {count:,} ({subcategoria_pct[subcat]:.1f}%)\")\n",
    "    \n",
    "    # An√°lise de Marcas\n",
    "    print(f\"\\nüè≠ AN√ÅLISE DE MARCAS:\")\n",
    "    print(f\"   ‚Ä¢ Total de marcas √∫nicas: {produtos_df['marca'].nunique():,}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP 15 MARCAS COM MAIS PRODUTOS:\")\n",
    "    top_marcas = produtos_df['marca'].value_counts().head(15)\n",
    "    marca_pct = produtos_df['marca'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for i, (marca, count) in enumerate(top_marcas.items()):\n",
    "        print(f\"   {i+1:2d}. {marca}: {count:,} ({marca_pct[marca]:.2f}%)\")\n",
    "    \n",
    "    # An√°lise de Fabricantes\n",
    "    print(f\"\\nüè≠ AN√ÅLISE DE FABRICANTES:\")\n",
    "    print(f\"   ‚Ä¢ Total de fabricantes √∫nicos: {produtos_df['fabricante'].nunique():,}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP 10 FABRICANTES COM MAIS PRODUTOS:\")\n",
    "    top_fabricantes = produtos_df['fabricante'].value_counts().head(10)\n",
    "    fabricante_pct = produtos_df['fabricante'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for i, (fabricante, count) in enumerate(top_fabricantes.items()):\n",
    "        print(f\"   {i+1:2d}. {fabricante}: {count:,} ({fabricante_pct[fabricante]:.2f}%)\")\n",
    "    \n",
    "    # An√°lise do campo Label\n",
    "    print(f\"\\nüè∑Ô∏è DISTRIBUI√á√ÉO POR LABEL:\")\n",
    "    if produtos_df['label'].notna().sum() > 0:\n",
    "        label_dist = produtos_df['label'].value_counts()\n",
    "        label_pct = produtos_df['label'].value_counts(normalize=True) * 100\n",
    "        \n",
    "        for label, count in label_dist.items():\n",
    "            print(f\"   ‚Ä¢ {label}: {count:,} ({label_pct[label]:.1f}%)\")\n",
    "        \n",
    "        missing_labels = produtos_df['label'].isna().sum()\n",
    "        print(f\"   ‚Ä¢ Missing/NaN: {missing_labels:,} ({missing_labels/len(produtos_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualiza√ß√µes\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Gr√°fico 1: Distribui√ß√£o por Categoria\n",
    "    categoria_prod_dist.plot(kind='pie', ax=axes[0,0], autopct='%1.1f%%', startangle=90)\n",
    "    axes[0,0].set_title('üè∑Ô∏è Distribui√ß√£o de Produtos por Categoria', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('')\n",
    "    \n",
    "    # Gr√°fico 2: Top 15 Subcategorias\n",
    "    subcategoria_dist.head(15).plot(kind='barh', ax=axes[0,1], color='lightblue')\n",
    "    axes[0,1].set_title('üìÇ Top 15 Subcategorias', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('N√∫mero de Produtos')\n",
    "    \n",
    "    # Gr√°fico 3: Top 15 Marcas\n",
    "    top_marcas.head(15).plot(kind='barh', ax=axes[0,2], color='lightgreen')\n",
    "    axes[0,2].set_title('üèÜ Top 15 Marcas', fontweight='bold')\n",
    "    axes[0,2].set_xlabel('N√∫mero de Produtos')\n",
    "    \n",
    "    # Gr√°fico 4: Top 10 Fabricantes\n",
    "    top_fabricantes.plot(kind='bar', ax=axes[1,0], color='coral')\n",
    "    axes[1,0].set_title('üè≠ Top 10 Fabricantes', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('N√∫mero de Produtos')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Gr√°fico 5: Distribui√ß√£o de Labels\n",
    "    if produtos_df['label'].notna().sum() > 0:\n",
    "        label_dist.plot(kind='bar', ax=axes[1,1], color='gold')\n",
    "        axes[1,1].set_title('üè∑Ô∏è Distribui√ß√£o por Label', fontweight='bold')\n",
    "        axes[1,1].set_ylabel('N√∫mero de Produtos')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Gr√°fico 6: Distribui√ß√£o por Tipos\n",
    "    tipos_dist = produtos_df['tipos'].value_counts().head(15)\n",
    "    tipos_dist.plot(kind='barh', ax=axes[1,2], color='plum')\n",
    "    axes[1,2].set_title('üì¶ Top 15 Tipos de Produto', fontweight='bold')\n",
    "    axes[1,2].set_xlabel('N√∫mero de Produtos')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lise de Long Tail (Distribui√ß√£o 80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Long Tail - Marcas e Categorias\n",
    "print(\"\\nüìä AN√ÅLISE DE LONG TAIL (Distribui√ß√£o 80/20)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if produtos_df is not None:\n",
    "    # An√°lise Long Tail - Marcas\n",
    "    print(f\"\\nüè≠ LONG TAIL - MARCAS:\")\n",
    "    marca_counts = produtos_df['marca'].value_counts()\n",
    "    total_produtos = len(produtos_df)\n",
    "    \n",
    "    # Calcular percentuais acumulativos\n",
    "    marca_cumsum = marca_counts.cumsum()\n",
    "    marca_cumsum_pct = (marca_cumsum / total_produtos) * 100\n",
    "    \n",
    "    # Encontrar quantas marcas representam 80% dos produtos\n",
    "    marcas_80pct = (marca_cumsum_pct <= 80).sum()\n",
    "    produtos_80pct = marca_cumsum.iloc[marcas_80pct-1] if marcas_80pct > 0 else 0\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total de marcas: {len(marca_counts):,}\")\n",
    "    print(f\"   ‚Ä¢ Top {marcas_80pct} marcas representam 80% dos produtos ({produtos_80pct:,} produtos)\")\n",
    "    print(f\"   ‚Ä¢ Concentra√ß√£o: {marcas_80pct/len(marca_counts)*100:.1f}% das marcas = 80% dos produtos\")\n",
    "    \n",
    "    # Marcas com apenas 1 produto (cauda longa)\n",
    "    marcas_1produto = (marca_counts == 1).sum()\n",
    "    print(f\"   ‚Ä¢ Marcas com apenas 1 produto: {marcas_1produto:,} ({marcas_1produto/len(marca_counts)*100:.1f}%)\")\n",
    "    \n",
    "    # An√°lise Long Tail - Categorias  \n",
    "    print(f\"\\nüè∑Ô∏è LONG TAIL - CATEGORIAS:\")\n",
    "    categoria_counts = produtos_df['categoria'].value_counts()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total de categorias: {len(categoria_counts):,}\")\n",
    "    for i, (categoria, count) in enumerate(categoria_counts.items()):\n",
    "        pct = count / total_produtos * 100\n",
    "        print(f\"   {i+1}. {categoria}: {count:,} produtos ({pct:.1f}%)\")\n",
    "    \n",
    "    # An√°lise Long Tail - Fabricantes\n",
    "    print(f\"\\nüè≠ LONG TAIL - FABRICANTES:\")\n",
    "    fabricante_counts = produtos_df['fabricante'].value_counts()\n",
    "    \n",
    "    # Calcular percentuais acumulativos para fabricantes\n",
    "    fabricante_cumsum = fabricante_counts.cumsum()\n",
    "    fabricante_cumsum_pct = (fabricante_cumsum / total_produtos) * 100\n",
    "    \n",
    "    # Encontrar quantos fabricantes representam 80% dos produtos\n",
    "    fabricantes_80pct = (fabricante_cumsum_pct <= 80).sum()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total de fabricantes: {len(fabricante_counts):,}\")\n",
    "    print(f\"   ‚Ä¢ Top {fabricantes_80pct} fabricantes representam 80% dos produtos\")\n",
    "    print(f\"   ‚Ä¢ Concentra√ß√£o: {fabricantes_80pct/len(fabricante_counts)*100:.1f}% dos fabricantes = 80% dos produtos\")\n",
    "    \n",
    "    # Fabricantes com apenas 1 produto\n",
    "    fabricantes_1produto = (fabricante_counts == 1).sum()\n",
    "    print(f\"   ‚Ä¢ Fabricantes com apenas 1 produto: {fabricantes_1produto:,} ({fabricantes_1produto/len(fabricante_counts)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualiza√ß√µes Long Tail\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Gr√°fico 1: Pareto - Top 20 Marcas\n",
    "    top20_marcas = marca_counts.head(20)\n",
    "    pareto_data = pd.DataFrame({\n",
    "        'Marcas': top20_marcas.values,\n",
    "        'Cumsum_Pct': (top20_marcas.cumsum() / total_produtos * 100).values\n",
    "    }, index=top20_marcas.index)\n",
    "    \n",
    "    ax1 = axes[0,0]\n",
    "    ax1_twin = ax1.twinx()\n",
    "    \n",
    "    bars = ax1.bar(range(len(top20_marcas)), pareto_data['Marcas'], alpha=0.7, color='skyblue')\n",
    "    line = ax1_twin.plot(range(len(top20_marcas)), pareto_data['Cumsum_Pct'], 'ro-', color='red', linewidth=2)\n",
    "    \n",
    "    ax1.set_title('üìä An√°lise de Pareto - Top 20 Marcas', fontweight='bold')\n",
    "    ax1.set_xlabel('Marcas')\n",
    "    ax1.set_ylabel('N√∫mero de Produtos', color='blue')\n",
    "    ax1_twin.set_ylabel('% Acumulativo', color='red')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1_twin.axhline(y=80, color='green', linestyle='--', label='80%')\n",
    "    ax1_twin.legend(['% Acumulativo', '80%'])\n",
    "    \n",
    "    # Gr√°fico 2: Distribui√ß√£o de Produtos por Marca (Log Scale)\n",
    "    axes[0,1].hist(marca_counts, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,1].set_yscale('log')\n",
    "    axes[0,1].set_title('üìà Distribui√ß√£o de Produtos por Marca (Log Scale)', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('N√∫mero de Produtos por Marca')\n",
    "    axes[0,1].set_ylabel('Frequ√™ncia (Log)')\n",
    "    \n",
    "    # Gr√°fico 3: Pareto - Top 15 Fabricantes\n",
    "    top15_fabricantes = fabricante_counts.head(15)\n",
    "    pareto_fab_data = pd.DataFrame({\n",
    "        'Fabricantes': top15_fabricantes.values,\n",
    "        'Cumsum_Pct': (top15_fabricantes.cumsum() / total_produtos * 100).values\n",
    "    }, index=top15_fabricantes.index)\n",
    "    \n",
    "    ax3 = axes[1,0]\n",
    "    ax3_twin = ax3.twinx()\n",
    "    \n",
    "    bars3 = ax3.bar(range(len(top15_fabricantes)), pareto_fab_data['Fabricantes'], alpha=0.7, color='coral')\n",
    "    line3 = ax3_twin.plot(range(len(top15_fabricantes)), pareto_fab_data['Cumsum_Pct'], 'ro-', color='red', linewidth=2)\n",
    "    \n",
    "    ax3.set_title('üè≠ An√°lise de Pareto - Top 15 Fabricantes', fontweight='bold')\n",
    "    ax3.set_xlabel('Fabricantes')\n",
    "    ax3.set_ylabel('N√∫mero de Produtos', color='coral')\n",
    "    ax3_twin.set_ylabel('% Acumulativo', color='red')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3_twin.axhline(y=80, color='green', linestyle='--')\n",
    "    \n",
    "    # Gr√°fico 4: Distribui√ß√£o de Produtos por Fabricante (Log Scale)\n",
    "    axes[1,1].hist(fabricante_counts, bins=50, alpha=0.7, color='plum', edgecolor='black')\n",
    "    axes[1,1].set_yscale('log')\n",
    "    axes[1,1].set_title('üìà Distribui√ß√£o de Produtos por Fabricante (Log Scale)', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('N√∫mero de Produtos por Fabricante')\n",
    "    axes[1,1].set_ylabel('Frequ√™ncia (Log)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cruzamento de Dados (Merge dos DataFrames)\n",
    "\n",
    "Agora vamos fazer o merge dos dados de transa√ß√µes com PDVs e produtos para an√°lises mais profundas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dos DataFrames\n",
    "print(\"üîó REALIZANDO MERGE DOS DADOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Primeiro, vamos identificar as chaves de join corretas\n",
    "print(f\"\\nüîç IDENTIFICANDO CHAVES DE JOIN:\")\n",
    "if transacoes_df is not None:\n",
    "    print(f\"   Transa√ß√µes:\")\n",
    "    print(f\"   ‚Ä¢ Colunas: {list(transacoes_df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Poss√≠veis chaves PDV: {[col for col in transacoes_df.columns if 'store' in col.lower() or 'pdv' in col.lower()]}\")\n",
    "    print(f\"   ‚Ä¢ Poss√≠veis chaves Produto: {[col for col in transacoes_df.columns if 'product' in col.lower() or 'produto' in col.lower()]}\")\n",
    "\n",
    "if pdvs_df is not None:\n",
    "    print(f\"   PDVs:\")\n",
    "    print(f\"   ‚Ä¢ Colunas: {list(pdvs_df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Chave identificada: 'pdv'\")\n",
    "\n",
    "if produtos_df is not None:\n",
    "    print(f\"   Produtos:\")\n",
    "    print(f\"   ‚Ä¢ Colunas: {list(produtos_df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Chave identificada: 'produto'\")\n",
    "\n",
    "# Realizar o merge\n",
    "merged_data = None\n",
    "\n",
    "try:\n",
    "    # Merge com PDVs\n",
    "    print(f\"\\nüîó MERGE COM DADOS DE PDV:\")\n",
    "    \n",
    "    # Primeiro merge: Transa√ß√µes + PDVs\n",
    "    if transacoes_df is not None and pdvs_df is not None:\n",
    "        # Identificar a chave correta para PDV\n",
    "        pdv_key_transacoes = 'internal_store_id'  # Da an√°lise dos dados\n",
    "        pdv_key_pdvs = 'pdv'\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Usando chaves: {pdv_key_transacoes} ‚Üê ‚Üí {pdv_key_pdvs}\")\n",
    "        print(f\"   ‚Ä¢ Transa√ß√µes √∫nicas: {transacoes_df[pdv_key_transacoes].nunique():,}\")\n",
    "        print(f\"   ‚Ä¢ PDVs √∫nicos: {pdvs_df[pdv_key_pdvs].nunique():,}\")\n",
    "        \n",
    "        # Verificar sobreposi√ß√£o\n",
    "        transacoes_stores = set(transacoes_df[pdv_key_transacoes].unique())\n",
    "        pdvs_stores = set(pdvs_df[pdv_key_pdvs].unique())\n",
    "        overlap_stores = transacoes_stores.intersection(pdvs_stores)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Sobreposi√ß√£o: {len(overlap_stores):,} PDVs ({len(overlap_stores)/len(pdvs_stores)*100:.1f}% dos PDVs cadastrados)\")\n",
    "        \n",
    "        # Realizar merge\n",
    "        merged_data = transacoes_df.merge(\n",
    "            pdvs_df, \n",
    "            left_on=pdv_key_transacoes, \n",
    "            right_on=pdv_key_pdvs, \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Resultado: {len(merged_data):,} transa√ß√µes ap√≥s merge com PDVs\")\n",
    "        print(f\"   ‚Ä¢ Transa√ß√µes sem PDV: {merged_data['premise'].isna().sum():,} ({merged_data['premise'].isna().sum()/len(merged_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Segundo merge: + Produtos\n",
    "    print(f\"\\nüîó MERGE COM DADOS DE PRODUTOS:\")\n",
    "    \n",
    "    if merged_data is not None and produtos_df is not None:\n",
    "        # Identificar a chave correta para Produto\n",
    "        produto_key_transacoes = 'internal_product_id'\n",
    "        produto_key_produtos = 'produto'\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Usando chaves: {produto_key_transacoes} ‚Üê ‚Üí {produto_key_produtos}\")\n",
    "        print(f\"   ‚Ä¢ Produtos √∫nicos em transa√ß√µes: {merged_data[produto_key_transacoes].nunique():,}\")\n",
    "        print(f\"   ‚Ä¢ Produtos √∫nicos no cadastro: {produtos_df[produto_key_produtos].nunique():,}\")\n",
    "        \n",
    "        # Verificar sobreposi√ß√£o\n",
    "        transacoes_produtos = set(merged_data[produto_key_transacoes].unique())\n",
    "        produtos_produtos = set(produtos_df[produto_key_produtos].unique())\n",
    "        overlap_produtos = transacoes_produtos.intersection(produtos_produtos)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Sobreposi√ß√£o: {len(overlap_produtos):,} produtos ({len(overlap_produtos)/len(produtos_produtos)*100:.1f}% dos produtos cadastrados)\")\n",
    "        \n",
    "        # Realizar merge\n",
    "        merged_data = merged_data.merge(\n",
    "            produtos_df, \n",
    "            left_on=produto_key_transacoes, \n",
    "            right_on=produto_key_produtos, \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Resultado: {len(merged_data):,} transa√ß√µes ap√≥s merge completo\")\n",
    "        print(f\"   ‚Ä¢ Transa√ß√µes sem produto: {merged_data['categoria'].isna().sum():,} ({merged_data['categoria'].isna().sum()/len(merged_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ MERGE CONCLU√çDO COM SUCESSO!\")\n",
    "    print(f\"   ‚Ä¢ Dataset final: {merged_data.shape} (linhas, colunas)\")\n",
    "    print(f\"   ‚Ä¢ Colunas dispon√≠veis: {len(merged_data.columns)} colunas\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Erro durante o merge: {e}\")\n",
    "    print(f\"Vamos tentar uma abordagem alternativa...\")\n",
    "    \n",
    "    # Fallback: usar apenas uma amostra para demonstrar\n",
    "    if transacoes_df is not None:\n",
    "        sample_transactions = transacoes_df.head(10000)  # Amostra menor\n",
    "        print(f\"Usando amostra de {len(sample_transactions):,} transa√ß√µes para demonstra√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. An√°lises do Dataset Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise das vendas por categoria de PDV\n",
    "print(\"üè™ AN√ÅLISE DAS VENDAS POR CATEGORIA DE PDV\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if merged_data is not None:\n",
    "    # Vendas por categoria de PDV\n",
    "    print(f\"\\nüìä VENDAS POR CATEGORIA DE PDV:\")\n",
    "    \n",
    "    vendas_por_categoria_pdv = merged_data.groupby('categoria_pdv').agg({\n",
    "        'quantity': 'sum',\n",
    "        'gross_value': 'sum',\n",
    "        'net_value': 'sum',\n",
    "        'transaction_date': 'count'  # n√∫mero de transa√ß√µes\n",
    "    }).round(2)\n",
    "    \n",
    "    vendas_por_categoria_pdv.columns = ['Quantidade_Total', 'Faturamento_Bruto', 'Faturamento_Liquido', 'Num_Transacoes']\n",
    "    \n",
    "    # Ordenar por faturamento e mostrar top 15\n",
    "    vendas_por_categoria_pdv = vendas_por_categoria_pdv.sort_values('Faturamento_Bruto', ascending=False)\n",
    "    top15_categorias_pdv = vendas_por_categoria_pdv.head(15)\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP 15 CATEGORIAS DE PDV POR FATURAMENTO:\")\n",
    "    for i, (categoria, row) in enumerate(top15_categorias_pdv.iterrows()):\n",
    "        if pd.notna(categoria):\n",
    "            pct_faturamento = row['Faturamento_Bruto'] / vendas_por_categoria_pdv['Faturamento_Bruto'].sum() * 100\n",
    "            print(f\"   {i+1:2d}. {categoria}:\")\n",
    "            print(f\"       ‚Ä¢ Faturamento: R$ {row['Faturamento_Bruto']:,.2f} ({pct_faturamento:.1f}% do total)\")\n",
    "            print(f\"       ‚Ä¢ Quantidade: {row['Quantidade_Total']:,.0f} unidades\")\n",
    "            print(f\"       ‚Ä¢ Transa√ß√µes: {row['Num_Transacoes']:,.0f}\")\n",
    "            print(f\"       ‚Ä¢ Ticket m√©dio: R$ {row['Faturamento_Bruto']/row['Num_Transacoes']:.2f}\")\n",
    "            print()\n",
    "    \n",
    "    # Vendas por tipo de premise (On/Off)\n",
    "    print(f\"\\nüè¢ VENDAS POR TIPO DE PREMISE (On/Off):\")\n",
    "    \n",
    "    vendas_por_premise = merged_data.groupby('premise').agg({\n",
    "        'quantity': 'sum',\n",
    "        'gross_value': 'sum',\n",
    "        'net_value': 'sum',\n",
    "        'transaction_date': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    vendas_por_premise.columns = ['Quantidade_Total', 'Faturamento_Bruto', 'Faturamento_Liquido', 'Num_Transacoes']\n",
    "    \n",
    "    for premise, row in vendas_por_premise.iterrows():\n",
    "        if pd.notna(premise):\n",
    "            pct_faturamento = row['Faturamento_Bruto'] / vendas_por_premise['Faturamento_Bruto'].sum() * 100\n",
    "            print(f\"   ‚Ä¢ {premise}:\")\n",
    "            print(f\"     - Faturamento: R$ {row['Faturamento_Bruto']:,.2f} ({pct_faturamento:.1f}%)\")\n",
    "            print(f\"     - Quantidade: {row['Quantidade_Total']:,.0f} unidades\")\n",
    "            print(f\"     - Transa√ß√µes: {row['Num_Transacoes']:,.0f}\")\n",
    "            print(f\"     - Ticket m√©dio: R$ {row['Faturamento_Bruto']/row['Num_Transacoes']:.2f}\")\n",
    "            print()\n",
    "    \n",
    "    # Visualiza√ß√µes\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Gr√°fico 1: Top 15 Categorias por Faturamento\n",
    "    top15_categorias_pdv['Faturamento_Bruto'].plot(kind='barh', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('üí∞ Top 15 Categorias de PDV por Faturamento', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Faturamento Bruto (R$)')\n",
    "    \n",
    "    # Gr√°fico 2: Top 15 Categorias por Quantidade\n",
    "    top15_categorias_pdv['Quantidade_Total'].plot(kind='barh', ax=axes[0,1], color='lightgreen')\n",
    "    axes[0,1].set_title('üì¶ Top 15 Categorias de PDV por Quantidade', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Quantidade Total')\n",
    "    \n",
    "    # Gr√°fico 3: Premise - Faturamento\n",
    "    vendas_por_premise['Faturamento_Bruto'].plot(kind='pie', ax=axes[1,0], autopct='%1.1f%%', startangle=90)\n",
    "    axes[1,0].set_title('üè¢ Faturamento por Tipo de Premise', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('')\n",
    "    \n",
    "    # Gr√°fico 4: Premise - Quantidade\n",
    "    vendas_por_premise['Quantidade_Total'].plot(kind='pie', ax=axes[1,1], autopct='%1.1f%%', startangle=90, colors=['lightcoral', 'gold'])\n",
    "    axes[1,1].set_title('üì¶ Quantidade por Tipo de Premise', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \nelse:\n",
    "    print(\"‚ùå Dataset merged n√£o dispon√≠vel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de produtos por tipo de PDV\n",
    "print(\"\\nüç∫ AN√ÅLISE DE PRODUTOS POR TIPO DE PDV\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if merged_data is not None:\n",
    "    # Produtos por tipo de premise\n",
    "    print(f\"\\nüìä CATEGORIAS DE PRODUTO MAIS POPULARES POR TIPO DE PREMISE:\")\n",
    "    \n",
    "    # Cross-tab entre categoria de produto e premise\n",
    "    produto_por_premise = pd.crosstab(merged_data['categoria'], merged_data['premise'], \n",
    "                                    values=merged_data['quantity'], aggfunc='sum', margins=True)\n",
    "    \n",
    "    print(f\"\\nüè¢ QUANTIDADE VENDIDA POR CATEGORIA x PREMISE:\")\n",
    "    display(produto_por_premise.fillna(0))\n",
    "    \n",
    "    # Calcular percentuais para cada premise\n",
    "    produto_por_premise_pct = pd.crosstab(merged_data['categoria'], merged_data['premise'], \n",
    "                                        values=merged_data['quantity'], aggfunc='sum', normalize='columns') * 100\n",
    "    \n",
    "    print(f\"\\nüìä PERCENTUAL DE VENDAS POR CATEGORIA EM CADA PREMISE:\")\n",
    "    display(produto_por_premise_pct.round(1).fillna(0))\n",
    "    \n",
    "    # Top categorias para On Premise\n",
    "    if 'On Premise' in produto_por_premise.columns:\n",
    "        top_on_premise = produto_por_premise['On Premise'].sort_values(ascending=False).head(10)\n",
    "        print(f\"\\nü•Ç TOP 10 CATEGORIAS EM ON PREMISE:\")\n",
    "        for i, (categoria, quantidade) in enumerate(top_on_premise.items()):\n",
    "            if categoria != 'All' and pd.notna(categoria):\n",
    "                pct = quantidade / produto_por_premise.loc['All', 'On Premise'] * 100\n",
    "                print(f\"   {i+1:2d}. {categoria}: {quantidade:,.0f} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Top categorias para Off Premise\n",
    "    if 'Off Premise' in produto_por_premise.columns:\n",
    "        top_off_premise = produto_por_premise['Off Premise'].sort_values(ascending=False).head(10)\n",
    "        print(f\"\\nüõí TOP 10 CATEGORIAS EM OFF PREMISE:\")\n",
    "        for i, (categoria, quantidade) in enumerate(top_off_premise.items()):\n",
    "            if categoria != 'All' and pd.notna(categoria):\n",
    "                pct = quantidade / produto_por_premise.loc['All', 'Off Premise'] * 100\n",
    "                print(f\"   {i+1:2d}. {categoria}: {quantidade:,.0f} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Visualiza√ß√µes\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Gr√°fico 1: Heatmap Categoria x Premise\n",
    "    if produto_por_premise_pct.shape[0] > 1 and produto_por_premise_pct.shape[1] > 1:\n",
    "        # Remover linha/coluna 'All' se existir\n",
    "        heatmap_data = produto_por_premise_pct.drop('All', errors='ignore')\n",
    "        sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='YlOrRd', ax=axes[0,0])\n",
    "        axes[0,0].set_title('üî• Heatmap: % Vendas por Categoria x Premise', fontweight='bold')\n",
    "        axes[0,0].set_ylabel('Categoria do Produto')\n",
    "        axes[0,0].set_xlabel('Tipo de Premise')\n",
    "    \n",
    "    # Gr√°fico 2: Top categorias On Premise\n",
    "    if 'On Premise' in produto_por_premise.columns:\n",
    "        top_on_plot = produto_por_premise['On Premise'].drop('All', errors='ignore').sort_values(ascending=True).tail(10)\n",
    "        top_on_plot.plot(kind='barh', ax=axes[0,1], color='lightblue')\n",
    "        axes[0,1].set_title('ü•Ç Top 10 Categorias - On Premise', fontweight='bold')\n",
    "        axes[0,1].set_xlabel('Quantidade Vendida')\n",
    "    \n",
    "    # Gr√°fico 3: Top categorias Off Premise\n",
    "    if 'Off Premise' in produto_por_premise.columns:\n",
    "        top_off_plot = produto_por_premise['Off Premise'].drop('All', errors='ignore').sort_values(ascending=True).tail(10)\n",
    "        top_off_plot.plot(kind='barh', ax=axes[1,0], color='lightgreen')\n",
    "        axes[1,0].set_title('üõí Top 10 Categorias - Off Premise', fontweight='bold')\n",
    "        axes[1,0].set_xlabel('Quantidade Vendida')\n",
    "    \n",
    "    # Gr√°fico 4: Compara√ß√£o direta On vs Off\n",
    "    if 'On Premise' in produto_por_premise.columns and 'Off Premise' in produto_por_premise.columns:\n",
    "        comparison_data = produto_por_premise[['On Premise', 'Off Premise']].drop('All', errors='ignore').head(7)\n",
    "        comparison_data.plot(kind='bar', ax=axes[1,1], width=0.8)\n",
    "        axes[1,1].set_title('‚öñÔ∏è Compara√ß√£o: On vs Off Premise por Categoria', fontweight='bold')\n",
    "        axes[1,1].set_ylabel('Quantidade Vendida')\n",
    "        axes[1,1].set_xlabel('Categoria do Produto')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \nelse:\n",
    "    print(\"‚ùå Dataset merged n√£o dispon√≠vel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. An√°lise Geogr√°fica (Zipcode x Volume de Vendas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de correla√ß√£o entre zipcode e volume de vendas\n",
    "print(\"\\nüó∫Ô∏è AN√ÅLISE DE CORRELA√á√ÉO ZIPCODE x VOLUME DE VENDAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if merged_data is not None:\n",
    "    # Agrega√ß√£o por zipcode\n",
    "    print(f\"\\nüìç AN√ÅLISE GEOGR√ÅFICA DAS VENDAS:\")\n",
    "    \n",
    "    vendas_por_zipcode = merged_data.groupby('zipcode').agg({\n",
    "        'quantity': 'sum',\n",
    "        'gross_value': 'sum',\n",
    "        'net_value': 'sum',\n",
    "        'transaction_date': 'count',\n",
    "        'pdv': 'nunique'  # n√∫mero √∫nico de PDVs por zipcode\n",
    "    }).round(2)\n",
    "    \n",
    "    vendas_por_zipcode.columns = ['Quantidade_Total', 'Faturamento_Bruto', 'Faturamento_Liquido', 'Num_Transacoes', 'Num_PDVs']\n",
    "    \n",
    "    # Calcular m√©dias por PDV em cada zipcode\n",
    "    vendas_por_zipcode['Quantidade_por_PDV'] = vendas_por_zipcode['Quantidade_Total'] / vendas_por_zipcode['Num_PDVs']\n",
    "    vendas_por_zipcode['Faturamento_por_PDV'] = vendas_por_zipcode['Faturamento_Bruto'] / vendas_por_zipcode['Num_PDVs']\n",
    "    \n",
    "    # Estat√≠sticas gerais\n",
    "    print(f\"   ‚Ä¢ Total de zipcodes com vendas: {len(vendas_por_zipcode):,}\")\n",
    "    print(f\"   ‚Ä¢ Zipcode com maior volume: {vendas_por_zipcode['Quantidade_Total'].idxmax()} ({vendas_por_zipcode['Quantidade_Total'].max():,.0f} unidades)\")\n",
    "    print(f\"   ‚Ä¢ Zipcode com maior faturamento: {vendas_por_zipcode['Faturamento_Bruto'].idxmax()} (R$ {vendas_por_zipcode['Faturamento_Bruto'].max():,.2f})\")\n",
    "    print(f\"   ‚Ä¢ Zipcode com mais PDVs: {vendas_por_zipcode['Num_PDVs'].idxmax()} ({vendas_por_zipcode['Num_PDVs'].max():.0f} PDVs)\")\n",
    "    \n",
    "    # Top 15 zipcodes por volume\n",
    "    print(f\"\\nüèÜ TOP 15 ZIPCODES POR VOLUME DE VENDAS:\")\n",
    "    top_zipcodes_volume = vendas_por_zipcode.sort_values('Quantidade_Total', ascending=False).head(15)\n",
    "    \n",
    "    for i, (zipcode, row) in enumerate(top_zipcodes_volume.iterrows()):\n",
    "        pct_volume = row['Quantidade_Total'] / vendas_por_zipcode['Quantidade_Total'].sum() * 100\n",
    "        print(f\"   {i+1:2d}. {zipcode}:\")\n",
    "        print(f\"       ‚Ä¢ Volume: {row['Quantidade_Total']:,.0f} unidades ({pct_volume:.1f}% do total)\")\n",
    "        print(f\"       ‚Ä¢ Faturamento: R$ {row['Faturamento_Bruto']:,.2f}\")\n",
    "        print(f\"       ‚Ä¢ PDVs: {row['Num_PDVs']:.0f}\")\n",
    "        print(f\"       ‚Ä¢ Volume/PDV: {row['Quantidade_por_PDV']:,.0f} unidades\")\n",
    "        print()\n",
    "    \n",
    "    # An√°lise de concentra√ß√£o geogr√°fica\n",
    "    print(f\"\\nüìä CONCENTRA√á√ÉO GEOGR√ÅFICA:\")\n",
    "    \n",
    "    # Calcular percentis\n",
    "    volume_cumsum = vendas_por_zipcode.sort_values('Quantidade_Total', ascending=False)['Quantidade_Total'].cumsum()\n",
    "    volume_cumsum_pct = volume_cumsum / vendas_por_zipcode['Quantidade_Total'].sum() * 100\n",
    "    \n",
    "    zipcodes_80pct = (volume_cumsum_pct <= 80).sum()\n",
    "    zipcodes_50pct = (volume_cumsum_pct <= 50).sum()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {zipcodes_50pct} zipcodes representam 50% do volume ({zipcodes_50pct/len(vendas_por_zipcode)*100:.1f}% dos zipcodes)\")\n",
    "    print(f\"   ‚Ä¢ {zipcodes_80pct} zipcodes representam 80% do volume ({zipcodes_80pct/len(vendas_por_zipcode)*100:.1f}% dos zipcodes)\")\n",
    "    \n",
    "    # Correla√ß√µes\n",
    "    print(f\"\\nüîó CORRELA√á√ïES:\")\n",
    "    correlations = vendas_por_zipcode[['Quantidade_Total', 'Faturamento_Bruto', 'Num_Transacoes', 'Num_PDVs']].corr()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Correla√ß√£o Volume x Faturamento: {correlations.loc['Quantidade_Total', 'Faturamento_Bruto']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Correla√ß√£o Volume x Num_PDVs: {correlations.loc['Quantidade_Total', 'Num_PDVs']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Correla√ß√£o Faturamento x Num_PDVs: {correlations.loc['Faturamento_Bruto', 'Num_PDVs']:.3f}\")\n",
    "    \n",
    "    # Visualiza√ß√µes\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Gr√°fico 1: Top 15 zipcodes por volume\n",
    "    top_zipcodes_volume.head(15)['Quantidade_Total'].plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('üìç Top 15 Zipcodes por Volume', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('Quantidade Total')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Gr√°fico 2: Distribui√ß√£o de volume por zipcode (histograma)\n",
    "    axes[0,1].hist(vendas_por_zipcode['Quantidade_Total'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0,1].set_yscale('log')\n",
    "    axes[0,1].set_title('üìä Distribui√ß√£o de Volume por Zipcode', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Quantidade Total por Zipcode')\n",
    "    axes[0,1].set_ylabel('Frequ√™ncia (Log)')\n",
    "    \n",
    "    # Gr√°fico 3: Scatter Volume x N√∫mero de PDVs\n",
    "    axes[0,2].scatter(vendas_por_zipcode['Num_PDVs'], vendas_por_zipcode['Quantidade_Total'], alpha=0.6, color='purple')\n",
    "    axes[0,2].set_title('üîó Volume vs N√∫mero de PDVs por Zipcode', fontweight='bold')\n",
    "    axes[0,2].set_xlabel('N√∫mero de PDVs')\n",
    "    axes[0,2].set_ylabel('Quantidade Total')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 4: Scatter Faturamento x Volume\n",
    "    axes[1,0].scatter(vendas_por_zipcode['Quantidade_Total'], vendas_por_zipcode['Faturamento_Bruto'], alpha=0.6, color='orange')\n",
    "    axes[1,0].set_title('üíπ Faturamento vs Volume por Zipcode', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Quantidade Total')\n",
    "    axes[1,0].set_ylabel('Faturamento Bruto (R$)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 5: Heatmap de correla√ß√µes\n",
    "    sns.heatmap(correlations, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])\n",
    "    axes[1,1].set_title('üî• Correla√ß√µes entre M√©tricas', fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico 6: Volume por PDV por zipcode\n",
    "    vendas_por_zipcode['Quantidade_por_PDV'].hist(bins=30, ax=axes[1,2], alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1,2].set_title('üìä Distribui√ß√£o Volume/PDV por Zipcode', fontweight='bold')\n",
    "    axes[1,2].set_xlabel('Quantidade por PDV')\n",
    "    axes[1,2].set_ylabel('Frequ√™ncia')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \nelse:\n",
    "    print(\"‚ùå Dataset merged n√£o dispon√≠vel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumo Final e Conclus√µes da EDA Estendida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final da EDA estendida\n",
    "print(\"üéØ RESUMO FINAL DA AN√ÅLISE EXPLORAT√ìRIA ESTENDIDA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä INSIGHTS DOS CADASTROS:\")\n",
    "if pdvs_df is not None:\n",
    "    print(f\"   üè™ PDVs:\")\n",
    "    print(f\"   ‚Ä¢ Total: {len(pdvs_df):,} pontos de venda\")\n",
    "    if 'premise' in pdvs_df.columns:\n",
    "        premise_counts = pdvs_df['premise'].value_counts()\n",
    "        for premise, count in premise_counts.items():\n",
    "            pct = count / len(pdvs_df) * 100\n",
    "            print(f\"   ‚Ä¢ {premise}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Categorias de PDV: {pdvs_df['categoria_pdv'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Zipcodes √∫nicos: {pdvs_df['zipcode'].nunique():,}\")\n",
    "\n",
    "if produtos_df is not None:\n",
    "    print(f\"\\n   üç∫ Produtos:\")\n",
    "    print(f\"   ‚Ä¢ Total: {len(produtos_df):,} produtos\")\n",
    "    print(f\"   ‚Ä¢ Categorias: {produtos_df['categoria'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Marcas: {produtos_df['marca'].nunique():,}\")\n",
    "    print(f\"   ‚Ä¢ Fabricantes: {produtos_df['fabricante'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nüîó RESULTADOS DO MERGE:\")\n",
    "if merged_data is not None:\n",
    "    print(f\"   ‚Ä¢ Dataset final: {merged_data.shape[0]:,} transa√ß√µes x {merged_data.shape[1]} colunas\")\n",
    "    print(f\"   ‚Ä¢ Cobertura PDV: {(~merged_data['premise'].isna()).sum() / len(merged_data) * 100:.1f}% das transa√ß√µes\")\n",
    "    print(f\"   ‚Ä¢ Cobertura Produto: {(~merged_data['categoria'].isna()).sum() / len(merged_data) * 100:.1f}% das transa√ß√µes\")\n",
    "\n",
    "print(f\"\\nüí° PRINCIPAIS DESCOBERTAS:\")\n",
    "\n",
    "if merged_data is not None:\n",
    "    # Top categoria de PDV\n",
    "    top_categoria_pdv = merged_data.groupby('categoria_pdv')['gross_value'].sum().idxmax()\n",
    "    top_categoria_pdv_valor = merged_data.groupby('categoria_pdv')['gross_value'].sum().max()\n",
    "    \n",
    "    # Top categoria de produto  \n",
    "    top_categoria_produto = merged_data.groupby('categoria')['quantity'].sum().idxmax()\n",
    "    top_categoria_produto_qtd = merged_data.groupby('categoria')['quantity'].sum().max()\n",
    "    \n",
    "    # Top zipcode\n",
    "    top_zipcode = merged_data.groupby('zipcode')['quantity'].sum().idxmax()\n",
    "    top_zipcode_qtd = merged_data.groupby('zipcode')['quantity'].sum().max()\n",
    "    \n",
    "    print(f\"   üèÜ Categoria PDV que mais fatura: {top_categoria_pdv} (R$ {top_categoria_pdv_valor:,.2f})\")\n",
    "    print(f\"   üç∫ Categoria produto mais vendida: {top_categoria_produto} ({top_categoria_produto_qtd:,.0f} unidades)\")\n",
    "    print(f\"   üìç Zipcode com maior volume: {top_zipcode} ({top_zipcode_qtd:,.0f} unidades)\")\n",
    "    \n",
    "    # On vs Off Premise\n",
    "    premise_stats = merged_data.groupby('premise').agg({\n",
    "        'quantity': 'sum',\n",
    "        'gross_value': 'sum'\n",
    "    })\n",
    "    \n",
    "    if 'On Premise' in premise_stats.index and 'Off Premise' in premise_stats.index:\n",
    "        on_premise_pct = premise_stats.loc['On Premise', 'gross_value'] / premise_stats['gross_value'].sum() * 100\n",
    "        off_premise_pct = premise_stats.loc['Off Premise', 'gross_value'] / premise_stats['gross_value'].sum() * 100\n",
    "        \n",
    "        print(f\"   üè¢ Faturamento On Premise: {on_premise_pct:.1f}%\")\n",
    "        print(f\"   üõí Faturamento Off Premise: {off_premise_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìà IMPLICA√á√ïES PARA MODELAGEM:\")\n",
    "print(f\"   ‚Ä¢ Incorporar features de PDV (categoria, premise, zipcode)\")\n",
    "print(f\"   ‚Ä¢ Considerar features de produto (categoria, marca, fabricante)\")\n",
    "print(f\"   ‚Ä¢ Usar encoding geogr√°fico (zipcode) como feature importante\")\n",
    "print(f\"   ‚Ä¢ Modelar separadamente On vs Off Premise se houver diferen√ßas significativas\")\n",
    "print(f\"   ‚Ä¢ Considerar sazonalidade espec√≠fica por categoria de PDV\")\n",
    "print(f\"   ‚Ä¢ Aplicar feature engineering em marcas/fabricantes (long tail)\")\n",
    "\n",
    "print(f\"\\nüéØ PR√ìXIMOS PASSOS RECOMENDADOS:\")\n",
    "print(f\"   1. Engenharia de Features baseada nos insights da EDA\")\n",
    "print(f\"   2. Modelagem hier√°rquica (por categoria de PDV/produto)\")\n",
    "print(f\"   3. Valida√ß√£o cruzada considerando distribui√ß√£o geogr√°fica\")\n",
    "print(f\"   4. Ensembles combinando diferentes abordagens\")\n",
    "print(f\"   5. Monitoramento de performance por segmento\")\n",
    "\n",
    "# Salvar o dataset merged para pr√≥ximas etapas\n",
    "if merged_data is not None:\n",
    "    # Salvar uma vers√£o resumida (√∫ltimos 3 meses) para otimizar performance\n",
    "    merged_data['transaction_date'] = pd.to_datetime(merged_data['transaction_date'])\n",
    "    recent_data = merged_data[merged_data['transaction_date'] >= '2022-10-01'].copy()\n",
    "    \n",
    "    recent_data.to_parquet('../data/merged_data_recent.parquet', index=False)\n",
    "    print(f\"\\nüíæ Dataset merged recente salvo: data/merged_data_recent.parquet\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odo: Out-Dez 2022 ({len(recent_data):,} transa√ß√µes)\")\n",
    "    \n",
    "    # Tamb√©m salvar agrega√ß√£o semanal merged\n",
    "    weekly_merged = recent_data.groupby([\n",
    "        pd.Grouper(key='transaction_date', freq='W'),\n",
    "        'categoria_pdv', 'premise', 'categoria', 'zipcode'\n",
    "    ]).agg({\n",
    "        'quantity': 'sum',\n",
    "        'gross_value': 'sum',\n",
    "        'net_value': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    weekly_merged.to_parquet('../data/weekly_merged_data.parquet', index=False)\n",
    "    print(f\"   ‚Ä¢ Agrega√ß√£o semanal merged salva: data/weekly_merged_data.parquet\")\n",
    "    print(f\"   ‚Ä¢ {len(weekly_merged):,} registros semanais por segmento\")\n",
    "\n",
    "# Salvar dados processados b√°sicos tamb√©m\n",
    "if 'daily_data' in locals():\n",
    "    daily_data.to_csv('../data/processed_daily_data.csv', index=False)\n",
    "    print(f\"\\nüíæ Dados di√°rios salvos em: data/processed_daily_data.csv\")\n",
    "\n",
    "if 'weekly_data' in locals():\n",
    "    weekly_data.to_csv('../data/processed_weekly_data.csv', index=False)\n",
    "    print(f\"üíæ Dados semanais salvos em: data/processed_weekly_data.csv\")\n",
    "\n",
    "print(f\"\\nüéâ AN√ÅLISE EXPLORAT√ìRIA COMPLETA FINALIZADA!\")\n",
    "print(f\"   ‚Ä¢ Dados b√°sicos ‚úì\")\n",
    "print(f\"   ‚Ä¢ S√©rie temporal ‚úì\") \n",
    "print(f\"   ‚Ä¢ Cadastros (PDV + Produtos) ‚úì\")\n",
    "print(f\"   ‚Ä¢ Merge e cruzamentos ‚úì\")\n",
    "print(f\"   ‚Ä¢ An√°lises geogr√°ficas ‚úì\")\n",
    "print(f\"   ‚Ä¢ Long tail e distribui√ß√µes ‚úì\")\n",
    "print(f\"   ‚Ä¢ Correla√ß√µes e insights ‚úì\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}