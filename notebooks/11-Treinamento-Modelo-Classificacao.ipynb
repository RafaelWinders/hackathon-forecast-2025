{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Modelo de Classifica√ß√£o (Prever SE vai vender)\n",
    "\n",
    "Este notebook treina o primeiro est√°gio do modelo de dois est√°gios: **classifica√ß√£o**.\n",
    "\n",
    "## Objetivo:\n",
    "- Prever se um item ter√° vendas (`vendeu = 1`) ou n√£o (`vendeu = 0`)\n",
    "- Usar LightGBM Classifier otimizado\n",
    "- Avaliar com m√©tricas de classifica√ß√£o (AUC, F1-Score, Matriz de Confus√£o)\n",
    "- Preparar para ser usado no pipeline de dois est√°gios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Iniciando Treinamento do Modelo de Classifica√ß√£o\n",
      "üèÜ Objetivo: Prever SE um item vai vender (vendeu = 1 ou 0)\n",
      "üìÅ Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('üéØ Iniciando Treinamento do Modelo de Classifica√ß√£o')\n",
    "print('üèÜ Objetivo: Prever SE um item vai vender (vendeu = 1 ou 0)')\n",
    "\n",
    "# Criar pasta submissao3 se n√£o existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('üìÅ Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados com Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados com features CORRIGIDAS (sem target leakage)...\n",
      "‚úÖ Dados CORRIGIDOS carregados com sucesso!\n",
      "üèãÔ∏è Dados de treino: (50126880, 54)\n",
      "üîç Dados de valida√ß√£o: (5221550, 54)\n",
      "\n",
      "üìä Distribui√ß√£o do target \"vendeu\" no treino:\n",
      "vendeu\n",
      "0    44583272\n",
      "1     5543608\n",
      "Name: count, dtype: int64\n",
      "   ‚Ä¢ Propor√ß√£o positiva: 11.06%\n",
      "\n",
      "üìä Distribui√ß√£o do target \"vendeu\" na valida√ß√£o:\n",
      "vendeu\n",
      "0    4649821\n",
      "1     571729\n",
      "Name: count, dtype: int64\n",
      "   ‚Ä¢ Propor√ß√£o positiva: 10.95%\n",
      "‚úÖ Dados carregados e target verificado\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com features avan√ßadas CORRIGIDAS\n",
    "print('üìÇ Carregando dados com features CORRIGIDAS (sem target leakage)...')\n",
    "\n",
    "# IMPORTANTE: Usar os dados CORRIGIDOS que foram salvos no Notebook 10\n",
    "try:\n",
    "    train_features = pd.read_parquet('../data/submissao3/train_features_CORRIGIDO.parquet')\n",
    "    validation_features = pd.read_parquet('../data/submissao3/validation_features_CORRIGIDO.parquet')\n",
    "    print('‚úÖ Dados CORRIGIDOS carregados com sucesso!')\n",
    "except FileNotFoundError:\n",
    "    print('‚ö†Ô∏è Dados CORRIGIDOS n√£o encontrados. Usando dados originais...')\n",
    "    print('   Recomenda√ß√£o: Execute o Notebook 10 primeiro para gerar os dados corrigidos.')\n",
    "    train_features = pd.read_parquet('../data/submissao3/train_features.parquet')\n",
    "    validation_features = pd.read_parquet('../data/submissao3/validation_features.parquet')\n",
    "\n",
    "print(f'üèãÔ∏è Dados de treino: {train_features.shape}')\n",
    "print(f'üîç Dados de valida√ß√£o: {validation_features.shape}')\n",
    "\n",
    "# Verificar target de classifica√ß√£o\n",
    "print(f'\\nüìä Distribui√ß√£o do target \"vendeu\" no treino:')\n",
    "print(train_features['vendeu'].value_counts())\n",
    "print(f'   ‚Ä¢ Propor√ß√£o positiva: {train_features[\"vendeu\"].mean()*100:.2f}%')\n",
    "\n",
    "print(f'\\nüìä Distribui√ß√£o do target \"vendeu\" na valida√ß√£o:')\n",
    "print(validation_features['vendeu'].value_counts())\n",
    "print(f'   ‚Ä¢ Propor√ß√£o positiva: {validation_features[\"vendeu\"].mean()*100:.2f}%')\n",
    "\n",
    "print('‚úÖ Dados carregados e target verificado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepara√ß√£o dos Dados para Classifica√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preparando dados para classifica√ß√£o SEM TARGET LEAKAGE...\n",
      "üìä Features dispon√≠veis: 54\n",
      "üìä Features SEGURAS para classifica√ß√£o: 41\n",
      "üìä Features exclu√≠das (com leakage): 20\n",
      "\n",
      "‚úÖ FEATURES SEGURAS (sem target leakage):\n",
      "   ‚Ä¢ preco_lag_1\n",
      "   ‚Ä¢ preco_lag_2\n",
      "   ‚Ä¢ variacao_preco_sku_semanal\n",
      "   ‚Ä¢ quantidade_lag_1\n",
      "   ‚Ä¢ quantidade_lag_2\n",
      "   ‚Ä¢ quantidade_lag_3\n",
      "   ‚Ä¢ quantidade_lag_4\n",
      "   ‚Ä¢ quantidade_media_4w\n",
      "   ‚Ä¢ quantidade_std_4w\n",
      "   ‚Ä¢ quantidade_max_4w\n",
      "   ‚Ä¢ quantidade_ewma_4w\n",
      "   ‚Ä¢ quantidade_ewma_8w\n",
      "   ‚Ä¢ preco_ewma_4w\n",
      "   ‚Ä¢ semana_do_ano\n",
      "   ‚Ä¢ eh_primeira_semana_mes\n",
      "   ‚Ä¢ eh_dezembro\n",
      "   ‚Ä¢ eh_janeiro\n",
      "   ‚Ä¢ eh_pos_festas\n",
      "   ‚Ä¢ semana_ano_sin\n",
      "   ‚Ä¢ semana_ano_cos\n",
      "   ‚Ä¢ preco_relativo_categoria\n",
      "   ‚Ä¢ preco_relativo_pdv\n",
      "   ‚Ä¢ preco_volatilidade\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_lag_1\n",
      "   ‚Ä¢ share_vendas_sku_categoria_lag_1\n",
      "   ‚Ä¢ momentum_ratio\n",
      "   ‚Ä¢ momentum_ratio_ewma\n",
      "   ‚Ä¢ aceleracao\n",
      "   ‚Ä¢ dia_do_mes\n",
      "   ‚Ä¢ semana_do_mes\n",
      "   ‚Ä¢ eh_inicio_mes\n",
      "   ‚Ä¢ eh_fim_mes\n",
      "   ‚Ä¢ mes\n",
      "   ‚Ä¢ mes_sin\n",
      "   ‚Ä¢ mes_cos\n",
      "   ‚Ä¢ pdv_hash\n",
      "   ‚Ä¢ produto_hash\n",
      "   ‚Ä¢ categoria_hash\n",
      "   ‚Ä¢ zipcode_hash\n",
      "   ‚Ä¢ pdv_produto_hash\n",
      "   ‚Ä¢ categoria_zipcode_hash\n",
      "\n",
      "üö® FEATURES REMOVIDAS (causavam leakage):\n",
      "   ‚ùå preco_unitario_atual\n",
      "   ‚ùå preco_medio_semanal_sku_atual\n",
      "   ‚ùå media_vendas_categoria_pdv_atual\n",
      "   ‚ùå share_vendas_sku_categoria_atual\n",
      "\n",
      "üìä Datasets preparados (SEM LEAKAGE):\n",
      "   üèãÔ∏è X_train: (50126880, 41), y_train: (50126880,)\n",
      "   üîç X_val: (5221550, 41), y_val: (5221550,)\n",
      "   üßπ NAs no treino: 0, NAs na valida√ß√£o: 0\n",
      "‚úÖ 41 features seguras - suficientes para treinamento!\n",
      "‚úÖ Dados preparados para classifica√ß√£o SEM TARGET LEAKAGE\n"
     ]
    }
   ],
   "source": [
    "# Definir features para o modelo de classifica√ß√£o SEM TARGET LEAKAGE\n",
    "print('üîß Preparando dados para classifica√ß√£o SEM TARGET LEAKAGE...')\n",
    "\n",
    "# CR√çTICO: Excluir features que causam vazamento de informa√ß√£o\n",
    "features_excluir = [\n",
    "    # IDs e data\n",
    "    'semana', 'pdv_id', 'produto_id',  \n",
    "    # Targets\n",
    "    'quantidade', 'faturamento', 'vendeu',       \n",
    "    # IDs auxiliares\n",
    "    'distributor_id',                  \n",
    "    # Categ√≥ricas brutas (usamos as hash)\n",
    "    'categoria', 'zipcode', 'tipo_loja',  \n",
    "    \n",
    "    # üö® CR√çTICO: Features que causam TARGET LEAKAGE (usam informa√ß√£o da semana atual)\n",
    "    'preco_unitario_atual',                     # Usa quantidade atual  \n",
    "    'preco_medio_semanal_sku_atual',           # Usa pre√ßo atual\n",
    "    'media_vendas_categoria_pdv_atual',        # Usa quantidade atual\n",
    "    'share_vendas_sku_categoria_atual',        # Usa quantidade atual\n",
    "    \n",
    "    # REMOVIDAS do modelo original que causavam leakage\n",
    "    'preco_unitario',                          # Calculado com quantidade atual\n",
    "    'preco_medio_semanal_sku',                # Usa pre√ßo atual  \n",
    "    'preco_medio_semanal_categoria_pdv',      # Usa pre√ßo atual\n",
    "    'media_vendas_categoria_no_pdv',          # Usa quantidade atual\n",
    "    'media_vendas_sku_no_zipcode',            # Usa quantidade atual  \n",
    "    'share_vendas_sku_na_categoria'           # Usa quantidade atual\n",
    "]\n",
    "\n",
    "# Selecionar features SEGURAS (sem leakage)\n",
    "features_disponiveis = list(train_features.columns)\n",
    "features_modelo_seguros = [col for col in features_disponiveis if col not in features_excluir]\n",
    "\n",
    "print(f'üìä Features dispon√≠veis: {len(features_disponiveis)}')\n",
    "print(f'üìä Features SEGURAS para classifica√ß√£o: {len(features_modelo_seguros)}')\n",
    "print(f'üìä Features exclu√≠das (com leakage): {len(features_excluir)}')\n",
    "\n",
    "# Mostrar features SEGURAS que ser√£o usadas\n",
    "print('\\n‚úÖ FEATURES SEGURAS (sem target leakage):')\n",
    "for feat in features_modelo_seguros:\n",
    "    print(f'   ‚Ä¢ {feat}')\n",
    "\n",
    "# Mostrar features REMOVIDAS (com leakage)  \n",
    "print('\\nüö® FEATURES REMOVIDAS (causavam leakage):')\n",
    "features_com_leakage = [f for f in features_excluir if 'preco_unitario' in f or 'media_vendas' in f or 'share_vendas' in f or 'preco_medio' in f]\n",
    "for feat in features_com_leakage:\n",
    "    if feat in features_disponiveis:\n",
    "        print(f'   ‚ùå {feat}')\n",
    "\n",
    "# Preparar datasets COM FEATURES SEGURAS\n",
    "X_train = train_features[features_modelo_seguros].copy()\n",
    "y_train = train_features['vendeu'].copy()\n",
    "\n",
    "X_val = validation_features[features_modelo_seguros].copy() \n",
    "y_val = validation_features['vendeu'].copy()\n",
    "\n",
    "print(f'\\nüìä Datasets preparados (SEM LEAKAGE):')\n",
    "print(f'   üèãÔ∏è X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'   üîç X_val: {X_val.shape}, y_val: {y_val.shape}')\n",
    "\n",
    "# Verificar se h√° NAs\n",
    "nas_train = X_train.isnull().sum().sum()\n",
    "nas_val = X_val.isnull().sum().sum()\n",
    "print(f'   üßπ NAs no treino: {nas_train}, NAs na valida√ß√£o: {nas_val}')\n",
    "\n",
    "if nas_train > 0 or nas_val > 0:\n",
    "    print('   ‚ö†Ô∏è Preenchendo NAs com 0...')\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_val = X_val.fillna(0)\n",
    "\n",
    "# Verificar se ainda temos features suficientes\n",
    "if len(features_modelo_seguros) < 10:\n",
    "    print('‚ö†Ô∏è AVISO: Poucas features dispon√≠veis! Pode indicar problema na engenharia de features.')\n",
    "else:\n",
    "    print(f'‚úÖ {len(features_modelo_seguros)} features seguras - suficientes para treinamento!')\n",
    "\n",
    "print('‚úÖ Dados preparados para classifica√ß√£o SEM TARGET LEAKAGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinamento do Modelo LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Treinando LightGBM Classifier...\n",
      "   üìö Iniciando treinamento...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.84728\tvalid_0's binary_logloss: 0.267367\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.84728\tvalid_0's binary_logloss: 0.267367\n",
      "‚úÖ Modelo treinado! Melhor itera√ß√£o: 100\n",
      "üìä Score de valida√ß√£o: defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('auc', np.float64(0.8472803725977874)), ('binary_logloss', np.float64(0.2673666610763523))])})\n"
     ]
    }
   ],
   "source": [
    "# Configurar e treinar LightGBM Classifier\n",
    "print('üöÄ Treinando LightGBM Classifier...')\n",
    "\n",
    "# Par√¢metros otimizados para classifica√ß√£o\n",
    "lgbm_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 100,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Criar modelo\n",
    "lgbm_classifier = lgb.LGBMClassifier(**lgbm_params)\n",
    "\n",
    "# Treinar modelo com callbacks para early stopping\n",
    "print('   üìö Iniciando treinamento...')\n",
    "lgbm_classifier.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['binary_logloss', 'auc'],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Modelo treinado! Melhor itera√ß√£o: {lgbm_classifier.best_iteration_}')\n",
    "print(f'üìä Score de valida√ß√£o: {lgbm_classifier.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avalia√ß√£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Avaliando modelo de classifica√ß√£o...\n",
      "\n",
      "üìä M√âTRICAS DE TREINO:\n",
      "   üéØ AUC: 0.8784\n",
      "   üéØ F1-Score: 0.3917\n",
      "   üéØ Precision: 0.7043\n",
      "   üéØ Recall: 0.2713\n",
      "\n",
      "üìä M√âTRICAS DE VALIDA√á√ÉO:\n",
      "   üéØ AUC: 0.8473\n",
      "   üéØ F1-Score: 0.1460\n",
      "   üéØ Precision: 0.7078\n",
      "   üéØ Recall: 0.0814\n",
      "\n",
      "üìä MATRIZ DE CONFUS√ÉO (Valida√ß√£o):\n",
      "                 Predito\n",
      "               0       1\n",
      "Real    0   4,630,608  19,213\n",
      "        1   525,188  46,541\n",
      "\n",
      "üìä AN√ÅLISE DE THRESHOLDS:\n",
      "   Threshold 0.3: F1=0.3520, Prec=0.5647, Rec=0.2557\n",
      "   Threshold 0.4: F1=0.2366, Prec=0.6448, Rec=0.1449\n",
      "   Threshold 0.5: F1=0.1460, Prec=0.7078, Rec=0.0814\n",
      "   Threshold 0.6: F1=0.0708, Prec=0.7642, Rec=0.0371\n",
      "   Threshold 0.7: F1=0.0089, Prec=0.8442, Rec=0.0045\n",
      "‚úÖ Avalia√ß√£o do modelo conclu√≠da\n"
     ]
    }
   ],
   "source": [
    "# Fazer previs√µes\n",
    "print('üîç Avaliando modelo de classifica√ß√£o...')\n",
    "\n",
    "# Previs√µes de probabilidade\n",
    "y_pred_proba_train = lgbm_classifier.predict_proba(X_train)[:, 1]\n",
    "y_pred_proba_val = lgbm_classifier.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Previs√µes bin√°rias (threshold = 0.5)\n",
    "y_pred_train = lgbm_classifier.predict(X_train)\n",
    "y_pred_val = lgbm_classifier.predict(X_val)\n",
    "\n",
    "# M√©tricas de treino\n",
    "print('\\nüìä M√âTRICAS DE TREINO:')\n",
    "print(f'   üéØ AUC: {roc_auc_score(y_train, y_pred_proba_train):.4f}')\n",
    "print(f'   üéØ F1-Score: {f1_score(y_train, y_pred_train):.4f}')\n",
    "print(f'   üéØ Precision: {precision_score(y_train, y_pred_train):.4f}')\n",
    "print(f'   üéØ Recall: {recall_score(y_train, y_pred_train):.4f}')\n",
    "\n",
    "# M√©tricas de valida√ß√£o\n",
    "print('\\nüìä M√âTRICAS DE VALIDA√á√ÉO:')\n",
    "auc_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "precision_val = precision_score(y_val, y_pred_val)\n",
    "recall_val = recall_score(y_val, y_pred_val)\n",
    "\n",
    "print(f'   üéØ AUC: {auc_val:.4f}')\n",
    "print(f'   üéØ F1-Score: {f1_val:.4f}')\n",
    "print(f'   üéØ Precision: {precision_val:.4f}')\n",
    "print(f'   üéØ Recall: {recall_val:.4f}')\n",
    "\n",
    "# Matriz de confus√£o\n",
    "print('\\nüìä MATRIZ DE CONFUS√ÉO (Valida√ß√£o):')\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "print(f'                 Predito')\n",
    "print(f'               0       1')\n",
    "print(f'Real    0   {cm[0,0]:7,} {cm[0,1]:7,}')\n",
    "print(f'        1   {cm[1,0]:7,} {cm[1,1]:7,}')\n",
    "\n",
    "# An√°lise de diferentes thresholds\n",
    "print('\\nüìä AN√ÅLISE DE THRESHOLDS:')\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba_val >= threshold).astype(int)\n",
    "    f1_thresh = f1_score(y_val, y_pred_thresh)\n",
    "    precision_thresh = precision_score(y_val, y_pred_thresh)\n",
    "    recall_thresh = recall_score(y_val, y_pred_thresh)\n",
    "    print(f'   Threshold {threshold:.1f}: F1={f1_thresh:.4f}, Prec={precision_thresh:.4f}, Rec={recall_thresh:.4f}')\n",
    "\n",
    "print('‚úÖ Avalia√ß√£o do modelo conclu√≠da')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise de Import√¢ncia das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analisando import√¢ncia das features...\n",
      "\n",
      "üèÜ TOP 20 FEATURES MAIS IMPORTANTES:\n",
      "    1. quantidade_ewma_8w             -   1271\n",
      "    2. preco_ewma_4w                  -    972\n",
      "    3. quantidade_ewma_4w             -    922\n",
      "    4. semana_do_ano                  -    890\n",
      "    5. preco_lag_1                    -    867\n",
      "    6. categoria_hash                 -    622\n",
      "    7. preco_relativo_categoria       -    622\n",
      "    8. produto_hash                   -    357\n",
      "    9. media_vendas_categoria_pdv_lag_1 -    354\n",
      "   10. preco_lag_2                    -    326\n",
      "   11. preco_relativo_pdv             -    307\n",
      "   12. momentum_ratio_ewma            -    294\n",
      "   13. quantidade_media_4w            -    281\n",
      "   14. categoria_zipcode_hash         -    240\n",
      "   15. preco_volatilidade             -    212\n",
      "   16. quantidade_std_4w              -    185\n",
      "   17. semana_ano_sin                 -    183\n",
      "   18. mes_sin                        -    117\n",
      "   19. quantidade_max_4w              -    114\n",
      "   20. aceleracao                     -     94\n",
      "\n",
      "üìä IMPORT√ÇNCIA POR CATEGORIA:\n",
      "   Lag         :     1887 (8 features)\n",
      "   Rolling     :      580 (3 features)\n",
      "   Pre√ßo       :     3330 (7 features)\n",
      "   Calend√°rio  :      917 (10 features)\n",
      "   Tend√™ncia   :      425 (3 features)\n",
      "   Hierarquia  :      435 (2 features)\n",
      "   Hash        :     1221 (6 features)\n",
      "‚úÖ An√°lise de import√¢ncia conclu√≠da\n"
     ]
    }
   ],
   "source": [
    "# Analisar import√¢ncia das features\n",
    "print('üìä Analisando import√¢ncia das features...')\n",
    "\n",
    "# Obter import√¢ncias\n",
    "feature_importance = lgbm_classifier.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Criar DataFrame de import√¢ncias\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 20 features mais importantes\n",
    "print('\\nüèÜ TOP 20 FEATURES MAIS IMPORTANTES:')\n",
    "for i, (_, row) in enumerate(importance_df.head(20).iterrows()):\n",
    "    print(f'   {i+1:2d}. {row[\"feature\"]:30s} - {row[\"importance\"]:6.0f}')\n",
    "\n",
    "# An√°lise por categoria de feature\n",
    "print('\\nüìä IMPORT√ÇNCIA POR CATEGORIA:')\n",
    "categorias_feature = {\n",
    "    'Lag': [f for f in feature_names if 'lag_' in f],\n",
    "    'Rolling': [f for f in feature_names if any(x in f for x in ['media_4w', 'std_4w', 'max_4w'])],\n",
    "    'Pre√ßo': [f for f in feature_names if 'preco' in f],\n",
    "    'Calend√°rio': [f for f in feature_names if any(x in f for x in ['mes', 'dia', 'inicio', 'fim'])],\n",
    "    'Tend√™ncia': [f for f in feature_names if any(x in f for x in ['momentum', 'aceleracao'])],\n",
    "    'Hierarquia': [f for f in feature_names if any(x in f for x in ['media_vendas', 'share'])],\n",
    "    'Hash': [f for f in feature_names if 'hash' in f]\n",
    "}\n",
    "\n",
    "for categoria, features in categorias_feature.items():\n",
    "    if features:\n",
    "        importancia_categoria = importance_df[importance_df['feature'].isin(features)]['importance'].sum()\n",
    "        print(f'   {categoria:12s}: {importancia_categoria:8.0f} ({len(features)} features)')\n",
    "\n",
    "print('‚úÖ An√°lise de import√¢ncia conclu√≠da')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Salvando modelo de classifica√ß√£o SEM LEAKAGE...\n",
      "‚úÖ Arquivos CORRIGIDOS salvos:\n",
      "   ‚Ä¢ data/submissao3/lgbm_classifier_CORRIGIDO.pkl\n",
      "   ‚Ä¢ data/submissao3/classification_features_CORRIGIDO.pkl\n",
      "   ‚Ä¢ data/submissao3/classification_feature_importance_CORRIGIDO.csv\n",
      "   ‚Ä¢ data/submissao3/lgbm_classifier_CORRIGIDO_metadata.pkl\n",
      "\\nüéâ MODELO DE CLASSIFICA√á√ÉO CORRIGIDO TREINADO!\n",
      "======================================================================\n",
      "üéØ Resumo da Corre√ß√£o:\n",
      "   ‚ùå Problema original: Rolling features inclu√≠am semana atual\n",
      "   ‚úÖ Corre√ß√£o aplicada: Rolling baseado em quantidade_lag_1\n",
      "   üìä Features seguras usadas: 41\n",
      "   üìä AUC: 0.8473\n",
      "   üìä F1-Score: 0.1460\n",
      "\\nü§î AUC MUITO BAIXO - POSS√çVEL PROBLEMA\n",
      "   ‚ö†Ô∏è AUC < 0.95 pode indicar features insuficientes\n",
      "   üîç Verifique se features importantes n√£o foram removidas\n",
      "\\nüí° Pr√≥ximo passo:\n",
      "   üîÑ Treinar modelo de regress√£o (Notebook 12)\n",
      "   üìä Pode usar features atuais no regressor (quando vendeu=1)\n",
      "\\nüöÄ Primeiro est√°gio do modelo de dois est√°gios!\n"
     ]
    }
   ],
   "source": [
    "# Salvar modelo treinado SEM TARGET LEAKAGE\n",
    "print('üíæ Salvando modelo de classifica√ß√£o SEM LEAKAGE...')\n",
    "\n",
    "# Salvar modelo\n",
    "with open('../data/submissao3/lgbm_classifier_CORRIGIDO.pkl', 'wb') as f:\n",
    "    pickle.dump(lgbm_classifier, f)\n",
    "\n",
    "# CR√çTICO: Salvar lista de features SEGURAS (sem leakage)\n",
    "with open('../data/submissao3/classification_features_CORRIGIDO.pkl', 'wb') as f:\n",
    "    pickle.dump(features_modelo_seguros, f)\n",
    "\n",
    "# Salvar import√¢ncias\n",
    "importance_df.to_csv('../data/submissao3/classification_feature_importance_CORRIGIDO.csv', index=False)\n",
    "\n",
    "# Salvar metadados do modelo CORRIGIDO\n",
    "metadados_classifier = {\n",
    "    'data_criacao': pd.Timestamp.now(),\n",
    "    'modelo': 'LightGBM Classifier SEM TARGET LEAKAGE',\n",
    "    'objetivo': 'Classifica√ß√£o: prever se vai vender (vendeu = 1 ou 0)',\n",
    "    'parametros': lgbm_params,\n",
    "    'melhor_iteracao': lgbm_classifier.best_iteration_,\n",
    "    'melhor_score': lgbm_classifier.best_score_,\n",
    "    'total_features': len(features_modelo_seguros),\n",
    "    'features_usadas': features_modelo_seguros,\n",
    "    'features_removidas_por_leakage': features_com_leakage,\n",
    "    'correcao_target_leakage': {\n",
    "        'problema_original': 'Rolling features inclu√≠am valor da semana atual (target leakage)',\n",
    "        'solucao_aplicada': 'Rolling/EWMA baseados em quantidade_lag_1, n√£o quantidade atual',\n",
    "        'dados_corrigidos': 'Usado train_features_CORRIGIDO.parquet',\n",
    "        'features_removidas': len(features_com_leakage),\n",
    "        'features_seguras_restantes': len(features_modelo_seguros)\n",
    "    },\n",
    "    'metricas_validacao': {\n",
    "        'auc': auc_val,\n",
    "        'f1_score': f1_val,\n",
    "        'precision': precision_val,\n",
    "        'recall': recall_val\n",
    "    },\n",
    "    'shape_treino': X_train.shape,\n",
    "    'shape_validacao': X_val.shape,\n",
    "    'distribuicao_target_treino': y_train.value_counts().to_dict(),\n",
    "    'distribuicao_target_validacao': y_val.value_counts().to_dict(),\n",
    "    'observacoes': [\n",
    "        'VERS√ÉO CORRIGIDA: Rolling features baseadas em dados defasados',\n",
    "        'Notebook 10 foi corrigido para calcular features sobre quantidade_lag_1',\n",
    "        'Features que usavam informa√ß√£o atual foram removidas',\n",
    "        'Dados carregados de train_features_CORRIGIDO.parquet'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../data/submissao3/lgbm_classifier_CORRIGIDO_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_classifier, f)\n",
    "\n",
    "print('‚úÖ Arquivos CORRIGIDOS salvos:')\n",
    "print('   ‚Ä¢ data/submissao3/lgbm_classifier_CORRIGIDO.pkl')\n",
    "print('   ‚Ä¢ data/submissao3/classification_features_CORRIGIDO.pkl')\n",
    "print('   ‚Ä¢ data/submissao3/classification_feature_importance_CORRIGIDO.csv')\n",
    "print('   ‚Ä¢ data/submissao3/lgbm_classifier_CORRIGIDO_metadata.pkl')\n",
    "\n",
    "print('\\\\nüéâ MODELO DE CLASSIFICA√á√ÉO CORRIGIDO TREINADO!')\n",
    "print('=' * 70)\n",
    "print('üéØ Resumo da Corre√ß√£o:')\n",
    "print(f'   ‚ùå Problema original: Rolling features inclu√≠am semana atual')\n",
    "print(f'   ‚úÖ Corre√ß√£o aplicada: Rolling baseado em quantidade_lag_1')\n",
    "print(f'   üìä Features seguras usadas: {len(features_modelo_seguros)}')\n",
    "print(f'   üìä AUC: {auc_val:.4f}')\n",
    "print(f'   üìä F1-Score: {f1_val:.4f}')\n",
    "\n",
    "# Diagn√≥stico final\n",
    "if auc_val > 0.999:\n",
    "    print('\\\\n‚ö†Ô∏è AINDA HAY TARGET LEAKAGE DETECTADO!')\n",
    "    print('   üîç AUC muito alto (>0.999) indica vazamento de informa√ß√£o')\n",
    "    print('   üí° Recomenda√ß√µes:')\n",
    "    print('   1. Verificar se os dados CORRIGIDOS foram usados')\n",
    "    print('   2. Re-executar Notebook 10 com as corre√ß√µes aplicadas')\n",
    "    print('   3. Verificar se rolling features est√£o baseadas em lags')\n",
    "elif auc_val > 0.95:\n",
    "    print('\\\\n‚úÖ TARGET LEAKAGE CORRIGIDO COM SUCESSO!')\n",
    "    print('   üéØ AUC em faixa realista (0.95-0.99)')\n",
    "    print('   üìä M√©tricas indicam modelo funcional')\n",
    "else:\n",
    "    print('\\\\nü§î AUC MUITO BAIXO - POSS√çVEL PROBLEMA')\n",
    "    print('   ‚ö†Ô∏è AUC < 0.95 pode indicar features insuficientes')\n",
    "    print('   üîç Verifique se features importantes n√£o foram removidas')\n",
    "\n",
    "print('\\\\nüí° Pr√≥ximo passo:')\n",
    "print('   üîÑ Treinar modelo de regress√£o (Notebook 12)')\n",
    "print('   üìä Pode usar features atuais no regressor (quando vendeu=1)')\n",
    "\n",
    "print('\\\\nüöÄ Primeiro est√°gio do modelo de dois est√°gios!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
