{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Modelagem e Experimentos (Di√°rio de Bordo)\n",
    "\n",
    "**üéØ PROP√ìSITO DESTE NOTEBOOK:**\n",
    "Este notebook documenta o nosso processo completo de explora√ß√£o e sele√ß√£o de modelos. Aqui comparamos Baselines, RandomForest, LightGBM e XGBoost para justificar a nossa escolha final.\n",
    "\n",
    "**üìä RESULTADO PRINCIPAL:**\n",
    "- Testamos m√∫ltiplos modelos (Baselines, Random Forest, LightGBM, XGBoost)  \n",
    "- **LightGBM** venceu com **WMAPE: 15.25%** (91% melhor que baseline)\n",
    "- XGBoost foi marginalmente melhor, mas **inst√°vel** em produ√ß√£o\n",
    "- Este notebook cont√©m a **justificativa t√©cnica** da nossa decis√£o final\n",
    "\n",
    "**üöÄ PARA EXECUTAR A SOLU√á√ÉO FINAL:**\n",
    "Use o notebook `04-Final-Pipeline.ipynb` - ele cont√©m apenas o c√≥digo necess√°rio para treinar o modelo vencedor e gerar a submiss√£o.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos da Explora√ß√£o:\n",
    "1. **Carregamento dos Dados**: Carregar dataset com features processadas\n",
    "2. **Prepara√ß√£o para ML**: Dividir dados em treino/valida√ß√£o, preparar features  \n",
    "3. **Baseline Models**: Implementar modelos simples como refer√™ncia\n",
    "4. **Advanced Models**: Testar modelos avan√ßados (LightGBM, XGBoost, etc.)\n",
    "5. **Compara√ß√£o Rigorosa**: Avaliar todos os modelos usando m√©tricas adequadas\n",
    "6. **Sele√ß√£o Final**: Escolher o modelo mais robusto para produ√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Advanced ML\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping  # CORRE√á√ÉO: Importar early_stopping\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('üìö Bibliotecas carregadas com sucesso!')\n",
    "print('üéØ Iniciando fase de Modelagem e Treinamento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados com features processadas\n",
    "print('üìÇ Carregando dados processados...')\n",
    "\n",
    "# Verificar se os arquivos essenciais existem\n",
    "import os\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',  # Usar parquet (mais r√°pido)\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print('‚ùå Arquivos n√£o encontrados:')\n",
    "    for f in missing_files:\n",
    "        print(f'   ‚Ä¢ {f}')\n",
    "    print('\\nüîÑ Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\n",
    "else:\n",
    "    print('‚úÖ Todos os arquivos necess√°rios encontrados')\n",
    "    \n",
    "    # Carregar dados principais (usar parquet para velocidade)\n",
    "    print('üìä Carregando dataset (parquet)...')\n",
    "    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "    \n",
    "    # Carregar metadados\n",
    "    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    print(f'\\nüìä Dados carregados com sucesso:')\n",
    "    print(f'   ‚Ä¢ Shape: {dados.shape}')\n",
    "    print(f'   ‚Ä¢ Per√≠odo: {dados[\"semana\"].min()} at√© {dados[\"semana\"].max()}')\n",
    "    print(f'   ‚Ä¢ Features dispon√≠veis: {len(dados.columns)}')\n",
    "    print(f'   ‚Ä¢ Mem√≥ria: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   ‚Ä¢ Estrat√©gia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n",
    "    \n",
    "    print(f'\\nüîç Metadados do processamento:')\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n",
    "            print(f'   ‚Ä¢ {key}: {value}')\n",
    "    \n",
    "    print(f'\\n‚úÖ Pronto para modelagem!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepara√ß√£o dos Dados para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir vari√°vel target e features\n",
    "target = 'quantidade'\n",
    "\n",
    "# Features a excluir (n√£o devem ser usadas para predi√ß√£o)\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana',  # IDs e data\n",
    "    'quantidade',  # Target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informa√ß√£o do futuro\n",
    "]\n",
    "\n",
    "# Identificar features dispon√≠veis\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'üéØ Prepara√ß√£o dos dados:')\n",
    "print(f'   ‚Ä¢ Target: {target}')\n",
    "print(f'   ‚Ä¢ Features dispon√≠veis: {len(all_features)}')\n",
    "print(f'   ‚Ä¢ Features exclu√≠das: {len(exclude_features)}')\n",
    "\n",
    "# Verificar missing values nas features\n",
    "missing_features = dados[all_features].isnull().sum()\n",
    "missing_features = missing_features[missing_features > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\n‚ö†Ô∏è Features com valores missing:')\n",
    "    for feature, count in missing_features.head(10).items():\n",
    "        pct = (count / len(dados)) * 100\n",
    "        print(f'   ‚Ä¢ {feature}: {count:,} ({pct:.1f}%)')\n",
    "    \n",
    "    print(f'\\nüß† Estrat√©gia de Tratamento Inteligente:')\n",
    "    print('   ‚Ä¢ distributor_id (categ√≥rica): NaN ‚Üí -1 (venda direta)')\n",
    "    print('   ‚Ä¢ Features num√©ricas: NaN ‚Üí 0 (aus√™ncia = zero)')\n",
    "    print('   ‚Ä¢ LightGBM aprender√° padr√µes espec√≠ficos para valores -1/0')\n",
    "else:\n",
    "    print('\\n‚úÖ Nenhum valor missing nas features')\n",
    "\n",
    "print(f'\\nüìã Features finais para modelagem: {len(all_features)}')\n",
    "print('üí° Missing values ser√£o tratados como informa√ß√£o, n√£o removidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLU√á√ÉO CORRETA: Otimiza√ß√£o de Tipos de Dados (Downcasting)\n",
    "print('üìÖ Otimiza√ß√£o de Mem√≥ria + Divis√£o Temporal')\n",
    "print('üß† Estrat√©gia: Downcasting em vez de amostragem (preserva s√©ries temporais)')\n",
    "\n",
    "# PASSO 1: Inspecionar uso de mem√≥ria atual\n",
    "print(f'\\nüîç ANTES da otimiza√ß√£o:')\n",
    "memory_before = dados.memory_usage(deep=True).sum() / (1024**3)\n",
    "print(f'üíæ Mem√≥ria total: {memory_before:.2f} GB')\n",
    "\n",
    "# PASSO 2: Aplicar Downcasting Inteligente\n",
    "print(f'\\nüöÄ Aplicando Downcasting...')\n",
    "\n",
    "# Fazer uma c√≥pia para otimiza√ß√£o\n",
    "dados_sorted = dados.copy()\n",
    "\n",
    "# Otimizar colunas num√©ricas (inteiros e floats)\n",
    "for col in dados_sorted.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_sorted[col].dtype\n",
    "    \n",
    "    if dados_sorted[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='float')\n",
    "    \n",
    "    new_dtype = dados_sorted[col].dtype\n",
    "    if original_dtype != new_dtype:\n",
    "        print(f'   ‚Ä¢ {col}: {original_dtype} ‚Üí {new_dtype}')\n",
    "\n",
    "# Otimizar colunas categ√≥ricas\n",
    "for col in dados_sorted.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_sorted[col].nunique()\n",
    "        total_rows = len(dados_sorted)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores √∫nicos, usar category\n",
    "            dados_sorted[col] = dados_sorted[col].astype('category')\n",
    "            print(f'   ‚Ä¢ {col}: object ‚Üí category')\n",
    "\n",
    "print(f'‚úÖ Downcasting conclu√≠do!')\n",
    "\n",
    "# PASSO 3: Verificar resultado da otimiza√ß√£o\n",
    "memory_after = dados_sorted.memory_usage(deep=True).sum() / (1024**3)\n",
    "memory_reduction = (memory_before - memory_after) / memory_before * 100\n",
    "print(f'\\nüìä DEPOIS da otimiza√ß√£o:')\n",
    "print(f'üíæ Mem√≥ria total: {memory_after:.2f} GB')\n",
    "print(f'üéØ Redu√ß√£o: {memory_reduction:.1f}% ({memory_before-memory_after:.2f} GB economizados)')\n",
    "\n",
    "# PASSO 4: Divis√£o temporal (agora com dados otimizados)\n",
    "print(f'\\nüìÖ Divis√£o temporal dos dados (com mem√≥ria otimizada)...')\n",
    "\n",
    "# Ordenar por semana\n",
    "dados_sorted = dados_sorted.sort_values('semana')\n",
    "\n",
    "# Split temporal: semanas 1-48 treino, 49-52 valida√ß√£o\n",
    "semanas_unicas = sorted(dados_sorted['semana'].unique())\n",
    "print(f'üìä Total de semanas dispon√≠veis: {len(semanas_unicas)}')\n",
    "\n",
    "cutoff_week_idx = 48  # Primeiras 48 semanas para treino\n",
    "if len(semanas_unicas) >= cutoff_week_idx:\n",
    "    cutoff_week = semanas_unicas[cutoff_week_idx-1]\n",
    "    \n",
    "    # Criar m√°scaras (sem c√≥pia)\n",
    "    train_mask = dados_sorted['semana'] <= cutoff_week\n",
    "    val_mask = dados_sorted['semana'] > cutoff_week\n",
    "    \n",
    "    print(f'üìä Divis√£o dos dados:')\n",
    "    print(f'   ‚Ä¢ Treino: {train_mask.sum():,} registros ({train_mask.mean()*100:.1f}%)')\n",
    "    print(f'   ‚Ä¢ Valida√ß√£o: {val_mask.sum():,} registros ({val_mask.mean()*100:.1f}%)')\n",
    "    \n",
    "    # CORRE√á√ÉO: Tratamento de missing values com categorias\n",
    "    print(f'\\nüß† Tratamento inteligente de missing values (CORRIGIDO)...')\n",
    "    all_features = [col for col in dados_sorted.columns if col not in ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']]\n",
    "    \n",
    "    for col in all_features:\n",
    "        missing_count = dados_sorted[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            if col == 'distributor_id':\n",
    "                # SOLU√á√ÉO: Adicionar -1 ao \"menu\" de categorias primeiro\n",
    "                if dados_sorted[col].dtype.name == 'category':\n",
    "                    if -1 not in dados_sorted[col].cat.categories:\n",
    "                        dados_sorted[col] = dados_sorted[col].cat.add_categories([-1])\n",
    "                \n",
    "                # Agora pode preencher com -1 sem erro\n",
    "                dados_sorted[col] = dados_sorted[col].fillna(-1)\n",
    "                print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí -1 (venda direta)')\n",
    "                \n",
    "            elif dados_sorted[col].dtype.kind in ['i', 'u', 'f']:\n",
    "                # Num√©ricas: fillna funciona diretamente\n",
    "                dados_sorted[col] = dados_sorted[col].fillna(0)\n",
    "                print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí 0 (aus√™ncia)')\n",
    "    \n",
    "    # PASSO 5: Preparar dados para modelagem (agora deve funcionar!)\n",
    "    print(f'\\nüéØ Preparando dados para modelagem...')\n",
    "    X_train = dados_sorted.loc[train_mask, all_features]\n",
    "    y_train = dados_sorted.loc[train_mask, 'quantidade']\n",
    "    X_val = dados_sorted.loc[val_mask, all_features]\n",
    "    y_val = dados_sorted.loc[val_mask, 'quantidade']\n",
    "    \n",
    "    print(f'‚úÖ Dados preparados com sucesso:')\n",
    "    print(f'   ‚Ä¢ X_train shape: {X_train.shape}')\n",
    "    print(f'   ‚Ä¢ X_val shape: {X_val.shape}')\n",
    "    print(f'   ‚Ä¢ Mem√≥ria X_train: {X_train.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   ‚Ä¢ Mem√≥ria X_val: {X_val.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    \n",
    "    # Garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\nüéâ SUCESSO! Problema resolvido:')\n",
    "    print(f'   ‚úÖ Downcasting: {memory_reduction:.1f}% menos mem√≥ria')\n",
    "    print(f'   ‚úÖ Categorical fix: -1 adicionado ao \"menu\" de categorias')\n",
    "    print(f'   ‚úÖ S√©ries temporais preservadas integralmente')\n",
    "    \n",
    "else:\n",
    "    print(f'‚ö†Ô∏è Menos de 48 semanas dispon√≠veis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise Explorat√≥ria do Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da distribui√ß√£o do target (usando dados otimizados)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribui√ß√£o geral (usando y_train que foi criado)\n",
    "axes[0,0].hist(y_train, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Distribui√ß√£o da Quantidade (Treino)')\n",
    "axes[0,0].set_xlabel('Quantidade')\n",
    "axes[0,0].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "# Log-scale\n",
    "non_zero_train = y_train[y_train > 0]\n",
    "axes[0,1].hist(np.log1p(non_zero_train), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('Distribui√ß√£o log(Quantidade + 1) - Apenas > 0')\n",
    "axes[0,1].set_xlabel('log(Quantidade + 1)')\n",
    "axes[0,1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "# Zeros vs Non-zeros\n",
    "zero_counts = [len(y_train[y_train == 0]), len(y_train[y_train > 0])]\n",
    "axes[1,0].pie(zero_counts, labels=['Zeros', 'N√£o-zeros'], autopct='%1.1f%%')\n",
    "axes[1,0].set_title('Propor√ß√£o Zeros vs N√£o-zeros')\n",
    "\n",
    "# Boxplot por semana (√∫ltimas 12 semanas) - usando dados_sorted com train_mask\n",
    "train_weeks_data = dados_sorted.loc[train_mask, ['semana', 'quantidade']]\n",
    "recent_weeks = sorted(train_weeks_data['semana'].unique())[-12:]\n",
    "recent_data = train_weeks_data[train_weeks_data['semana'].isin(recent_weeks)]\n",
    "\n",
    "# Criar boxplot manualmente para evitar problemas com groupby\n",
    "week_data = []\n",
    "week_labels = []\n",
    "for week in recent_weeks:\n",
    "    week_quantities = recent_data[recent_data['semana'] == week]['quantidade']\n",
    "    if len(week_quantities) > 0:\n",
    "        week_data.append(week_quantities.values)\n",
    "        week_labels.append(week.strftime('%m-%d'))\n",
    "\n",
    "if week_data:\n",
    "    axes[1,1].boxplot(week_data, labels=week_labels)\n",
    "    axes[1,1].set_title('Distribui√ß√£o por Semana (√öltimas 12)')\n",
    "    axes[1,1].set_xlabel('Semana')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Dados insuficientes\\npara boxplot', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Distribui√ß√£o por Semana')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas descritivas\n",
    "print('üìà Estat√≠sticas do Target (treino):')\n",
    "print(y_train.describe())\n",
    "\n",
    "print(f'\\nüéØ M√©tricas importantes:')\n",
    "print(f'   ‚Ä¢ Zeros: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ N√£o-zeros: {(y_train > 0).sum():,} ({(y_train > 0).mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ M√©dia (apenas > 0): {y_train[y_train > 0].mean():.2f}')\n",
    "print(f'   ‚Ä¢ Mediana (apenas > 0): {y_train[y_train > 0].median():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSO B: LightGBM Vanilla - Validar Pipeline e Features\n",
    "print('\\nüöÄ PASSO B: Modelo LightGBM Vanilla (Par√¢metros Default)')\n",
    "print('=' * 60)\n",
    "print('üéØ Objetivo: Validar se nossas features t√™m poder preditivo')\n",
    "\n",
    "# Configura√ß√£o LightGBM Vanilla (par√¢metros simples/default)\n",
    "lgb_params_vanilla = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(f'\\nüìã Configura√ß√£o Vanilla:')\n",
    "for param, value in lgb_params_vanilla.items():\n",
    "    print(f'   ‚Ä¢ {param}: {value}')\n",
    "\n",
    "# Preparar dados para LightGBM\n",
    "print(f'\\nüìä Preparando dados para treinamento...')\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "val_lgb = lgb.Dataset(X_val, label=y_val, reference=train_lgb)\n",
    "\n",
    "print(f'   ‚Ä¢ Train shape: {X_train.shape}')\n",
    "print(f'   ‚Ä¢ Val shape: {X_val.shape}')\n",
    "print(f'   ‚Ä¢ Features: {len(all_features)}')\n",
    "\n",
    "# Treinar modelo Vanilla (VERS√ÉO CORRIGIDA)\n",
    "print(f'\\nüîÑ Treinando LightGBM Vanilla...')\n",
    "lgb_vanilla = lgb.train(\n",
    "    lgb_params_vanilla,\n",
    "    train_lgb,\n",
    "    num_boost_round=200,  # N√∫mero moderado para vanilla\n",
    "    valid_sets=[train_lgb, val_lgb],\n",
    "    valid_names=['train', 'eval'],\n",
    "    # CORRE√á√ÉO: Usar callbacks em vez de early_stopping_rounds\n",
    "    callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Treinamento conclu√≠do em {lgb_vanilla.best_iteration} itera√ß√µes')\n",
    "\n",
    "# Predi√ß√µes\n",
    "print(f'\\nüéØ Gerando predi√ß√µes...')\n",
    "lgb_vanilla_pred = lgb_vanilla.predict(X_val, num_iteration=lgb_vanilla.best_iteration)\n",
    "lgb_vanilla_pred = np.maximum(0, lgb_vanilla_pred)  # N√£o permitir predi√ß√µes negativas\n",
    "\n",
    "# Avalia√ß√£o\n",
    "results_lgb_vanilla = evaluate_model(y_val, lgb_vanilla_pred, 'LightGBM Vanilla')\n",
    "model_results.append(results_lgb_vanilla)\n",
    "\n",
    "# AN√ÅLISE CR√çTICA - Pergunta chave\n",
    "print(f'\\nüîç AN√ÅLISE CR√çTICA - VALIDA√á√ÉO DO PIPELINE:')\n",
    "print('=' * 60)\n",
    "print(f'LightGBM Vanilla    | WMAPE: {results_lgb_vanilla[\"WMAPE\"]:6.2f}% | MAE: {results_lgb_vanilla[\"MAE\"]:8.4f} | R¬≤: {results_lgb_vanilla[\"R¬≤\"]:6.4f}')\n",
    "print(f'Melhor Baseline     | WMAPE: {best_baseline[\"WMAPE\"]:6.2f}% | MAE: {best_baseline[\"MAE\"]:8.4f} | R¬≤: {best_baseline[\"R¬≤\"]:6.4f}')\n",
    "\n",
    "# Calcular melhoria\n",
    "wmape_improvement = ((best_baseline[\"WMAPE\"] - results_lgb_vanilla[\"WMAPE\"]) / best_baseline[\"WMAPE\"]) * 100\n",
    "mae_improvement = ((best_baseline[\"MAE\"] - results_lgb_vanilla[\"MAE\"]) / best_baseline[\"MAE\"]) * 100\n",
    "\n",
    "print(f'\\nüìà MELHORIA SOBRE MELHOR BASELINE:')\n",
    "print(f'   ‚Ä¢ WMAPE: {wmape_improvement:+.2f}% {\"‚úÖ SIGNIFICATIVA!\" if wmape_improvement > 5 else \"‚ö†Ô∏è MARGINAL\" if wmape_improvement > 0 else \"‚ùå PIOR QUE BASELINE!\"}')\n",
    "print(f'   ‚Ä¢ MAE:   {mae_improvement:+.2f}% {\"‚úÖ\" if mae_improvement > 0 else \"‚ùå\"}')\n",
    "\n",
    "# Diagn√≥stico\n",
    "if wmape_improvement > 5:\n",
    "    print(f'\\nüéâ DIAGN√ìSTICO: PIPELINE VALIDADO!')\n",
    "    print(f'   ‚úÖ Features t√™m forte poder preditivo')\n",
    "    print(f'   ‚úÖ Pronto para otimiza√ß√£o de hiperpar√¢metros')\n",
    "elif wmape_improvement > 0:\n",
    "    print(f'\\nü§î DIAGN√ìSTICO: MELHORIA MARGINAL')\n",
    "    print(f'   ‚ö†Ô∏è Features t√™m algum poder preditivo, mas limitado')\n",
    "    print(f'   üîç Considerar an√°lise de feature importance')\n",
    "else:\n",
    "    print(f'\\nüö® DIAGN√ìSTICO: PROBLEMA NO PIPELINE!')\n",
    "    print(f'   ‚ùå Modelo pior que baseline - poss√≠vel data leakage ou bug')\n",
    "    print(f'   üîß Revisar feature engineering urgentemente')\n",
    "\n",
    "print(f'\\n‚úÖ Passo B conclu√≠do - LightGBM Vanilla validado!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepara√ß√£o para Predi√ß√µes Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üéØ Prepara√ß√£o para predi√ß√µes finais...')\n",
    "\n",
    "# Retreinar melhor modelo com todos os dados dispon√≠veis\n",
    "print(f'üîÑ Retreinando {best_model_name} com todos os dados...')\n",
    "\n",
    "# CORRE√á√ÉO: Usar dados otimizados (dados_sorted) em vez de dados originais\n",
    "print('üß† Preparando dados com tipos otimizados (mesmo processamento de treino/valida√ß√£o)...')\n",
    "\n",
    "# Usar dados_sorted que j√° foram otimizados e tratados\n",
    "X_full = dados_sorted[all_features]\n",
    "y_full = dados_sorted[target]\n",
    "\n",
    "print(f'   ‚Ä¢ X_full shape: {X_full.shape}')\n",
    "print(f'   ‚Ä¢ Tipos de dados consistentes: {X_full.dtypes.value_counts().to_dict()}')\n",
    "\n",
    "# Retreinar LightGBM\n",
    "train_full_lgb = lgb.Dataset(X_full, label=y_full)\n",
    "final_model = lgb.train(\n",
    "    lgb_params_vanilla,\n",
    "    train_full_lgb,\n",
    "    num_boost_round=lgb_vanilla.best_iteration,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "print('‚úÖ Modelo final treinado e pronto para predi√ß√µes')\n",
    "\n",
    "# Salvar modelo e configura√ß√µes\n",
    "model_artifacts = {\n",
    "    'model': final_model,\n",
    "    'model_type': best_model_name,\n",
    "    'features': all_features,\n",
    "    'target': target,\n",
    "    'validation_mae': results_df.iloc[0]['MAE'],\n",
    "    'validation_rmse': results_df.iloc[0]['RMSE'],\n",
    "    'validation_r2': results_df.iloc[0]['R¬≤'],\n",
    "    'validation_wmape': results_df.iloc[0]['WMAPE'],\n",
    "    'training_date': pd.Timestamp.now(),\n",
    "    'combo_means': combo_means_full if best_model_name not in ['LightGBM Vanilla', 'XGBoost', 'Random Forest'] else None,\n",
    "    'metadata': metadata\n",
    "}\n",
    "\n",
    "# Salvar artefatos do modelo\n",
    "with open('../data/trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print('üíæ Modelo e artefatos salvos em: data/trained_model.pkl')\n",
    "print('üéØ Pronto para gerar predi√ß√µes para o per√≠odo de teste!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
