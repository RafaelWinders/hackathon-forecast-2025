{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final de Produ√ß√£o\n",
    "\n",
    "**üéØ PROP√ìSITO:**  \n",
    "Este notebook implementa a solu√ß√£o final para gerar as predi√ß√µes de submiss√£o do hackathon.\n",
    "\n",
    "**üöÄ EXECU√á√ÉO:**  \n",
    "Execute todas as c√©lulas em sequ√™ncia. O arquivo `submission.csv` e `submission.parquet` ser√£o gerados automaticamente.\n",
    "\n",
    "**üìä MODELO ESCOLHIDO:**  \n",
    "LightGBM - selecionado ap√≥s rigorosa compara√ß√£o no notebook `03-Modeling-Experiments.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Completo:\n",
    "1. **Carregar dados processados** (do notebook 02)\n",
    "2. **Otimizar mem√≥ria** com downcasting\n",
    "3. **Treinar LightGBM** no dataset completo de 2022  \n",
    "4. **Implementar previs√£o iterativa** para as 5 semanas de 2023\n",
    "5. **Gerar submiss√£o** nos formatos CSV e Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Bibliotecas carregadas com sucesso!\n",
      "üöÄ Iniciando Pipeline Final de Produ√ß√£o\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "print('üìö Bibliotecas carregadas com sucesso!')\n",
    "print('üöÄ Iniciando Pipeline Final de Produ√ß√£o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Carregar dados com features processadas (EXATAMENTE como no experimento)\nprint('üìÇ Carregando dados processados...')\n\n# Verificar se os arquivos essenciais existem\nimport os\nrequired_files = [\n    '../data/dados_features_completo.parquet',  # Usar parquet (mais r√°pido)\n    '../data/feature_engineering_metadata.pkl'\n]\n\nmissing_files = [f for f in required_files if not os.path.exists(f)]\nif missing_files:\n    print('‚ùå Arquivos n√£o encontrados:')\n    for f in missing_files:\n        print(f'   ‚Ä¢ {f}')\n    print('\\nüîÑ Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\nelse:\n    print('‚úÖ Todos os arquivos necess√°rios encontrados')\n    \n    # Carregar dados principais (usar parquet para velocidade)\n    print('üìä Carregando dataset (parquet)...')\n    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n    \n    # Carregar metadados\n    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n        metadata = pickle.load(f)\n    \n    print(f'\\nüìä Dados carregados com sucesso:')\n    print(f'   ‚Ä¢ Shape: {dados.shape}')\n    print(f'   ‚Ä¢ Per√≠odo: {dados[\"semana\"].min()} at√© {dados[\"semana\"].max()}')\n    print(f'   ‚Ä¢ Features dispon√≠veis: {len(dados.columns)}')\n    print(f'   ‚Ä¢ Mem√≥ria: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n    print(f'   ‚Ä¢ Estrat√©gia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n    \n    print(f'\\nüîç Metadados do processamento:')\n    for key, value in metadata.items():\n        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n            print(f'   ‚Ä¢ {key}: {value}')\n    \n    print(f'\\n‚úÖ Pronto para modelagem!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimiza√ß√£o de Mem√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Aplicando otimiza√ß√£o de mem√≥ria (downcasting)...\n",
      "‚úÖ Mem√≥ria otimizada: 4491.9 MB\n",
      "üìà Redu√ß√£o de mem√≥ria: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print('üîß Aplicando otimiza√ß√£o de mem√≥ria (downcasting)...')\n",
    "\n",
    "# Downcasting de tipos num√©ricos\n",
    "for col in dados.select_dtypes(include=[np.number]).columns:\n",
    "    if dados[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='float')\n",
    "\n",
    "# Otimizar categ√≥ricas\n",
    "for col in dados.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:\n",
    "        nunique = dados[col].nunique()\n",
    "        if nunique / len(dados) < 0.5:\n",
    "            dados[col] = dados[col].astype('category')\n",
    "\n",
    "# Tratar missing values\n",
    "if 'distributor_id' in dados.columns:\n",
    "    if dados['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados['distributor_id'].cat.categories:\n",
    "            dados['distributor_id'] = dados['distributor_id'].cat.add_categories([-1])\n",
    "    dados['distributor_id'] = dados['distributor_id'].fillna(-1)\n",
    "\n",
    "# Preencher outros NaNs\n",
    "for col in dados.columns:\n",
    "    if dados[col].isnull().sum() > 0 and dados[col].dtype.kind in ['i', 'u', 'f']:\n",
    "        dados[col] = dados[col].fillna(0)\n",
    "\n",
    "memory_optimized = dados.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f'‚úÖ Mem√≥ria otimizada: {memory_optimized:.1f} MB')\n",
    "print(f'üìà Redu√ß√£o de mem√≥ria: {((memory_optimized / (dados.memory_usage(deep=True).sum() / (1024**2)) - 1) * 100):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# SOLU√á√ÉO CORRETA: Otimiza√ß√£o de Tipos de Dados (Downcasting) - C√âLULA COMPLETA DO EXPERIMENTO\nprint('üìÖ Otimiza√ß√£o de Mem√≥ria para Produ√ß√£o (Dataset Completo)')\nprint('üß† Estrat√©gia: Downcasting em vez de amostragem (preserva s√©ries temporais)')\n\n# PASSO 1: Inspecionar uso de mem√≥ria atual\nprint(f'\\nüîç ANTES da otimiza√ß√£o:')\nmemory_before = dados.memory_usage(deep=True).sum() / (1024**3)\nprint(f'üíæ Mem√≥ria total: {memory_before:.2f} GB')\n\n# PASSO 2: Aplicar Downcasting Inteligente\nprint(f'\\nüöÄ Aplicando Downcasting...')\n\n# Fazer uma c√≥pia para otimiza√ß√£o\ndados_sorted = dados.copy()\n\n# Otimizar colunas num√©ricas (inteiros e floats)\nfor col in dados_sorted.select_dtypes(include=[np.number]).columns:\n    original_dtype = dados_sorted[col].dtype\n    \n    if dados_sorted[col].dtype.kind in ['i', 'u']:  # Inteiros\n        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='integer')\n    else:  # Floats\n        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='float')\n    \n    new_dtype = dados_sorted[col].dtype\n    if original_dtype != new_dtype:\n        print(f'   ‚Ä¢ {col}: {original_dtype} ‚Üí {new_dtype}')\n\n# Otimizar colunas categ√≥ricas\nfor col in dados_sorted.select_dtypes(include=['object']).columns:\n    if col not in ['semana']:  # Preservar datetime\n        nunique = dados_sorted[col].nunique()\n        total_rows = len(dados_sorted)\n        if nunique / total_rows < 0.5:  # Se <50% valores √∫nicos, usar category\n            dados_sorted[col] = dados_sorted[col].astype('category')\n            print(f'   ‚Ä¢ {col}: object ‚Üí category')\n\nprint(f'‚úÖ Downcasting conclu√≠do!')\n\n# PASSO 3: Verificar resultado da otimiza√ß√£o\nmemory_after = dados_sorted.memory_usage(deep=True).sum() / (1024**3)\nmemory_reduction = (memory_before - memory_after) / memory_before * 100\nprint(f'\\nüìä DEPOIS da otimiza√ß√£o:')\nprint(f'üíæ Mem√≥ria total: {memory_after:.2f} GB')\nprint(f'üéØ Redu√ß√£o: {memory_reduction:.1f}% ({memory_before-memory_after:.2f} GB economizados)')\n\n# PASSO 4: SEM DIVIS√ÉO TEMPORAL (diferen√ßa para produ√ß√£o - usar TODO o dataset)\nprint(f'\\nüìÖ Prepara√ß√£o para Produ√ß√£o (Dataset Completo)...')\n\n# Ordenar por semana (manter consist√™ncia)\ndados_sorted = dados_sorted.sort_values('semana')\n\n# Verificar dados dispon√≠veis\nsemanas_unicas = sorted(dados_sorted['semana'].unique())\nprint(f'üìä Total de semanas dispon√≠veis: {len(semanas_unicas)}')\nprint(f'üìä Dataset completo ser√° usado para treinamento (produ√ß√£o)')\n\n# CORRE√á√ÉO: Tratamento de missing values com categorias\nprint(f'\\nüß† Tratamento inteligente de missing values (CORRIGIDO)...')\nall_features = [col for col in dados_sorted.columns if col not in ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']]\n\nfor col in all_features:\n    missing_count = dados_sorted[col].isnull().sum()\n    if missing_count > 0:\n        if col == 'distributor_id':\n            # SOLU√á√ÉO: Adicionar -1 ao \"menu\" de categorias primeiro\n            if dados_sorted[col].dtype.name == 'category':\n                if -1 not in dados_sorted[col].cat.categories:\n                    dados_sorted[col] = dados_sorted[col].cat.add_categories([-1])\n            \n            # Agora pode preencher com -1 sem erro\n            dados_sorted[col] = dados_sorted[col].fillna(-1)\n            print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí -1 (venda direta)')\n            \n        elif dados_sorted[col].dtype.kind in ['i', 'u', 'f']:\n            # Num√©ricas: fillna funciona diretamente\n            dados_sorted[col] = dados_sorted[col].fillna(0)\n            print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí 0 (aus√™ncia)')\n\n# PASSO 5: Preparar dados para modelagem (TODO O DATASET para produ√ß√£o)\nprint(f'\\nüéØ Preparando dados para modelagem (PRODU√á√ÉO - M√ÅXIMO PODER PREDITIVO)...')\nX_train = dados_sorted[all_features]\ny_train = dados_sorted['quantidade']\n\nprint(f'‚úÖ Dados preparados com sucesso:')\nprint(f'   ‚Ä¢ X_train shape: {X_train.shape}')\nprint(f'   ‚Ä¢ Mem√≥ria X_train: {X_train.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n\n# Garbage collection\nimport gc\ngc.collect()\n\nprint(f'\\nüéâ SUCESSO! Problema resolvido:')\nprint(f'   ‚úÖ Downcasting: {memory_reduction:.1f}% menos mem√≥ria')\nprint(f'   ‚úÖ Categorical fix: -1 adicionado ao \"menu\" de categorias')\nprint(f'   ‚úÖ S√©ries temporais preservadas integralmente')\n\nprint(f'\\nüí° ADAPTA√á√ÉO PARA PRODU√á√ÉO:')\nprint(f'   ‚Ä¢ Experimento: 98% treino + 2% valida√ß√£o')\nprint(f'   ‚Ä¢ Produ√ß√£o: 100% treino (m√°ximo poder preditivo)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o para Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 3. Treinamento do Modelo LightGBM (Produ√ß√£o)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ PASSO B: Modelo LightGBM Vanilla (Par√¢metros Default)\n",
      "============================================================\n",
      "üéØ Objetivo: Validar se nossas features t√™m poder preditivo\n",
      "\n",
      "üìã Configura√ß√£o Vanilla:\n",
      "   ‚Ä¢ objective: regression_l1\n",
      "   ‚Ä¢ metric: mae\n",
      "   ‚Ä¢ boosting_type: gbdt\n",
      "   ‚Ä¢ verbosity: -1\n",
      "   ‚Ä¢ random_state: 42\n",
      "   ‚Ä¢ n_jobs: -1\n",
      "\n",
      "üìä Preparando dados para treinamento...\n",
      "   ‚Ä¢ Train shape: (51171190, 20)\n",
      "   ‚Ä¢ Features: 20\n",
      "\n",
      "üîÑ Treinando LightGBM Vanilla (produ√ß√£o - dataset completo)...\n"
     ]
    }
   ],
   "source": [
    "# PASSO B: LightGBM Vanilla - Validar Pipeline e Features\n",
    "print('\\nüöÄ PASSO B: Modelo LightGBM Vanilla (Par√¢metros Default)')\n",
    "print('=' * 60)\n",
    "print('üéØ Objetivo: Validar se nossas features t√™m poder preditivo')\n",
    "\n",
    "# Configura√ß√£o LightGBM Vanilla (par√¢metros simples/default)\n",
    "lgb_params_vanilla = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print('\\nüìã Configura√ß√£o Vanilla:')\n",
    "for param, value in lgb_params_vanilla.items():\n",
    "    print(f'   ‚Ä¢ {param}: {value}')\n",
    "\n",
    "# Preparar dados para LightGBM\n",
    "print('\\nüìä Preparando dados para treinamento...')\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "print(f'   ‚Ä¢ Train shape: {X_train.shape}')\n",
    "print(f'   ‚Ä¢ Features: {len(all_features)}')\n",
    "\n",
    "# Treinar modelo Vanilla (VERS√ÉO PARA PRODU√á√ÉO - SEM VALIDA√á√ÉO)\n",
    "print('\\nüîÑ Treinando LightGBM Vanilla (produ√ß√£o - dataset completo)...')\n",
    "lgb_vanilla = lgb.train(\n",
    "    lgb_params_vanilla,\n",
    "    train_lgb,\n",
    "    num_boost_round=200  # N√∫mero moderado para vanilla\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Treinamento conclu√≠do em {lgb_vanilla.num_trees()} itera√ß√µes')\n",
    "\n",
    "# Salvar modelo\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "lgb_vanilla.save_model('../models/lightgbm_final.txt')\n",
    "print('üíæ Modelo salvo em: ../models/lightgbm_final.txt')\n",
    "\n",
    "print('\\n‚úÖ LightGBM treinado e salvo com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementa√ß√£o da Previs√£o Iterativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîÑ Implementando previs√£o iterativa para 2023...')\n",
    "\n",
    "# Obter combina√ß√µes √∫nicas de PDV x Produto de 2022\n",
    "combinacoes_2022 = dados[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "print(f'üìä Combina√ß√µes PDV x Produto encontradas em 2022: {len(combinacoes_2022):,}')\n",
    "\n",
    "# Definir as 5 semanas de 2023 para previs√£o\n",
    "# Assumindo que 2023 come√ßa na semana seguinte ao √∫ltimo dado de 2022\n",
    "ultima_semana_2022 = dados['semana'].max()\n",
    "semanas_2023 = []\n",
    "for i in range(1, 6):  # 5 semanas\n",
    "    proxima_semana = ultima_semana_2022 + timedelta(weeks=i)\n",
    "    semanas_2023.append(proxima_semana)\n",
    "\n",
    "print('üìÖ Semanas para previs√£o:')\n",
    "for i, semana in enumerate(semanas_2023, 1):\n",
    "    print(f'   {i}. {semana.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "# Preparar dados hist√≥ricos para features iterativas\n",
    "dados_historicos = dados.copy()\n",
    "previsoes_finais = []\n",
    "\n",
    "print('\\\\nüîÑ Iniciando previs√µes iterativas...')\n",
    "for week_num, semana_target in enumerate(semanas_2023, 1):\n",
    "    print(f'\\\\n   Semana {week_num}/5: {semana_target.strftime(\"%Y-%m-%d\")}')\n",
    "    \n",
    "    # Criar features para a semana alvo\n",
    "    dados_semana = combinacoes_2022.copy()\n",
    "    dados_semana['semana'] = semana_target\n",
    "    \n",
    "    # Engenharia de features baseada no hist√≥rico atual\n",
    "    print('     üîß Criando features...')\n",
    "    \n",
    "    # Features temporais\n",
    "    dados_semana['mes_sin'] = np.sin(2 * np.pi * semana_target.month / 12)\n",
    "    dados_semana['mes_cos'] = np.cos(2 * np.pi * semana_target.month / 12)\n",
    "    \n",
    "    # Features de lag (baseadas no hist√≥rico)\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        semana_lag = semana_target - timedelta(weeks=lag)\n",
    "        dados_lag = dados_historicos[dados_historicos['semana'] == semana_lag]\n",
    "        \n",
    "        if len(dados_lag) > 0:\n",
    "            lag_dict = dados_lag.set_index(['pdv_id', 'produto_id'])['quantidade'].to_dict()\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = dados_semana.apply(\n",
    "                lambda row: lag_dict.get((row['pdv_id'], row['produto_id']), 0), axis=1\n",
    "            )\n",
    "        else:\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = 0\n",
    "    \n",
    "    # Features de rolling (√∫ltimas 4 semanas)\n",
    "    semanas_rolling = [semana_target - timedelta(weeks=i) for i in range(1, 5)]\n",
    "    dados_rolling = dados_historicos[dados_historicos['semana'].isin(semanas_rolling)]\n",
    "    \n",
    "    if len(dados_rolling) > 0:\n",
    "        rolling_stats = dados_rolling.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "            'mean': 'mean',\n",
    "            'max': 'max', \n",
    "            'min': 'min'\n",
    "        }).reset_index()\n",
    "        \n",
    "        rolling_stats.columns = ['pdv_id', 'produto_id', 'quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']\n",
    "        dados_semana = dados_semana.merge(rolling_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']:\n",
    "        if col in dados_semana.columns:\n",
    "            dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Features de hash e hist√≥ricas (copiar l√≥gica do notebook de feature engineering)\n",
    "    dados_semana['pdv_hash'] = dados_semana['pdv_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['produto_hash'] = dados_semana['produto_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['pdv_produto_hash'] = (dados_semana['pdv_id'].astype(str) + '_' + dados_semana['produto_id'].astype(str)).apply(hash) % 1000\n",
    "    \n",
    "    # Features hist√≥ricas (toda a s√©rie at√© agora)\n",
    "    hist_stats = dados_historicos.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "        'mean': 'mean',\n",
    "        'std': 'std',\n",
    "        'max': 'max',\n",
    "        'count': 'count'\n",
    "    }).reset_index()\n",
    "    hist_stats.columns = ['pdv_id', 'produto_id', 'hist_mean', 'hist_std', 'hist_max', 'hist_count']\n",
    "    dados_semana = dados_semana.merge(hist_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['hist_mean', 'hist_std', 'hist_max', 'hist_count']:\n",
    "        dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Adicionar distributor_id (assumir como -1 para novas predi√ß√µes)\n",
    "    dados_semana['distributor_id'] = -1\n",
    "    \n",
    "    # Garantir que todas as features necess√°rias existem\n",
    "    for feature in all_features:\n",
    "        if feature not in dados_semana.columns:\n",
    "            dados_semana[feature] = 0\n",
    "    \n",
    "    # Fazer previs√µes\n",
    "    print('     üéØ Fazendo previs√µes...')\n",
    "    X_pred = dados_semana[all_features]\n",
    "    \n",
    "    # Garantir tipos de dados consistentes\n",
    "    for col in X_pred.columns:\n",
    "        if X_pred[col].dtype != X_train[col].dtype:\n",
    "            X_pred[col] = X_pred[col].astype(X_train[col].dtype)\n",
    "    \n",
    "    # CORRE√á√ÉO: Usar lgb_vanilla em vez de model\n",
    "    predictions = lgb_vanilla.predict(X_pred)\n",
    "    predictions = np.maximum(0, predictions)  # N√£o permitir valores negativos\n",
    "    \n",
    "    # Armazenar previs√µes\n",
    "    resultado_semana = dados_semana[['pdv_id', 'produto_id', 'semana']].copy()\n",
    "    resultado_semana['quantidade'] = predictions\n",
    "    previsoes_finais.append(resultado_semana)\n",
    "    \n",
    "    # Atualizar dados hist√≥ricos com as previs√µes\n",
    "    dados_historicos = pd.concat([dados_historicos, resultado_semana], ignore_index=True)\n",
    "    \n",
    "    print(f'     ‚úÖ {len(predictions):,} previs√µes geradas')\n",
    "\n",
    "print('\\\\n‚úÖ Previs√£o iterativa conclu√≠da!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gera√ß√£o dos Arquivos de Submiss√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üìã Preparando arquivos de submiss√£o...')\n",
    "\n",
    "# Consolidar todas as previs√µes\n",
    "submission_df = pd.concat(previsoes_finais, ignore_index=True)\n",
    "\n",
    "print(f'üìä Estat√≠sticas das previs√µes:')\n",
    "print(f'   ‚Ä¢ Total de registros: {len(submission_df):,}')\n",
    "print(f'   ‚Ä¢ Combina√ß√µes √∫nicas: {submission_df.groupby([\"pdv_id\", \"produto_id\"]).size().shape[0]:,}')\n",
    "print(f'   ‚Ä¢ Semanas: {submission_df[\"semana\"].nunique()}')\n",
    "print(f'   ‚Ä¢ Quantidade m√©dia: {submission_df[\"quantidade\"].mean():.4f}')\n",
    "print(f'   ‚Ä¢ Quantidade m√°xima: {submission_df[\"quantidade\"].max():.4f}')\n",
    "print(f'   ‚Ä¢ % zeros: {(submission_df[\"quantidade\"] == 0).mean()*100:.1f}%')\n",
    "\n",
    "# Criar diret√≥rio de submiss√£o\n",
    "os.makedirs('../submission', exist_ok=True)\n",
    "\n",
    "# Salvar em CSV\n",
    "csv_path = '../submission/submission.csv'\n",
    "submission_df.to_csv(csv_path, index=False)\n",
    "print(f'üíæ Arquivo CSV salvo: {csv_path}')\n",
    "\n",
    "# Salvar em Parquet\n",
    "parquet_path = '../submission/submission.parquet'\n",
    "submission_df.to_parquet(parquet_path, index=False)\n",
    "print(f'üíæ Arquivo Parquet salvo: {parquet_path}')\n",
    "\n",
    "# Verificar tamanhos dos arquivos\n",
    "csv_size = os.path.getsize(csv_path) / (1024**2)\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024**2)\n",
    "\n",
    "print(f'\\nüìè Tamanhos dos arquivos:')\n",
    "print(f'   ‚Ä¢ CSV: {csv_size:.1f} MB')\n",
    "print(f'   ‚Ä¢ Parquet: {parquet_size:.1f} MB ({parquet_size/csv_size*100:.1f}% do CSV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Valida√ß√£o e Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîç Valida√ß√£o final dos dados...')\n",
    "\n",
    "# Verificar estrutura dos dados\n",
    "print(f'\\nüìã Estrutura final:')\n",
    "print(f'   ‚Ä¢ Colunas: {list(submission_df.columns)}')\n",
    "print(f'   ‚Ä¢ Tipos: {submission_df.dtypes.to_dict()}')\n",
    "print(f'   ‚Ä¢ Shape: {submission_df.shape}')\n",
    "\n",
    "# Verificar completude\n",
    "print(f'\\n‚úÖ Verifica√ß√µes:')\n",
    "print(f'   ‚Ä¢ Valores nulos: {submission_df.isnull().sum().sum()} (deve ser 0)')\n",
    "print(f'   ‚Ä¢ Valores negativos: {(submission_df[\"quantidade\"] < 0).sum()} (deve ser 0)')\n",
    "print(f'   ‚Ä¢ Semanas √∫nicas: {sorted(submission_df[\"semana\"].dt.strftime(\"%Y-%m-%d\").unique())}')\n",
    "\n",
    "# An√°lise por semana\n",
    "print(f'\\nüìä An√°lise por semana:')\n",
    "weekly_stats = submission_df.groupby('semana')['quantidade'].agg({\n",
    "    'count': 'count',\n",
    "    'mean': 'mean',\n",
    "    'sum': 'sum',\n",
    "    'zeros': lambda x: (x == 0).sum()\n",
    "}).round(4)\n",
    "\n",
    "for semana, stats in weekly_stats.iterrows():\n",
    "    zeros_pct = (stats['zeros'] / stats['count']) * 100\n",
    "    print(f'   ‚Ä¢ {semana.strftime(\"%Y-%m-%d\")}: {stats[\"count\"]:,} registros, m√©dia={stats[\"mean\"]:.4f}, zeros={zeros_pct:.1f}%')\n",
    "\n",
    "print(f'\\nüéâ PIPELINE FINAL CONCLU√çDO COM SUCESSO!')\n",
    "print('=' * 60)\n",
    "print(f'‚úÖ Modelo LightGBM treinado com {len(dados):,} registros de 2022')\n",
    "print(f'‚úÖ Previs√µes iterativas geradas para 5 semanas de 2023')\n",
    "print(f'‚úÖ {len(submission_df):,} previs√µes salvas em CSV e Parquet')\n",
    "print(f'‚úÖ Arquivos prontos para submiss√£o no hackathon!')\n",
    "\n",
    "print(f'\\nüìÅ Arquivos gerados:')\n",
    "print(f'   ‚Ä¢ {csv_path}')\n",
    "print(f'   ‚Ä¢ {parquet_path}')\n",
    "print(f'   ‚Ä¢ ../models/lightgbm_final.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}