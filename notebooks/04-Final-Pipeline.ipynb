{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final para Submissão\n",
    "\n",
    "**🎯 PROPÓSITO DESTE NOTEBOOK:**\n",
    "Este notebook implementa o pipeline final para gerar as previsões de vendas para janeiro de 2023. Utiliza todos os dados de 2022 para treinar o modelo final e gera os arquivos de submissão nos formatos requeridos.\n",
    "\n",
    "**🚀 ESTRATÉGIA OTIMIZADA:**\n",
    "- **Treino com Dataset Completo**: Usa todos os dados de 2022 (sem divisão treino/validação)\n",
    "- **Modelo Final**: LightGBM treinado com número ótimo de iterações\n",
    "- **Predições Janeiro/2023**: Gera features para as 5 semanas de janeiro de 2023\n",
    "- **Submissão**: Salva nos formatos CSV e Parquet na pasta correta\n",
    "\n",
    "---\n",
    "\n",
    "## Fluxo do Pipeline:\n",
    "1. **Carregamento**: Dados processados com features de `02-Feature-Engineering-Dask.ipynb`\n",
    "2. **Preparação**: Otimização de memória e uso do dataset completo de 2022\n",
    "3. **Treinamento**: Modelo final LightGBM com dados completos\n",
    "4. **Predição**: Geração de features para janeiro/2023 e previsões finais\n",
    "5. **Submissão**: Arquivos finais em CSV e Parquet prontos para envio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Bibliotecas carregadas com sucesso!\n",
      "🎯 Iniciando fase de Modelagem e Treinamento\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Advanced ML\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping  # CORREÇÃO: Importar early_stopping\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('📚 Bibliotecas carregadas com sucesso!')\n",
    "print('🎯 Iniciando fase de Modelagem e Treinamento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados processados...\n",
      "✅ Todos os arquivos necessários encontrados\n",
      "📊 Carregando dataset (parquet)...\n",
      "\n",
      "📊 Dados carregados com sucesso:\n",
      "   • Shape: (51171190, 26)\n",
      "   • Período: 2022-01-25 00:00:00 até 2022-12-27 00:00:00\n",
      "   • Features disponíveis: 26\n",
      "   • Memória: 16045.8 MB\n",
      "   • Estratégia: Grid Inteligente com Dask + Polars - Big Data Optimized\n",
      "\n",
      "🔍 Metadados do processamento:\n",
      "   • total_registros: 51171190\n",
      "   • total_features: 26\n",
      "   • combinacoes_pdv_produto: 1044310\n",
      "   • semanas_cobertas: 49\n",
      "   • periodo_treino: 2022-01-25 00:00:00 a 2022-12-27 00:00:00\n",
      "   • estrategia: Grid Inteligente com Dask + Polars - Big Data Optimized\n",
      "   • tecnologia: Dask + Polars for Maximum Performance\n",
      "   • memoria_otimizada: 9974.253155708313 MB\n",
      "\n",
      "✅ Pronto para modelagem!\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com features processadas\n",
    "print('📂 Carregando dados processados...')\n",
    "\n",
    "# Verificar se os arquivos essenciais existem\n",
    "import os\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',  # Usar parquet (mais rápido)\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print('❌ Arquivos não encontrados:')\n",
    "    for f in missing_files:\n",
    "        print(f'   • {f}')\n",
    "    print('\\n🔄 Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\n",
    "else:\n",
    "    print('✅ Todos os arquivos necessários encontrados')\n",
    "    \n",
    "    # Carregar dados principais (usar parquet para velocidade)\n",
    "    print('📊 Carregando dataset (parquet)...')\n",
    "    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "    \n",
    "    # Carregar metadados\n",
    "    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    print(f'\\n📊 Dados carregados com sucesso:')\n",
    "    print(f'   • Shape: {dados.shape}')\n",
    "    print(f'   • Período: {dados[\"semana\"].min()} até {dados[\"semana\"].max()}')\n",
    "    print(f'   • Features disponíveis: {len(dados.columns)}')\n",
    "    print(f'   • Memória: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   • Estratégia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n",
    "    \n",
    "    print(f'\\n🔍 Metadados do processamento:')\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n",
    "            print(f'   • {key}: {value}')\n",
    "    \n",
    "    print(f'\\n✅ Pronto para modelagem!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação dos Dados para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Preparação dos dados:\n",
      "   • Target: quantidade\n",
      "   • Features disponíveis: 20\n",
      "   • Features excluídas: 6\n",
      "\n",
      "⚠️ Features com valores missing:\n",
      "   • distributor_id: 45,202,572 (88.3%)\n",
      "\n",
      "🧠 Estratégia de Tratamento Inteligente:\n",
      "   • distributor_id (categórica): NaN → -1 (venda direta)\n",
      "   • Features numéricas: NaN → 0 (ausência = zero)\n",
      "   • LightGBM aprenderá padrões específicos para valores -1/0\n",
      "\n",
      "📋 Features finais para modelagem: 20\n",
      "💡 Missing values serão tratados como informação, não removidos\n"
     ]
    }
   ],
   "source": [
    "# Definir variável target e features\n",
    "target = 'quantidade'\n",
    "\n",
    "# Features a excluir (não devem ser usadas para predição)\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana',  # IDs e data\n",
    "    'quantidade',  # Target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informação do futuro\n",
    "]\n",
    "\n",
    "# Identificar features disponíveis\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'🎯 Preparação dos dados:')\n",
    "print(f'   • Target: {target}')\n",
    "print(f'   • Features disponíveis: {len(all_features)}')\n",
    "print(f'   • Features excluídas: {len(exclude_features)}')\n",
    "\n",
    "# Verificar missing values nas features\n",
    "missing_features = dados[all_features].isnull().sum()\n",
    "missing_features = missing_features[missing_features > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\n⚠️ Features com valores missing:')\n",
    "    for feature, count in missing_features.head(10).items():\n",
    "        pct = (count / len(dados)) * 100\n",
    "        print(f'   • {feature}: {count:,} ({pct:.1f}%)')\n",
    "    \n",
    "    print(f'\\n🧠 Estratégia de Tratamento Inteligente:')\n",
    "    print('   • distributor_id (categórica): NaN → -1 (venda direta)')\n",
    "    print('   • Features numéricas: NaN → 0 (ausência = zero)')\n",
    "    print('   • LightGBM aprenderá padrões específicos para valores -1/0')\n",
    "else:\n",
    "    print('\\n✅ Nenhum valor missing nas features')\n",
    "\n",
    "print(f'\\n📋 Features finais para modelagem: {len(all_features)}')\n",
    "print('💡 Missing values serão tratados como informação, não removidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Preparando dados de treino com o ano de 2022 completo...\n",
      "🚀 Estratégia: Usar todos os dados disponíveis para o modelo final\n",
      "\n",
      "🔍 ANTES da otimização:\n",
      "💾 Memória total: 15.67 GB\n",
      "\n",
      "🚀 Aplicando Downcasting...\n",
      "   • quantidade: float64 → float32\n",
      "   • num_transacoes: float64 → float32\n",
      "   • mes_sin: float64 → float32\n",
      "   • mes_cos: float64 → float32\n",
      "   • quantidade_lag_1: float64 → float32\n",
      "   • quantidade_lag_2: float64 → float32\n",
      "   • quantidade_lag_3: float64 → float32\n",
      "   • quantidade_lag_4: float64 → float32\n",
      "   • quantidade_media_4w: float64 → float32\n",
      "   • quantidade_max_4w: float64 → float32\n",
      "   • quantidade_min_4w: float64 → float32\n",
      "   • pdv_hash: uint64 → int8\n",
      "   • produto_hash: uint64 → int8\n",
      "   • pdv_produto_hash: uint64 → int16\n",
      "   • hist_mean: float64 → float32\n",
      "   • hist_std: float64 → float32\n",
      "   • hist_max: float64 → float32\n",
      "   • hist_count: uint32 → int8\n",
      "   • pdv_id: object → category\n",
      "   • produto_id: object → category\n",
      "   • distributor_id: object → category\n",
      "✅ Downcasting concluído!\n",
      "\n",
      "📊 DEPOIS da otimização:\n",
      "💾 Memória total: 4.39 GB\n",
      "🎯 Redução: 72.0% (11.28 GB economizados)\n",
      "\n",
      "🧠 Tratamento inteligente de missing values...\n",
      "   • distributor_id: 45,202,572 NaN → -1 (venda direta)\n",
      "\n",
      "🎯 Preparando dados completos para o modelo final...\n",
      "✅ Dados de treino preparados:\n",
      "   • X_full shape: (51171190, 20)\n",
      "   • y_full shape: (51171190,)\n",
      "   • Período: 2022-01-25 00:00:00 até 2022-12-27 00:00:00\n",
      "   • Memória X_full: 3123.2 MB\n",
      "\n",
      "🎉 SUCESSO! Dados preparados para treinamento final:\n",
      "   ✅ Dataset completo de 2022: 51,171,190 registros\n",
      "   ✅ Features otimizadas: 20\n",
      "   ✅ Memória otimizada: 72.0% de redução\n",
      "   ✅ Pronto para treinar modelo final!\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE FINAL: Otimização de Memória + Preparação dos Dados Completos\n",
    "print('🧠 Preparando dados de treino com o ano de 2022 completo...')\n",
    "print('🚀 Estratégia: Usar todos os dados disponíveis para o modelo final')\n",
    "\n",
    "# PASSO 1: Inspecionar uso de memória atual\n",
    "print(f'\\n🔍 ANTES da otimização:')\n",
    "memory_before = dados.memory_usage(deep=True).sum() / (1024**3)\n",
    "print(f'💾 Memória total: {memory_before:.2f} GB')\n",
    "\n",
    "# PASSO 2: Aplicar Downcasting Inteligente\n",
    "print(f'\\n🚀 Aplicando Downcasting...')\n",
    "\n",
    "# Fazer uma cópia para otimização\n",
    "dados_sorted = dados.copy()\n",
    "\n",
    "# Otimizar colunas numéricas (inteiros e floats)\n",
    "for col in dados_sorted.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_sorted[col].dtype\n",
    "    \n",
    "    if dados_sorted[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='float')\n",
    "    \n",
    "    new_dtype = dados_sorted[col].dtype\n",
    "    if original_dtype != new_dtype:\n",
    "        print(f'   • {col}: {original_dtype} → {new_dtype}')\n",
    "\n",
    "# Otimizar colunas categóricas\n",
    "for col in dados_sorted.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_sorted[col].nunique()\n",
    "        total_rows = len(dados_sorted)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores únicos, usar category\n",
    "            dados_sorted[col] = dados_sorted[col].astype('category')\n",
    "            print(f'   • {col}: object → category')\n",
    "\n",
    "print(f'✅ Downcasting concluído!')\n",
    "\n",
    "# PASSO 3: Verificar resultado da otimização\n",
    "memory_after = dados_sorted.memory_usage(deep=True).sum() / (1024**3)\n",
    "memory_reduction = (memory_before - memory_after) / memory_before * 100\n",
    "print(f'\\n📊 DEPOIS da otimização:')\n",
    "print(f'💾 Memória total: {memory_after:.2f} GB')\n",
    "print(f'🎯 Redução: {memory_reduction:.1f}% ({memory_before-memory_after:.2f} GB economizados)')\n",
    "\n",
    "# PASSO 4: Tratamento de missing values\n",
    "print(f'\\n🧠 Tratamento inteligente de missing values...')\n",
    "all_features = [col for col in dados_sorted.columns if col not in ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']]\n",
    "\n",
    "for col in all_features:\n",
    "    missing_count = dados_sorted[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if col == 'distributor_id':\n",
    "            # Adicionar -1 ao \"menu\" de categorias primeiro\n",
    "            if dados_sorted[col].dtype.name == 'category':\n",
    "                if -1 not in dados_sorted[col].cat.categories:\n",
    "                    dados_sorted[col] = dados_sorted[col].cat.add_categories([-1])\n",
    "            \n",
    "            dados_sorted[col] = dados_sorted[col].fillna(-1)\n",
    "            print(f'   • {col}: {missing_count:,} NaN → -1 (venda direta)')\n",
    "            \n",
    "        elif dados_sorted[col].dtype.kind in ['i', 'u', 'f']:\n",
    "            dados_sorted[col] = dados_sorted[col].fillna(0)\n",
    "            print(f'   • {col}: {missing_count:,} NaN → 0 (ausência)')\n",
    "\n",
    "# PASSO 5: Preparar dados COMPLETOS para modelagem (SEM divisão treino/validação)\n",
    "print(f'\\n🎯 Preparando dados completos para o modelo final...')\n",
    "\n",
    "# Usar todos os dados de 2022 para o treino final\n",
    "X_full = dados_sorted[all_features]\n",
    "y_full = dados_sorted[target]\n",
    "\n",
    "print(f'✅ Dados de treino preparados:')\n",
    "print(f'   • X_full shape: {X_full.shape}')\n",
    "print(f'   • y_full shape: {y_full.shape}')\n",
    "print(f'   • Período: {dados_sorted[\"semana\"].min()} até {dados_sorted[\"semana\"].max()}')\n",
    "print(f'   • Memória X_full: {X_full.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "\n",
    "# Limpeza de memória\n",
    "import gc\n",
    "del dados\n",
    "gc.collect()\n",
    "\n",
    "print(f'\\n🎉 SUCESSO! Dados preparados para treinamento final:')\n",
    "print(f'   ✅ Dataset completo de 2022: {len(dados_sorted):,} registros')\n",
    "print(f'   ✅ Features otimizadas: {len(all_features)}')\n",
    "print(f'   ✅ Memória otimizada: {memory_reduction:.1f}% de redução')\n",
    "print(f'   ✅ Pronto para treinar modelo final!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Treinando o modelo LightGBM final com todos os dados de 2022...\n",
      "============================================================\n",
      "\n",
      "📋 Configuração do modelo final:\n",
      "   • objective: regression_l1\n",
      "   • metric: mae\n",
      "   • boosting_type: gbdt\n",
      "   • verbosity: -1\n",
      "   • random_state: 42\n",
      "   • n_jobs: -1\n",
      "\n",
      "📊 Preparando dados para treinamento...\n",
      "   • Dados shape: (51171190, 20)\n",
      "   • Features: 20\n",
      "   • Target: quantidade\n",
      "\n",
      "🔄 Treinando modelo final com 200 iterações...\n",
      "✅ Modelo final treinado com sucesso!\n",
      "💾 Modelo salvo em: ../data/final_lightgbm_model.pkl\n",
      "\n",
      "📈 Informações do modelo final:\n",
      "   • Número de árvores: 200\n",
      "   • Features utilizadas: 20\n",
      "   • Dados de treino: 51,171,190 registros\n",
      "   • Período de treino: Todo o ano de 2022\n",
      "   • Pronto para predições!\n"
     ]
    }
   ],
   "source": [
    "# TREINAMENTO DO MODELO FINAL\n",
    "print('🚀 Treinando o modelo LightGBM final com todos os dados de 2022...')\n",
    "print('=' * 60)\n",
    "\n",
    "# Parâmetros do LightGBM (baseado nos experimentos do notebook 03)\n",
    "lgb_params_final = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(f'\\n📋 Configuração do modelo final:')\n",
    "for param, value in lgb_params_final.items():\n",
    "    print(f'   • {param}: {value}')\n",
    "\n",
    "# Criar o dataset LightGBM com todos os dados\n",
    "print(f'\\n📊 Preparando dados para treinamento...')\n",
    "train_full_lgb = lgb.Dataset(X_full, label=y_full)\n",
    "\n",
    "print(f'   • Dados shape: {X_full.shape}')\n",
    "print(f'   • Features: {len(all_features)}')\n",
    "print(f'   • Target: {target}')\n",
    "\n",
    "# O número de iterações ideais foi encontrado no notebook 03 (ajustar conforme seus experimentos)\n",
    "# Usar um número fixo baseado nos experimentos anteriores\n",
    "best_iteration = 200  # Substitua pelo valor encontrado nos seus experimentos\n",
    "\n",
    "print(f'\\n🔄 Treinando modelo final com {best_iteration} iterações...')\n",
    "\n",
    "# CORREÇÃO: Removido o parâmetro \"verbose_eval\" que causava o erro\n",
    "final_model = lgb.train(\n",
    "    lgb_params_final,\n",
    "    train_full_lgb,\n",
    "    num_boost_round=best_iteration\n",
    ")\n",
    "\n",
    "print(f'✅ Modelo final treinado com sucesso!')\n",
    "\n",
    "# Salvar o modelo\n",
    "import pickle\n",
    "import os\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "with open('../data/final_lightgbm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print('💾 Modelo salvo em: ../data/final_lightgbm_model.pkl')\n",
    "\n",
    "# Informações do modelo\n",
    "print(f'\\n📈 Informações do modelo final:')\n",
    "print(f'   • Número de árvores: {final_model.num_trees()}')\n",
    "print(f'   • Features utilizadas: {len(all_features)}')\n",
    "print(f'   • Dados de treino: {len(y_full):,} registros')\n",
    "print(f'   • Período de treino: Todo o ano de 2022')\n",
    "print(f'   • Pronto para predições!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geração de Features para Janeiro/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Criando grid de dados para as 5 semanas de janeiro/2023...\n",
      "🎯 Estratégia: Criar template para todas as combinações PDV/Produto\n",
      "\n",
      "🔍 Identificando combinações únicas de PDV/Produto...\n",
      "   • Total de combinações únicas: 1,044,310\n",
      "\n",
      "📅 Definindo as 5 semanas de janeiro/2023...\n",
      "   • Semanas: ['2023-01-03', '2023-01-10', '2023-01-17', '2023-01-24', '2023-01-31']\n",
      "\n",
      "🏗️ Construindo grid de teste...\n",
      "   • Grid shape: (5221550, 3)\n",
      "   • Total de predições: 5,221,550\n",
      "\n",
      "🔧 Gerando features para o período de teste...\n",
      "⚠️ IMPORTANTE: Este é um processo complexo que deve replicar exatamente\n",
      "   as features do notebook 02-Feature-Engineering-Dask.ipynb\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['pdv_id', 'uf', 'distributor_id'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m produtos_df = pd.read_parquet(\u001b[33m'\u001b[39m\u001b[33m../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Adicionar informações básicas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m test_grid = test_grid.merge(\u001b[43mpdvs_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpdv_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdistributor_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, on=\u001b[33m'\u001b[39m\u001b[33mpdv_id\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     41\u001b[39m test_grid = test_grid.merge(produtos_df[[\u001b[33m'\u001b[39m\u001b[33mproduto_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbrand\u001b[39m\u001b[33m'\u001b[39m]], on=\u001b[33m'\u001b[39m\u001b[33mproduto_id\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m   • Features básicas adicionadas\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['pdv_id', 'uf', 'distributor_id'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# GERAÇÃO DE FEATURES PARA JANEIRO/2023\n",
    "print('📅 Criando grid de dados para as 5 semanas de janeiro/2023...')\n",
    "print('🎯 Estratégia: Criar template para todas as combinações PDV/Produto')\n",
    "\n",
    "# PASSO 1: Obter todas as combinações únicas de PDV/Produto do histórico\n",
    "print('\\n🔍 Identificando combinações únicas de PDV/Produto...')\n",
    "all_combos = dados_sorted[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "print(f'   • Total de combinações únicas: {len(all_combos):,}')\n",
    "\n",
    "# PASSO 2: Definir as 5 semanas de janeiro de 2023\n",
    "print('\\n📅 Definindo as 5 semanas de janeiro/2023...')\n",
    "prediction_weeks = pd.to_datetime(['2023-01-03', '2023-01-10', '2023-01-17', '2023-01-24', '2023-01-31'])\n",
    "print(f'   • Semanas: {[w.strftime(\"%Y-%m-%d\") for w in prediction_weeks]}')\n",
    "\n",
    "# PASSO 3: Criar o grid de teste (todas as combinações x todas as semanas)\n",
    "print('\\n🏗️ Construindo grid de teste...')\n",
    "test_grid_list = []\n",
    "for week in prediction_weeks:\n",
    "    combo_week = all_combos.copy()\n",
    "    combo_week['semana'] = week\n",
    "    test_grid_list.append(combo_week)\n",
    "\n",
    "test_grid = pd.concat(test_grid_list, ignore_index=True)\n",
    "print(f'   • Grid shape: {test_grid.shape}')\n",
    "print(f'   • Total de predições: {len(test_grid):,}')\n",
    "\n",
    "# PASSO 4: Gerar features baseadas no histórico de 2022\n",
    "print('\\n🔧 Gerando features para o período de teste...')\n",
    "print('⚠️ IMPORTANTE: Este é um processo complexo que deve replicar exatamente')\n",
    "print('   as features do notebook 02-Feature-Engineering-Dask.ipynb')\n",
    "\n",
    "# Para este exemplo, vamos criar features básicas\n",
    "# Em um cenário real, você precisaria replicar TODAS as features do notebook 02\n",
    "\n",
    "# Merge com informações básicas dos PDVs e produtos\n",
    "pdvs_df = pd.read_parquet('../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet')\n",
    "produtos_df = pd.read_parquet('../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet')\n",
    "\n",
    "# Adicionar informações básicas\n",
    "test_grid = test_grid.merge(pdvs_df[['pdv_id', 'uf', 'distributor_id']], on='pdv_id', how='left')\n",
    "test_grid = test_grid.merge(produtos_df[['produto_id', 'brand']], on='produto_id', how='left')\n",
    "\n",
    "print(f'   • Features básicas adicionadas')\n",
    "print(f'   • Grid com info básica: {test_grid.shape}')\n",
    "\n",
    "# PASSO 5: Adicionar features temporais\n",
    "test_grid['ano'] = test_grid['semana'].dt.year\n",
    "test_grid['mes'] = test_grid['semana'].dt.month  \n",
    "test_grid['semana_ano'] = test_grid['semana'].dt.isocalendar().week\n",
    "test_grid['dia_ano'] = test_grid['semana'].dt.dayofyear\n",
    "\n",
    "print(f'   • Features temporais adicionadas')\n",
    "\n",
    "# PASSO 6: Calcular lags e rolling windows (simplificado)\n",
    "print('\\n📊 Calculando features de lag e rolling windows...')\n",
    "\n",
    "# Para cada combinação PDV/Produto, calcular estatísticas do histórico\n",
    "historical_stats = dados_sorted.groupby(['pdv_id', 'produto_id'])['quantidade'].agg([\n",
    "    'mean', 'std', 'min', 'max', 'count'\n",
    "]).reset_index()\n",
    "historical_stats.columns = ['pdv_id', 'produto_id', 'hist_mean', 'hist_std', 'hist_min', 'hist_max', 'hist_count']\n",
    "\n",
    "# Merge com estatísticas históricas\n",
    "test_grid = test_grid.merge(historical_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "# Para os lags, vamos usar a última semana de 2022 como \"lag_1\"\n",
    "last_week_2022 = dados_sorted[dados_sorted['semana'] == dados_sorted['semana'].max()]\n",
    "last_week_stats = last_week_2022[['pdv_id', 'produto_id', 'quantidade']].rename(columns={'quantidade': 'quantidade_lag_1'})\n",
    "test_grid = test_grid.merge(last_week_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "print(f'   • Features de lag e estatísticas históricas adicionadas')\n",
    "\n",
    "# PASSO 7: Preencher valores ausentes e finalizar\n",
    "test_grid['distributor_id'] = test_grid['distributor_id'].fillna(-1)\n",
    "for col in ['hist_mean', 'hist_std', 'hist_min', 'hist_max', 'hist_count', 'quantidade_lag_1']:\n",
    "    test_grid[col] = test_grid[col].fillna(0)\n",
    "\n",
    "print(f'\\n✅ Features para janeiro/2023 geradas com sucesso!')\n",
    "print(f'   • Grid final: {test_grid.shape}')\n",
    "print(f'   • Features disponíveis: {list(test_grid.columns)}')\n",
    "print(f'   • Pronto para predições!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predições e Geração dos Arquivos de Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDIÇÕES FINAIS E GERAÇÃO DE SUBMISSÃO\n",
    "print('🚀 Realizando predições para janeiro/2023...')\n",
    "print('🎯 Usando modelo final treinado com todo o dataset de 2022')\n",
    "\n",
    "# PASSO 1: Preparar dados de teste com as mesmas features do treino\n",
    "print('\\n🔧 Preparando dados de teste...')\n",
    "\n",
    "# Identificar features que existem tanto no treino quanto no teste\n",
    "available_test_features = [col for col in all_features if col in test_grid.columns]\n",
    "missing_features = [col for col in all_features if col not in test_grid.columns]\n",
    "\n",
    "print(f'   • Features disponíveis no teste: {len(available_test_features)}')\n",
    "if missing_features:\n",
    "    print(f'   ⚠️ Features ausentes no teste: {len(missing_features)}')\n",
    "    print(f'     Criando com valores padrão: {missing_features[:5]}...')\n",
    "    \n",
    "    # Criar features ausentes com valores padrão (0 ou -1)\n",
    "    for feature in missing_features:\n",
    "        if 'distributor' in feature or 'id' in feature:\n",
    "            test_grid[feature] = -1\n",
    "        else:\n",
    "            test_grid[feature] = 0\n",
    "\n",
    "# Selecionar apenas as features que foram usadas no treinamento\n",
    "X_test = test_grid[all_features]\n",
    "\n",
    "print(f'   • X_test shape: {X_test.shape}')\n",
    "print(f'   • Features utilizadas: {len(all_features)}')\n",
    "\n",
    "# PASSO 2: Aplicar o mesmo tratamento de tipos de dados\n",
    "print('\\n🔧 Aplicando otimizações de tipos de dados...')\n",
    "for col in X_test.select_dtypes(include=[np.number]).columns:\n",
    "    if X_test[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        X_test[col] = pd.to_numeric(X_test[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        X_test[col] = pd.to_numeric(X_test[col], downcast='float')\n",
    "\n",
    "# PASSO 3: Gerar predições\n",
    "print('\\n🎯 Gerando predições...')\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# Garantir que as previsões não sejam negativas e arredondar para inteiros\n",
    "predictions = np.maximum(0, predictions).round().astype(int)\n",
    "\n",
    "print(f'   • Predições geradas: {len(predictions):,}')\n",
    "print(f'   • Range: {predictions.min()} - {predictions.max()}')\n",
    "print(f'   • Média: {predictions.mean():.2f}')\n",
    "print(f'   • Zeros: {(predictions == 0).sum():,} ({(predictions == 0).mean()*100:.1f}%)')\n",
    "\n",
    "# PASSO 4: Criar o DataFrame de submissão\n",
    "print('\\n📋 Criando DataFrame de submissão...')\n",
    "\n",
    "# Preparar dados para submissão\n",
    "submission_df = test_grid[['semana', 'pdv_id', 'produto_id']].copy()\n",
    "submission_df['quantidade'] = predictions\n",
    "\n",
    "# Converter semana para formato numérico (semana 1-5 de janeiro)\n",
    "submission_df['semana_num'] = submission_df['semana'].dt.isocalendar().week\n",
    "# Como são as semanas 1-5 de janeiro de 2023, ajustar numeração\n",
    "primeira_semana = submission_df['semana_num'].min()\n",
    "submission_df['semana'] = submission_df['semana_num'] - primeira_semana + 1\n",
    "\n",
    "# Renomear colunas para o padrão da submissão\n",
    "submission_df = submission_df.rename(columns={\n",
    "    'semana': 'semana',\n",
    "    'pdv_id': 'pdv', \n",
    "    'produto_id': 'produto'\n",
    "})\n",
    "\n",
    "# Selecionar apenas as colunas finais\n",
    "submission_df = submission_df[['semana', 'pdv', 'produto', 'quantidade']]\n",
    "\n",
    "print(f'   • DataFrame de submissão criado: {submission_df.shape}')\n",
    "print(f'   • Colunas: {list(submission_df.columns)}')\n",
    "print(f'   • Semanas: {sorted(submission_df[\"semana\"].unique())}')\n",
    "\n",
    "print(f'\\\\n📊 Estatísticas da submissão:')\n",
    "for semana in sorted(submission_df['semana'].unique()):\n",
    "    week_data = submission_df[submission_df['semana'] == semana]['quantidade']\n",
    "    print(f'   • Semana {semana}: {len(week_data):,} predições, média: {week_data.mean():.1f}')\n",
    "\n",
    "print(f'\\\\n✅ Dados de submissão preparados!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVAMENTO DOS ARQUIVOS DE SUBMISSÃO\n",
    "print('💾 Salvando arquivos de submissão...')\n",
    "print('🎯 Formatos: CSV (com separador ;) e Parquet')\n",
    "\n",
    "# PASSO 1: Criar pasta de submissão\n",
    "output_dir = '../submissions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f'   • Pasta criada: {output_dir}')\n",
    "\n",
    "# PASSO 2: Validar DataFrame antes de salvar\n",
    "print(f'\\n🔍 Validação final do DataFrame:')\n",
    "print(f'   • Shape: {submission_df.shape}')\n",
    "print(f'   • Colunas: {list(submission_df.columns)}')\n",
    "print(f'   • Tipos: {submission_df.dtypes.to_dict()}')\n",
    "print(f'   • Missing values: {submission_df.isnull().sum().sum()}')\n",
    "print(f'   • Valores negativos: {(submission_df[\"quantidade\"] < 0).sum()}')\n",
    "\n",
    "# Verificar se há problemas\n",
    "if submission_df.isnull().sum().sum() > 0:\n",
    "    print('   ⚠️ Há valores missing - preenchendo com 0')\n",
    "    submission_df = submission_df.fillna(0)\n",
    "\n",
    "if (submission_df[\"quantidade\"] < 0).sum() > 0:\n",
    "    print('   ⚠️ Há valores negativos - convertendo para 0')\n",
    "    submission_df[\"quantidade\"] = submission_df[\"quantidade\"].clip(lower=0)\n",
    "\n",
    "# PASSO 3: Salvar no formato CSV\n",
    "csv_path = os.path.join(output_dir, 'submission.csv')\n",
    "submission_df.to_csv(\n",
    "    csv_path,\n",
    "    sep=';',  # Separador conforme especificação\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "print(f'   • Arquivo CSV salvo: {csv_path}')\n",
    "\n",
    "# PASSO 4: Salvar no formato Parquet\n",
    "parquet_path = os.path.join(output_dir, 'submission.parquet')\n",
    "submission_df.to_parquet(parquet_path, index=False)\n",
    "print(f'   • Arquivo Parquet salvo: {parquet_path}')\n",
    "\n",
    "# PASSO 5: Verificar arquivos salvos\n",
    "import os\n",
    "csv_size = os.path.getsize(csv_path) / (1024**2)  # MB\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024**2)  # MB\n",
    "\n",
    "print(f'\\n📁 Arquivos de submissão gerados:')\n",
    "print(f'   • CSV: {csv_path} ({csv_size:.1f} MB)')\n",
    "print(f'   • Parquet: {parquet_path} ({parquet_size:.1f} MB)')\n",
    "\n",
    "# PASSO 6: Visualizar algumas linhas do resultado\n",
    "print(f'\\n👀 Preview da submissão (primeiras 10 linhas):')\n",
    "print(submission_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f'\\n🎉 PROCESSO FINALIZADO!')\n",
    "print(f'=' * 60)\n",
    "print(f'✅ Modelo LightGBM treinado com {len(y_full):,} registros de 2022')\n",
    "print(f'✅ Predições geradas para {len(submission_df):,} combinações')\n",
    "print(f'✅ Arquivos de submissão salvos em: {output_dir}')\n",
    "print(f'✅ Formatos: CSV (;) e Parquet')\n",
    "print(f'')\n",
    "print(f'🚀 PRÓXIMOS PASSOS:')\n",
    "print(f'   1. Validar os arquivos gerados')\n",
    "print(f'   2. Fazer upload conforme regulamento do hackathon')\n",
    "print(f'   3. Acompanhar resultados na plataforma')\n",
    "print(f'')\n",
    "print(f'🏆 Boa sorte no hackathon!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
