{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final de Produﾃｧﾃ｣o\n",
    "\n",
    "**識 PROPﾃ鉄ITO:**  \n",
    "Este notebook implementa a soluﾃｧﾃ｣o final para gerar as prediﾃｧﾃｵes de submissﾃ｣o do hackathon.\n",
    "\n",
    "**噫 EXECUﾃﾃグ:**  \n",
    "Execute todas as cﾃｩlulas em sequﾃｪncia. O arquivo `submission.csv` e `submission.parquet` serﾃ｣o gerados automaticamente.\n",
    "\n",
    "**投 MODELO ESCOLHIDO:**  \n",
    "LightGBM - selecionado apﾃｳs rigorosa comparaﾃｧﾃ｣o no notebook `03-Modeling-Experiments.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Completo:\n",
    "1. **Carregar dados processados** (do notebook 02)\n",
    "2. **Otimizar memﾃｳria** com downcasting\n",
    "3. **Treinar LightGBM** no dataset completo de 2022  \n",
    "4. **Implementar previsﾃ｣o iterativa** para as 5 semanas de 2023\n",
    "5. **Gerar submissﾃ｣o** nos formatos CSV e Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答 Bibliotecas carregadas com sucesso!\n",
      "噫 Iniciando Pipeline Final de Produﾃｧﾃ｣o\n"
     ]
    }
   ],
   "source": [
    "# Importaﾃｧﾃｵes necessﾃ｡rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "print('答 Bibliotecas carregadas com sucesso!')\n",
    "print('噫 Iniciando Pipeline Final de Produﾃｧﾃ｣o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Preparaﾃｧﾃ｣o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐 Carregando dados processados...\n",
      "笨 Dados carregados: (51171190, 26)\n",
      "套 Perﾃｭodo: 2022-01-25 00:00:00 atﾃｩ 2022-12-27 00:00:00\n",
      "沈 Memﾃｳria inicial: 16045.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Verificar arquivos necessﾃ｡rios\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    raise FileNotFoundError(f'Arquivos nﾃ｣o encontrados: {missing_files}')\n",
    "\n",
    "print('唐 Carregando dados processados...')\n",
    "dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "\n",
    "with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f'笨 Dados carregados: {dados.shape}')\n",
    "print(f'套 Perﾃｭodo: {dados[\"semana\"].min()} atﾃｩ {dados[\"semana\"].max()}')\n",
    "print(f'沈 Memﾃｳria inicial: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimizaﾃｧﾃ｣o de Memﾃｳria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "肌 Aplicando otimizaﾃｧﾃ｣o de memﾃｳria (downcasting)...\n",
      "笨 Memﾃｳria otimizada: 4491.9 MB\n",
      "嶋 Reduﾃｧﾃ｣o de memﾃｳria: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print('肌 Aplicando otimizaﾃｧﾃ｣o de memﾃｳria (downcasting)...')\n",
    "\n",
    "# Downcasting de tipos numﾃｩricos\n",
    "for col in dados.select_dtypes(include=[np.number]).columns:\n",
    "    if dados[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='float')\n",
    "\n",
    "# Otimizar categﾃｳricas\n",
    "for col in dados.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:\n",
    "        nunique = dados[col].nunique()\n",
    "        if nunique / len(dados) < 0.5:\n",
    "            dados[col] = dados[col].astype('category')\n",
    "\n",
    "# Tratar missing values\n",
    "if 'distributor_id' in dados.columns:\n",
    "    if dados['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados['distributor_id'].cat.categories:\n",
    "            dados['distributor_id'] = dados['distributor_id'].cat.add_categories([-1])\n",
    "    dados['distributor_id'] = dados['distributor_id'].fillna(-1)\n",
    "\n",
    "# Preencher outros NaNs\n",
    "for col in dados.columns:\n",
    "    if dados[col].isnull().sum() > 0 and dados[col].dtype.kind in ['i', 'u', 'f']:\n",
    "        dados[col] = dados[col].fillna(0)\n",
    "\n",
    "memory_optimized = dados.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f'笨 Memﾃｳria otimizada: {memory_optimized:.1f} MB')\n",
    "print(f'嶋 Reduﾃｧﾃ｣o de memﾃｳria: {((memory_optimized / (dados.memory_usage(deep=True).sum() / (1024**2)) - 1) * 100):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparaﾃｧﾃ｣o para Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "識 Configuraﾃｧﾃ｣o do modelo:\n",
      "   窶｢ Target: quantidade\n",
      "   窶｢ Features: 20\n",
      "投 Dados de treinamento:\n",
      "   窶｢ Shape: (51171190, 20)\n",
      "   窶｢ Perﾃｭodo completo de 2022\n"
     ]
    }
   ],
   "source": [
    "# Definir features e target\n",
    "target = 'quantidade'\n",
    "exclude_features = ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'識 Configuraﾃｧﾃ｣o do modelo:')\n",
    "print(f'   窶｢ Target: {target}')\n",
    "print(f'   窶｢ Features: {len(all_features)}')\n",
    "\n",
    "# Preparar dados para treinamento (todo o dataset de 2022)\n",
    "X_train = dados[all_features]\n",
    "y_train = dados[target]\n",
    "\n",
    "print(f'投 Dados de treinamento:')\n",
    "print(f'   窶｢ Shape: {X_train.shape}')\n",
    "print(f'   窶｢ Perﾃｭodo completo de 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "噫 PASSO B: Modelo LightGBM Vanilla (Parﾃ｢metros Default)\n",
      "============================================================\n",
      "識 Objetivo: Validar se nossas features tﾃｪm poder preditivo\n",
      "\n",
      "搭 Configuraﾃｧﾃ｣o Vanilla:\n",
      "   窶｢ objective: regression_l1\n",
      "   窶｢ metric: mae\n",
      "   窶｢ boosting_type: gbdt\n",
      "   窶｢ verbosity: -1\n",
      "   窶｢ random_state: 42\n",
      "   窶｢ n_jobs: -1\n",
      "\n",
      "投 Preparando dados para treinamento...\n",
      "   窶｢ Train shape: (51171190, 20)\n",
      "   窶｢ Features: 20\n",
      "\n",
      "売 Treinando LightGBM Vanilla (produﾃｧﾃ｣o - dataset completo)...\n"
     ]
    }
   ],
   "source": [
    "# PASSO B: LightGBM Vanilla - Validar Pipeline e Features\n",
    "print('\\n噫 PASSO B: Modelo LightGBM Vanilla (Parﾃ｢metros Default)')\n",
    "print('=' * 60)\n",
    "print('識 Objetivo: Validar se nossas features tﾃｪm poder preditivo')\n",
    "\n",
    "# Configuraﾃｧﾃ｣o LightGBM Vanilla (parﾃ｢metros simples/default)\n",
    "lgb_params_vanilla = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print('\\n搭 Configuraﾃｧﾃ｣o Vanilla:')\n",
    "for param, value in lgb_params_vanilla.items():\n",
    "    print(f'   窶｢ {param}: {value}')\n",
    "\n",
    "# Preparar dados para LightGBM\n",
    "print('\\n投 Preparando dados para treinamento...')\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "print(f'   窶｢ Train shape: {X_train.shape}')\n",
    "print(f'   窶｢ Features: {len(all_features)}')\n",
    "\n",
    "# Treinar modelo Vanilla (VERSﾃグ PARA PRODUﾃﾃグ - SEM VALIDAﾃﾃグ)\n",
    "print('\\n売 Treinando LightGBM Vanilla (produﾃｧﾃ｣o - dataset completo)...')\n",
    "lgb_vanilla = lgb.train(\n",
    "    lgb_params_vanilla,\n",
    "    train_lgb,\n",
    "    num_boost_round=200  # Nﾃｺmero moderado para vanilla\n",
    ")\n",
    "\n",
    "print(f'笨 Treinamento concluﾃｭdo em {lgb_vanilla.num_trees()} iteraﾃｧﾃｵes')\n",
    "\n",
    "# Salvar modelo\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "lgb_vanilla.save_model('../models/lightgbm_final.txt')\n",
    "print('沈 Modelo salvo em: ../models/lightgbm_final.txt')\n",
    "\n",
    "print('\\n笨 LightGBM treinado e salvo com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementaﾃｧﾃ｣o da Previsﾃ｣o Iterativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('売 Implementando previsﾃ｣o iterativa para 2023...')\n",
    "\n",
    "# Obter combinaﾃｧﾃｵes ﾃｺnicas de PDV x Produto de 2022\n",
    "combinacoes_2022 = dados[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "print(f'投 Combinaﾃｧﾃｵes PDV x Produto encontradas em 2022: {len(combinacoes_2022):,}')\n",
    "\n",
    "# Definir as 5 semanas de 2023 para previsﾃ｣o\n",
    "# Assumindo que 2023 comeﾃｧa na semana seguinte ao ﾃｺltimo dado de 2022\n",
    "ultima_semana_2022 = dados['semana'].max()\n",
    "semanas_2023 = []\n",
    "for i in range(1, 6):  # 5 semanas\n",
    "    proxima_semana = ultima_semana_2022 + timedelta(weeks=i)\n",
    "    semanas_2023.append(proxima_semana)\n",
    "\n",
    "print('套 Semanas para previsﾃ｣o:')\n",
    "for i, semana in enumerate(semanas_2023, 1):\n",
    "    print(f'   {i}. {semana.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "# Preparar dados histﾃｳricos para features iterativas\n",
    "dados_historicos = dados.copy()\n",
    "previsoes_finais = []\n",
    "\n",
    "print('\\\\n売 Iniciando previsﾃｵes iterativas...')\n",
    "for week_num, semana_target in enumerate(semanas_2023, 1):\n",
    "    print(f'\\\\n   Semana {week_num}/5: {semana_target.strftime(\"%Y-%m-%d\")}')\n",
    "    \n",
    "    # Criar features para a semana alvo\n",
    "    dados_semana = combinacoes_2022.copy()\n",
    "    dados_semana['semana'] = semana_target\n",
    "    \n",
    "    # Engenharia de features baseada no histﾃｳrico atual\n",
    "    print('     肌 Criando features...')\n",
    "    \n",
    "    # Features temporais\n",
    "    dados_semana['mes_sin'] = np.sin(2 * np.pi * semana_target.month / 12)\n",
    "    dados_semana['mes_cos'] = np.cos(2 * np.pi * semana_target.month / 12)\n",
    "    \n",
    "    # Features de lag (baseadas no histﾃｳrico)\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        semana_lag = semana_target - timedelta(weeks=lag)\n",
    "        dados_lag = dados_historicos[dados_historicos['semana'] == semana_lag]\n",
    "        \n",
    "        if len(dados_lag) > 0:\n",
    "            lag_dict = dados_lag.set_index(['pdv_id', 'produto_id'])['quantidade'].to_dict()\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = dados_semana.apply(\n",
    "                lambda row: lag_dict.get((row['pdv_id'], row['produto_id']), 0), axis=1\n",
    "            )\n",
    "        else:\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = 0\n",
    "    \n",
    "    # Features de rolling (ﾃｺltimas 4 semanas)\n",
    "    semanas_rolling = [semana_target - timedelta(weeks=i) for i in range(1, 5)]\n",
    "    dados_rolling = dados_historicos[dados_historicos['semana'].isin(semanas_rolling)]\n",
    "    \n",
    "    if len(dados_rolling) > 0:\n",
    "        rolling_stats = dados_rolling.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "            'mean': 'mean',\n",
    "            'max': 'max', \n",
    "            'min': 'min'\n",
    "        }).reset_index()\n",
    "        \n",
    "        rolling_stats.columns = ['pdv_id', 'produto_id', 'quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']\n",
    "        dados_semana = dados_semana.merge(rolling_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']:\n",
    "        if col in dados_semana.columns:\n",
    "            dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Features de hash e histﾃｳricas (copiar lﾃｳgica do notebook de feature engineering)\n",
    "    dados_semana['pdv_hash'] = dados_semana['pdv_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['produto_hash'] = dados_semana['produto_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['pdv_produto_hash'] = (dados_semana['pdv_id'].astype(str) + '_' + dados_semana['produto_id'].astype(str)).apply(hash) % 1000\n",
    "    \n",
    "    # Features histﾃｳricas (toda a sﾃｩrie atﾃｩ agora)\n",
    "    hist_stats = dados_historicos.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "        'mean': 'mean',\n",
    "        'std': 'std',\n",
    "        'max': 'max',\n",
    "        'count': 'count'\n",
    "    }).reset_index()\n",
    "    hist_stats.columns = ['pdv_id', 'produto_id', 'hist_mean', 'hist_std', 'hist_max', 'hist_count']\n",
    "    dados_semana = dados_semana.merge(hist_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['hist_mean', 'hist_std', 'hist_max', 'hist_count']:\n",
    "        dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Adicionar distributor_id (assumir como -1 para novas prediﾃｧﾃｵes)\n",
    "    dados_semana['distributor_id'] = -1\n",
    "    \n",
    "    # Garantir que todas as features necessﾃ｡rias existem\n",
    "    for feature in all_features:\n",
    "        if feature not in dados_semana.columns:\n",
    "            dados_semana[feature] = 0\n",
    "    \n",
    "    # Fazer previsﾃｵes\n",
    "    print('     識 Fazendo previsﾃｵes...')\n",
    "    X_pred = dados_semana[all_features]\n",
    "    \n",
    "    # Garantir tipos de dados consistentes\n",
    "    for col in X_pred.columns:\n",
    "        if X_pred[col].dtype != X_train[col].dtype:\n",
    "            X_pred[col] = X_pred[col].astype(X_train[col].dtype)\n",
    "    \n",
    "    # CORREﾃﾃグ: Usar lgb_vanilla em vez de model\n",
    "    predictions = lgb_vanilla.predict(X_pred)\n",
    "    predictions = np.maximum(0, predictions)  # Nﾃ｣o permitir valores negativos\n",
    "    \n",
    "    # Armazenar previsﾃｵes\n",
    "    resultado_semana = dados_semana[['pdv_id', 'produto_id', 'semana']].copy()\n",
    "    resultado_semana['quantidade'] = predictions\n",
    "    previsoes_finais.append(resultado_semana)\n",
    "    \n",
    "    # Atualizar dados histﾃｳricos com as previsﾃｵes\n",
    "    dados_historicos = pd.concat([dados_historicos, resultado_semana], ignore_index=True)\n",
    "    \n",
    "    print(f'     笨 {len(predictions):,} previsﾃｵes geradas')\n",
    "\n",
    "print('\\\\n笨 Previsﾃ｣o iterativa concluﾃｭda!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geraﾃｧﾃ｣o dos Arquivos de Submissﾃ｣o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('搭 Preparando arquivos de submissﾃ｣o...')\n",
    "\n",
    "# Consolidar todas as previsﾃｵes\n",
    "submission_df = pd.concat(previsoes_finais, ignore_index=True)\n",
    "\n",
    "print(f'投 Estatﾃｭsticas das previsﾃｵes:')\n",
    "print(f'   窶｢ Total de registros: {len(submission_df):,}')\n",
    "print(f'   窶｢ Combinaﾃｧﾃｵes ﾃｺnicas: {submission_df.groupby([\"pdv_id\", \"produto_id\"]).size().shape[0]:,}')\n",
    "print(f'   窶｢ Semanas: {submission_df[\"semana\"].nunique()}')\n",
    "print(f'   窶｢ Quantidade mﾃｩdia: {submission_df[\"quantidade\"].mean():.4f}')\n",
    "print(f'   窶｢ Quantidade mﾃ｡xima: {submission_df[\"quantidade\"].max():.4f}')\n",
    "print(f'   窶｢ % zeros: {(submission_df[\"quantidade\"] == 0).mean()*100:.1f}%')\n",
    "\n",
    "# Criar diretﾃｳrio de submissﾃ｣o\n",
    "os.makedirs('../submission', exist_ok=True)\n",
    "\n",
    "# Salvar em CSV\n",
    "csv_path = '../submission/submission.csv'\n",
    "submission_df.to_csv(csv_path, index=False)\n",
    "print(f'沈 Arquivo CSV salvo: {csv_path}')\n",
    "\n",
    "# Salvar em Parquet\n",
    "parquet_path = '../submission/submission.parquet'\n",
    "submission_df.to_parquet(parquet_path, index=False)\n",
    "print(f'沈 Arquivo Parquet salvo: {parquet_path}')\n",
    "\n",
    "# Verificar tamanhos dos arquivos\n",
    "csv_size = os.path.getsize(csv_path) / (1024**2)\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024**2)\n",
    "\n",
    "print(f'\\n棟 Tamanhos dos arquivos:')\n",
    "print(f'   窶｢ CSV: {csv_size:.1f} MB')\n",
    "print(f'   窶｢ Parquet: {parquet_size:.1f} MB ({parquet_size/csv_size*100:.1f}% do CSV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validaﾃｧﾃ｣o e Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('剥 Validaﾃｧﾃ｣o final dos dados...')\n",
    "\n",
    "# Verificar estrutura dos dados\n",
    "print(f'\\n搭 Estrutura final:')\n",
    "print(f'   窶｢ Colunas: {list(submission_df.columns)}')\n",
    "print(f'   窶｢ Tipos: {submission_df.dtypes.to_dict()}')\n",
    "print(f'   窶｢ Shape: {submission_df.shape}')\n",
    "\n",
    "# Verificar completude\n",
    "print(f'\\n笨 Verificaﾃｧﾃｵes:')\n",
    "print(f'   窶｢ Valores nulos: {submission_df.isnull().sum().sum()} (deve ser 0)')\n",
    "print(f'   窶｢ Valores negativos: {(submission_df[\"quantidade\"] < 0).sum()} (deve ser 0)')\n",
    "print(f'   窶｢ Semanas ﾃｺnicas: {sorted(submission_df[\"semana\"].dt.strftime(\"%Y-%m-%d\").unique())}')\n",
    "\n",
    "# Anﾃ｡lise por semana\n",
    "print(f'\\n投 Anﾃ｡lise por semana:')\n",
    "weekly_stats = submission_df.groupby('semana')['quantidade'].agg({\n",
    "    'count': 'count',\n",
    "    'mean': 'mean',\n",
    "    'sum': 'sum',\n",
    "    'zeros': lambda x: (x == 0).sum()\n",
    "}).round(4)\n",
    "\n",
    "for semana, stats in weekly_stats.iterrows():\n",
    "    zeros_pct = (stats['zeros'] / stats['count']) * 100\n",
    "    print(f'   窶｢ {semana.strftime(\"%Y-%m-%d\")}: {stats[\"count\"]:,} registros, mﾃｩdia={stats[\"mean\"]:.4f}, zeros={zeros_pct:.1f}%')\n",
    "\n",
    "print(f'\\n脂 PIPELINE FINAL CONCLUﾃ好O COM SUCESSO!')\n",
    "print('=' * 60)\n",
    "print(f'笨 Modelo LightGBM treinado com {len(dados):,} registros de 2022')\n",
    "print(f'笨 Previsﾃｵes iterativas geradas para 5 semanas de 2023')\n",
    "print(f'笨 {len(submission_df):,} previsﾃｵes salvas em CSV e Parquet')\n",
    "print(f'笨 Arquivos prontos para submissﾃ｣o no hackathon!')\n",
    "\n",
    "print(f'\\n刀 Arquivos gerados:')\n",
    "print(f'   窶｢ {csv_path}')\n",
    "print(f'   窶｢ {parquet_path}')\n",
    "print(f'   窶｢ ../models/lightgbm_final.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
