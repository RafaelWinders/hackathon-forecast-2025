{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final de Produ√ß√£o\n",
    "\n",
    "**üéØ PROP√ìSITO:**  \n",
    "Este notebook implementa a solu√ß√£o final para gerar as predi√ß√µes de submiss√£o do hackathon.\n",
    "\n",
    "**üöÄ EXECU√á√ÉO:**  \n",
    "Execute todas as c√©lulas em sequ√™ncia. O arquivo `submission.csv` e `submission.parquet` ser√£o gerados automaticamente.\n",
    "\n",
    "**üìä MODELO ESCOLHIDO:**  \n",
    "LightGBM - selecionado ap√≥s rigorosa compara√ß√£o no notebook `03-Modeling-Experiments.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Completo:\n",
    "1. **Carregar dados processados** (do notebook 02)\n",
    "2. **Otimizar mem√≥ria** com downcasting\n",
    "3. **Treinar LightGBM** no dataset completo de 2022  \n",
    "4. **Implementar previs√£o iterativa** para as 5 semanas de 2023\n",
    "5. **Gerar submiss√£o** nos formatos CSV e Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping\n",
    "import gc\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "print('üìö Bibliotecas carregadas com sucesso!')\n",
    "print('üöÄ Iniciando Pipeline Final de Produ√ß√£o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar arquivos necess√°rios\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    raise FileNotFoundError(f'Arquivos n√£o encontrados: {missing_files}')\n",
    "\n",
    "print('üìÇ Carregando dados processados...')\n",
    "dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "\n",
    "with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f'‚úÖ Dados carregados: {dados.shape}')\n",
    "print(f'üìÖ Per√≠odo: {dados[\"semana\"].min()} at√© {dados[\"semana\"].max()}')\n",
    "print(f'üíæ Mem√≥ria inicial: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimiza√ß√£o de Mem√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîß Aplicando otimiza√ß√£o de mem√≥ria (downcasting)...')\n",
    "\n",
    "# Downcasting de tipos num√©ricos\n",
    "for col in dados.select_dtypes(include=[np.number]).columns:\n",
    "    if dados[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='float')\n",
    "\n",
    "# Otimizar categ√≥ricas\n",
    "for col in dados.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:\n",
    "        nunique = dados[col].nunique()\n",
    "        if nunique / len(dados) < 0.5:\n",
    "            dados[col] = dados[col].astype('category')\n",
    "\n",
    "# Tratar missing values\n",
    "if 'distributor_id' in dados.columns:\n",
    "    if dados['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados['distributor_id'].cat.categories:\n",
    "            dados['distributor_id'] = dados['distributor_id'].cat.add_categories([-1])\n",
    "    dados['distributor_id'] = dados['distributor_id'].fillna(-1)\n",
    "\n",
    "# Preencher outros NaNs\n",
    "for col in dados.columns:\n",
    "    if dados[col].isnull().sum() > 0 and dados[col].dtype.kind in ['i', 'u', 'f']:\n",
    "        dados[col] = dados[col].fillna(0)\n",
    "\n",
    "memory_optimized = dados.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f'‚úÖ Mem√≥ria otimizada: {memory_optimized:.1f} MB')\n",
    "print(f'üìà Redu√ß√£o de mem√≥ria: {((memory_optimized / (dados.memory_usage(deep=True).sum() / (1024**2)) - 1) * 100):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o para Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir features e target\n",
    "target = 'quantidade'\n",
    "exclude_features = ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'üéØ Configura√ß√£o do modelo:')\n",
    "print(f'   ‚Ä¢ Target: {target}')\n",
    "print(f'   ‚Ä¢ Features: {len(all_features)}')\n",
    "\n",
    "# Preparar dados para treinamento (todo o dataset de 2022)\n",
    "X_train = dados[all_features]\n",
    "y_train = dados[target]\n",
    "\n",
    "print(f'üìä Dados de treinamento:')\n",
    "print(f'   ‚Ä¢ Shape: {X_train.shape}')\n",
    "print(f'   ‚Ä¢ Per√≠odo completo de 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üöÄ Treinando modelo LightGBM...')\n",
    "\n",
    "# Par√¢metros otimizados do LightGBM\n",
    "lgb_params = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# Criar dataset LightGBM\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Treinar modelo\n",
    "model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_lgb,\n",
    "    num_boost_round=500,\n",
    "    callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Modelo treinado com {model.num_trees()} √°rvores')\n",
    "\n",
    "# Salvar modelo\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "model.save_model('../models/lightgbm_final.txt')\n",
    "print('üíæ Modelo salvo em: ../models/lightgbm_final.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementa√ß√£o da Previs√£o Iterativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîÑ Implementando previs√£o iterativa para 2023...')\n",
    "\n",
    "# Obter combina√ß√µes √∫nicas de PDV x Produto de 2022\n",
    "combinacoes_2022 = dados[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "print(f'üìä Combina√ß√µes PDV x Produto encontradas em 2022: {len(combinacoes_2022):,}')\n",
    "\n",
    "# Definir as 5 semanas de 2023 para previs√£o\n",
    "# Assumindo que 2023 come√ßa na semana seguinte ao √∫ltimo dado de 2022\n",
    "ultima_semana_2022 = dados['semana'].max()\n",
    "semanas_2023 = []\n",
    "for i in range(1, 6):  # 5 semanas\n",
    "    proxima_semana = ultima_semana_2022 + timedelta(weeks=i)\n",
    "    semanas_2023.append(proxima_semana)\n",
    "\n",
    "print(f'üìÖ Semanas para previs√£o:')\n",
    "for i, semana in enumerate(semanas_2023, 1):\n",
    "    print(f'   {i}. {semana.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "# Preparar dados hist√≥ricos para features iterativas\n",
    "dados_historicos = dados.copy()\n",
    "previsoes_finais = []\n",
    "\n",
    "print('\\nüîÑ Iniciando previs√µes iterativas...')\n",
    "for week_num, semana_target in enumerate(semanas_2023, 1):\n",
    "    print(f'\\n   Semana {week_num}/5: {semana_target.strftime(\"%Y-%m-%d\")}')\n",
    "    \n",
    "    # Criar features para a semana alvo\n",
    "    dados_semana = combinacoes_2022.copy()\n",
    "    dados_semana['semana'] = semana_target\n",
    "    \n",
    "    # Engenharia de features baseada no hist√≥rico atual\n",
    "    print('     üîß Criando features...')\n",
    "    \n",
    "    # Features temporais\n",
    "    dados_semana['mes_sin'] = np.sin(2 * np.pi * semana_target.month / 12)\n",
    "    dados_semana['mes_cos'] = np.cos(2 * np.pi * semana_target.month / 12)\n",
    "    \n",
    "    # Features de lag (baseadas no hist√≥rico)\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        semana_lag = semana_target - timedelta(weeks=lag)\n",
    "        dados_lag = dados_historicos[dados_historicos['semana'] == semana_lag]\n",
    "        \n",
    "        if len(dados_lag) > 0:\n",
    "            lag_dict = dados_lag.set_index(['pdv_id', 'produto_id'])['quantidade'].to_dict()\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = dados_semana.apply(\n",
    "                lambda row: lag_dict.get((row['pdv_id'], row['produto_id']), 0), axis=1\n",
    "            )\n",
    "        else:\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = 0\n",
    "    \n",
    "    # Features de rolling (√∫ltimas 4 semanas)\n",
    "    semanas_rolling = [semana_target - timedelta(weeks=i) for i in range(1, 5)]\n",
    "    dados_rolling = dados_historicos[dados_historicos['semana'].isin(semanas_rolling)]\n",
    "    \n",
    "    if len(dados_rolling) > 0:\n",
    "        rolling_stats = dados_rolling.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "            'mean': 'mean',\n",
    "            'max': 'max', \n",
    "            'min': 'min'\n",
    "        }).reset_index()\n",
    "        \n",
    "        rolling_stats.columns = ['pdv_id', 'produto_id', 'quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']\n",
    "        dados_semana = dados_semana.merge(rolling_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']:\n",
    "        if col in dados_semana.columns:\n",
    "            dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Features de hash e hist√≥ricas (copiar l√≥gica do notebook de feature engineering)\n",
    "    dados_semana['pdv_hash'] = dados_semana['pdv_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['produto_hash'] = dados_semana['produto_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['pdv_produto_hash'] = (dados_semana['pdv_id'].astype(str) + '_' + dados_semana['produto_id'].astype(str)).apply(hash) % 1000\n",
    "    \n",
    "    # Features hist√≥ricas (toda a s√©rie at√© agora)\n",
    "    hist_stats = dados_historicos.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "        'mean': 'mean',\n",
    "        'std': 'std',\n",
    "        'max': 'max',\n",
    "        'count': 'count'\n",
    "    }).reset_index()\n",
    "    hist_stats.columns = ['pdv_id', 'produto_id', 'hist_mean', 'hist_std', 'hist_max', 'hist_count']\n",
    "    dados_semana = dados_semana.merge(hist_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['hist_mean', 'hist_std', 'hist_max', 'hist_count']:\n",
    "        dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Adicionar distributor_id (assumir como -1 para novas predi√ß√µes)\n",
    "    dados_semana['distributor_id'] = -1\n",
    "    \n",
    "    # Garantir que todas as features necess√°rias existem\n",
    "    for feature in all_features:\n",
    "        if feature not in dados_semana.columns:\n",
    "            dados_semana[feature] = 0\n",
    "    \n",
    "    # Fazer previs√µes\n",
    "    print('     üéØ Fazendo previs√µes...')\n",
    "    X_pred = dados_semana[all_features]\n",
    "    \n",
    "    # Garantir tipos de dados consistentes\n",
    "    for col in X_pred.columns:\n",
    "        if X_pred[col].dtype != X_train[col].dtype:\n",
    "            X_pred[col] = X_pred[col].astype(X_train[col].dtype)\n",
    "    \n",
    "    predictions = model.predict(X_pred)\n",
    "    predictions = np.maximum(0, predictions)  # N√£o permitir valores negativos\n",
    "    \n",
    "    # Armazenar previs√µes\n",
    "    resultado_semana = dados_semana[['pdv_id', 'produto_id', 'semana']].copy()\n",
    "    resultado_semana['quantidade'] = predictions\n",
    "    previsoes_finais.append(resultado_semana)\n",
    "    \n",
    "    # Atualizar dados hist√≥ricos com as previs√µes\n",
    "    dados_historicos = pd.concat([dados_historicos, resultado_semana], ignore_index=True)\n",
    "    \n",
    "    print(f'     ‚úÖ {len(predictions):,} previs√µes geradas')\n",
    "\n",
    "print(f'\\n‚úÖ Previs√£o iterativa conclu√≠da!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gera√ß√£o dos Arquivos de Submiss√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üìã Preparando arquivos de submiss√£o...')\n",
    "\n",
    "# Consolidar todas as previs√µes\n",
    "submission_df = pd.concat(previsoes_finais, ignore_index=True)\n",
    "\n",
    "print(f'üìä Estat√≠sticas das previs√µes:')\n",
    "print(f'   ‚Ä¢ Total de registros: {len(submission_df):,}')\n",
    "print(f'   ‚Ä¢ Combina√ß√µes √∫nicas: {submission_df.groupby([\"pdv_id\", \"produto_id\"]).size().shape[0]:,}')\n",
    "print(f'   ‚Ä¢ Semanas: {submission_df[\"semana\"].nunique()}')\n",
    "print(f'   ‚Ä¢ Quantidade m√©dia: {submission_df[\"quantidade\"].mean():.4f}')\n",
    "print(f'   ‚Ä¢ Quantidade m√°xima: {submission_df[\"quantidade\"].max():.4f}')\n",
    "print(f'   ‚Ä¢ % zeros: {(submission_df[\"quantidade\"] == 0).mean()*100:.1f}%')\n",
    "\n",
    "# Criar diret√≥rio de submiss√£o\n",
    "os.makedirs('../submission', exist_ok=True)\n",
    "\n",
    "# Salvar em CSV\n",
    "csv_path = '../submission/submission.csv'\n",
    "submission_df.to_csv(csv_path, index=False)\n",
    "print(f'üíæ Arquivo CSV salvo: {csv_path}')\n",
    "\n",
    "# Salvar em Parquet\n",
    "parquet_path = '../submission/submission.parquet'\n",
    "submission_df.to_parquet(parquet_path, index=False)\n",
    "print(f'üíæ Arquivo Parquet salvo: {parquet_path}')\n",
    "\n",
    "# Verificar tamanhos dos arquivos\n",
    "csv_size = os.path.getsize(csv_path) / (1024**2)\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024**2)\n",
    "\n",
    "print(f'\\nüìè Tamanhos dos arquivos:')\n",
    "print(f'   ‚Ä¢ CSV: {csv_size:.1f} MB')\n",
    "print(f'   ‚Ä¢ Parquet: {parquet_size:.1f} MB ({parquet_size/csv_size*100:.1f}% do CSV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Valida√ß√£o e Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üîç Valida√ß√£o final dos dados...')\n",
    "\n",
    "# Verificar estrutura dos dados\n",
    "print(f'\\nüìã Estrutura final:')\n",
    "print(f'   ‚Ä¢ Colunas: {list(submission_df.columns)}')\n",
    "print(f'   ‚Ä¢ Tipos: {submission_df.dtypes.to_dict()}')\n",
    "print(f'   ‚Ä¢ Shape: {submission_df.shape}')\n",
    "\n",
    "# Verificar completude\n",
    "print(f'\\n‚úÖ Verifica√ß√µes:')\n",
    "print(f'   ‚Ä¢ Valores nulos: {submission_df.isnull().sum().sum()} (deve ser 0)')\n",
    "print(f'   ‚Ä¢ Valores negativos: {(submission_df[\"quantidade\"] < 0).sum()} (deve ser 0)')\n",
    "print(f'   ‚Ä¢ Semanas √∫nicas: {sorted(submission_df[\"semana\"].dt.strftime(\"%Y-%m-%d\").unique())}')\n",
    "\n",
    "# An√°lise por semana\n",
    "print(f'\\nüìä An√°lise por semana:')\n",
    "weekly_stats = submission_df.groupby('semana')['quantidade'].agg({\n",
    "    'count': 'count',\n",
    "    'mean': 'mean',\n",
    "    'sum': 'sum',\n",
    "    'zeros': lambda x: (x == 0).sum()\n",
    "}).round(4)\n",
    "\n",
    "for semana, stats in weekly_stats.iterrows():\n",
    "    zeros_pct = (stats['zeros'] / stats['count']) * 100\n",
    "    print(f'   ‚Ä¢ {semana.strftime(\"%Y-%m-%d\")}: {stats[\"count\"]:,} registros, m√©dia={stats[\"mean\"]:.4f}, zeros={zeros_pct:.1f}%')\n",
    "\n",
    "print(f'\\nüéâ PIPELINE FINAL CONCLU√çDO COM SUCESSO!')\n",
    "print('=' * 60)\n",
    "print(f'‚úÖ Modelo LightGBM treinado com {len(dados):,} registros de 2022')\n",
    "print(f'‚úÖ Previs√µes iterativas geradas para 5 semanas de 2023')\n",
    "print(f'‚úÖ {len(submission_df):,} previs√µes salvas em CSV e Parquet')\n",
    "print(f'‚úÖ Arquivos prontos para submiss√£o no hackathon!')\n",
    "\n",
    "print(f'\\nüìÅ Arquivos gerados:')\n",
    "print(f'   ‚Ä¢ {csv_path}')\n",
    "print(f'   ‚Ä¢ {parquet_path}')\n",
    "print(f'   ‚Ä¢ ../models/lightgbm_final.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}