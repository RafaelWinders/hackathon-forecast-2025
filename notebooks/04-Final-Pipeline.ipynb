{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final de Produção\n",
    "\n",
    "**🎯 PROPÓSITO:**  \n",
    "Este notebook implementa a solução final para gerar as predições de submissão do hackathon.\n",
    "\n",
    "**🚀 EXECUÇÃO:**  \n",
    "Execute todas as células em sequência. O arquivo `submission.csv` e `submission.parquet` serão gerados automaticamente.\n",
    "\n",
    "**📊 MODELO ESCOLHIDO:**  \n",
    "LightGBM - selecionado após rigorosa comparação no notebook `03-Modeling-Experiments.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Completo:\n",
    "1. **Carregar dados processados** (do notebook 02)\n",
    "2. **Otimizar memória** com downcasting\n",
    "3. **Treinar LightGBM** no dataset completo de 2022  \n",
    "4. **Implementar previsão iterativa** para as 5 semanas de 2023\n",
    "5. **Gerar submissão** nos formatos CSV e Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Bibliotecas carregadas com sucesso!\n",
      "🚀 Iniciando Pipeline Final de Produção\n"
     ]
    }
   ],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "print('📚 Bibliotecas carregadas com sucesso!')\n",
    "print('🚀 Iniciando Pipeline Final de Produção')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados processados...\n",
      "✅ Dados carregados: (51171190, 26)\n",
      "📅 Período: 2022-01-25 00:00:00 até 2022-12-27 00:00:00\n",
      "💾 Memória inicial: 16045.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Verificar arquivos necessários\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    raise FileNotFoundError(f'Arquivos não encontrados: {missing_files}')\n",
    "\n",
    "print('📂 Carregando dados processados...')\n",
    "dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "\n",
    "with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f'✅ Dados carregados: {dados.shape}')\n",
    "print(f'📅 Período: {dados[\"semana\"].min()} até {dados[\"semana\"].max()}')\n",
    "print(f'💾 Memória inicial: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimização de Memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Aplicando otimização de memória (downcasting)...\n",
      "✅ Memória otimizada: 4491.9 MB\n",
      "📈 Redução de memória: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print('🔧 Aplicando otimização de memória (downcasting)...')\n",
    "\n",
    "# Downcasting de tipos numéricos\n",
    "for col in dados.select_dtypes(include=[np.number]).columns:\n",
    "    if dados[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados[col] = pd.to_numeric(dados[col], downcast='float')\n",
    "\n",
    "# Otimizar categóricas\n",
    "for col in dados.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:\n",
    "        nunique = dados[col].nunique()\n",
    "        if nunique / len(dados) < 0.5:\n",
    "            dados[col] = dados[col].astype('category')\n",
    "\n",
    "# Tratar missing values\n",
    "if 'distributor_id' in dados.columns:\n",
    "    if dados['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados['distributor_id'].cat.categories:\n",
    "            dados['distributor_id'] = dados['distributor_id'].cat.add_categories([-1])\n",
    "    dados['distributor_id'] = dados['distributor_id'].fillna(-1)\n",
    "\n",
    "# Preencher outros NaNs\n",
    "for col in dados.columns:\n",
    "    if dados[col].isnull().sum() > 0 and dados[col].dtype.kind in ['i', 'u', 'f']:\n",
    "        dados[col] = dados[col].fillna(0)\n",
    "\n",
    "memory_optimized = dados.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f'✅ Memória otimizada: {memory_optimized:.1f} MB')\n",
    "print(f'📈 Redução de memória: {((memory_optimized / (dados.memory_usage(deep=True).sum() / (1024**2)) - 1) * 100):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparação para Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Configuração do modelo:\n",
      "   • Target: quantidade\n",
      "   • Features: 20\n",
      "📊 Dados de treinamento:\n",
      "   • Shape: (51171190, 20)\n",
      "   • Período completo de 2022\n"
     ]
    }
   ],
   "source": [
    "# Definir features e target\n",
    "target = 'quantidade'\n",
    "exclude_features = ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'🎯 Configuração do modelo:')\n",
    "print(f'   • Target: {target}')\n",
    "print(f'   • Features: {len(all_features)}')\n",
    "\n",
    "# Preparar dados para treinamento (todo o dataset de 2022)\n",
    "X_train = dados[all_features]\n",
    "y_train = dados[target]\n",
    "\n",
    "print(f'📊 Dados de treinamento:')\n",
    "print(f'   • Shape: {X_train.shape}')\n",
    "print(f'   • Período completo de 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do Modelo LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 PASSO B: Modelo LightGBM Vanilla (Parâmetros Default)\n",
      "============================================================\n",
      "🎯 Objetivo: Validar se nossas features têm poder preditivo\n",
      "\n",
      "📋 Configuração Vanilla:\n",
      "   • objective: regression_l1\n",
      "   • metric: mae\n",
      "   • boosting_type: gbdt\n",
      "   • verbosity: -1\n",
      "   • random_state: 42\n",
      "   • n_jobs: -1\n",
      "\n",
      "📊 Preparando dados para treinamento...\n",
      "   • Train shape: (51171190, 20)\n",
      "   • Features: 20\n",
      "\n",
      "🔄 Treinando LightGBM Vanilla (produção - dataset completo)...\n"
     ]
    }
   ],
   "source": [
    "# PASSO B: LightGBM Vanilla - Validar Pipeline e Features\n",
    "print('\\n🚀 PASSO B: Modelo LightGBM Vanilla (Parâmetros Default)')\n",
    "print('=' * 60)\n",
    "print('🎯 Objetivo: Validar se nossas features têm poder preditivo')\n",
    "\n",
    "# Configuração LightGBM Vanilla (parâmetros simples/default)\n",
    "lgb_params_vanilla = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print('\\n📋 Configuração Vanilla:')\n",
    "for param, value in lgb_params_vanilla.items():\n",
    "    print(f'   • {param}: {value}')\n",
    "\n",
    "# Preparar dados para LightGBM\n",
    "print('\\n📊 Preparando dados para treinamento...')\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "print(f'   • Train shape: {X_train.shape}')\n",
    "print(f'   • Features: {len(all_features)}')\n",
    "\n",
    "# Treinar modelo Vanilla (VERSÃO PARA PRODUÇÃO - SEM VALIDAÇÃO)\n",
    "print('\\n🔄 Treinando LightGBM Vanilla (produção - dataset completo)...')\n",
    "lgb_vanilla = lgb.train(\n",
    "    lgb_params_vanilla,\n",
    "    train_lgb,\n",
    "    num_boost_round=200  # Número moderado para vanilla\n",
    ")\n",
    "\n",
    "print(f'✅ Treinamento concluído em {lgb_vanilla.num_trees()} iterações')\n",
    "\n",
    "# Salvar modelo\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "lgb_vanilla.save_model('../models/lightgbm_final.txt')\n",
    "print('💾 Modelo salvo em: ../models/lightgbm_final.txt')\n",
    "\n",
    "print('\\n✅ LightGBM treinado e salvo com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementação da Previsão Iterativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('🔄 Implementando previsão iterativa para 2023...')\n",
    "\n",
    "# Obter combinações únicas de PDV x Produto de 2022\n",
    "combinacoes_2022 = dados[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "print(f'📊 Combinações PDV x Produto encontradas em 2022: {len(combinacoes_2022):,}')\n",
    "\n",
    "# Definir as 5 semanas de 2023 para previsão\n",
    "# Assumindo que 2023 começa na semana seguinte ao último dado de 2022\n",
    "ultima_semana_2022 = dados['semana'].max()\n",
    "semanas_2023 = []\n",
    "for i in range(1, 6):  # 5 semanas\n",
    "    proxima_semana = ultima_semana_2022 + timedelta(weeks=i)\n",
    "    semanas_2023.append(proxima_semana)\n",
    "\n",
    "print('📅 Semanas para previsão:')\n",
    "for i, semana in enumerate(semanas_2023, 1):\n",
    "    print(f'   {i}. {semana.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "# Preparar dados históricos para features iterativas\n",
    "dados_historicos = dados.copy()\n",
    "previsoes_finais = []\n",
    "\n",
    "print('\\\\n🔄 Iniciando previsões iterativas...')\n",
    "for week_num, semana_target in enumerate(semanas_2023, 1):\n",
    "    print(f'\\\\n   Semana {week_num}/5: {semana_target.strftime(\"%Y-%m-%d\")}')\n",
    "    \n",
    "    # Criar features para a semana alvo\n",
    "    dados_semana = combinacoes_2022.copy()\n",
    "    dados_semana['semana'] = semana_target\n",
    "    \n",
    "    # Engenharia de features baseada no histórico atual\n",
    "    print('     🔧 Criando features...')\n",
    "    \n",
    "    # Features temporais\n",
    "    dados_semana['mes_sin'] = np.sin(2 * np.pi * semana_target.month / 12)\n",
    "    dados_semana['mes_cos'] = np.cos(2 * np.pi * semana_target.month / 12)\n",
    "    \n",
    "    # Features de lag (baseadas no histórico)\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        semana_lag = semana_target - timedelta(weeks=lag)\n",
    "        dados_lag = dados_historicos[dados_historicos['semana'] == semana_lag]\n",
    "        \n",
    "        if len(dados_lag) > 0:\n",
    "            lag_dict = dados_lag.set_index(['pdv_id', 'produto_id'])['quantidade'].to_dict()\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = dados_semana.apply(\n",
    "                lambda row: lag_dict.get((row['pdv_id'], row['produto_id']), 0), axis=1\n",
    "            )\n",
    "        else:\n",
    "            dados_semana[f'quantidade_lag_{lag}'] = 0\n",
    "    \n",
    "    # Features de rolling (últimas 4 semanas)\n",
    "    semanas_rolling = [semana_target - timedelta(weeks=i) for i in range(1, 5)]\n",
    "    dados_rolling = dados_historicos[dados_historicos['semana'].isin(semanas_rolling)]\n",
    "    \n",
    "    if len(dados_rolling) > 0:\n",
    "        rolling_stats = dados_rolling.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "            'mean': 'mean',\n",
    "            'max': 'max', \n",
    "            'min': 'min'\n",
    "        }).reset_index()\n",
    "        \n",
    "        rolling_stats.columns = ['pdv_id', 'produto_id', 'quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']\n",
    "        dados_semana = dados_semana.merge(rolling_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['quantidade_media_4w', 'quantidade_max_4w', 'quantidade_min_4w']:\n",
    "        if col in dados_semana.columns:\n",
    "            dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Features de hash e históricas (copiar lógica do notebook de feature engineering)\n",
    "    dados_semana['pdv_hash'] = dados_semana['pdv_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['produto_hash'] = dados_semana['produto_id'].astype(str).apply(hash) % 100\n",
    "    dados_semana['pdv_produto_hash'] = (dados_semana['pdv_id'].astype(str) + '_' + dados_semana['produto_id'].astype(str)).apply(hash) % 1000\n",
    "    \n",
    "    # Features históricas (toda a série até agora)\n",
    "    hist_stats = dados_historicos.groupby(['pdv_id', 'produto_id'])['quantidade'].agg({\n",
    "        'mean': 'mean',\n",
    "        'std': 'std',\n",
    "        'max': 'max',\n",
    "        'count': 'count'\n",
    "    }).reset_index()\n",
    "    hist_stats.columns = ['pdv_id', 'produto_id', 'hist_mean', 'hist_std', 'hist_max', 'hist_count']\n",
    "    dados_semana = dados_semana.merge(hist_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "    \n",
    "    # Preencher NaNs\n",
    "    for col in ['hist_mean', 'hist_std', 'hist_max', 'hist_count']:\n",
    "        dados_semana[col] = dados_semana[col].fillna(0)\n",
    "    \n",
    "    # Adicionar distributor_id (assumir como -1 para novas predições)\n",
    "    dados_semana['distributor_id'] = -1\n",
    "    \n",
    "    # Garantir que todas as features necessárias existem\n",
    "    for feature in all_features:\n",
    "        if feature not in dados_semana.columns:\n",
    "            dados_semana[feature] = 0\n",
    "    \n",
    "    # Fazer previsões\n",
    "    print('     🎯 Fazendo previsões...')\n",
    "    X_pred = dados_semana[all_features]\n",
    "    \n",
    "    # Garantir tipos de dados consistentes\n",
    "    for col in X_pred.columns:\n",
    "        if X_pred[col].dtype != X_train[col].dtype:\n",
    "            X_pred[col] = X_pred[col].astype(X_train[col].dtype)\n",
    "    \n",
    "    # CORREÇÃO: Usar lgb_vanilla em vez de model\n",
    "    predictions = lgb_vanilla.predict(X_pred)\n",
    "    predictions = np.maximum(0, predictions)  # Não permitir valores negativos\n",
    "    \n",
    "    # Armazenar previsões\n",
    "    resultado_semana = dados_semana[['pdv_id', 'produto_id', 'semana']].copy()\n",
    "    resultado_semana['quantidade'] = predictions\n",
    "    previsoes_finais.append(resultado_semana)\n",
    "    \n",
    "    # Atualizar dados históricos com as previsões\n",
    "    dados_historicos = pd.concat([dados_historicos, resultado_semana], ignore_index=True)\n",
    "    \n",
    "    print(f'     ✅ {len(predictions):,} previsões geradas')\n",
    "\n",
    "print('\\\\n✅ Previsão iterativa concluída!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geração dos Arquivos de Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('📋 Preparando arquivos de submissão...')\n",
    "\n",
    "# Consolidar todas as previsões\n",
    "submission_df = pd.concat(previsoes_finais, ignore_index=True)\n",
    "\n",
    "print(f'📊 Estatísticas das previsões:')\n",
    "print(f'   • Total de registros: {len(submission_df):,}')\n",
    "print(f'   • Combinações únicas: {submission_df.groupby([\"pdv_id\", \"produto_id\"]).size().shape[0]:,}')\n",
    "print(f'   • Semanas: {submission_df[\"semana\"].nunique()}')\n",
    "print(f'   • Quantidade média: {submission_df[\"quantidade\"].mean():.4f}')\n",
    "print(f'   • Quantidade máxima: {submission_df[\"quantidade\"].max():.4f}')\n",
    "print(f'   • % zeros: {(submission_df[\"quantidade\"] == 0).mean()*100:.1f}%')\n",
    "\n",
    "# Criar diretório de submissão\n",
    "os.makedirs('../submission', exist_ok=True)\n",
    "\n",
    "# Salvar em CSV\n",
    "csv_path = '../submission/submission.csv'\n",
    "submission_df.to_csv(csv_path, index=False)\n",
    "print(f'💾 Arquivo CSV salvo: {csv_path}')\n",
    "\n",
    "# Salvar em Parquet\n",
    "parquet_path = '../submission/submission.parquet'\n",
    "submission_df.to_parquet(parquet_path, index=False)\n",
    "print(f'💾 Arquivo Parquet salvo: {parquet_path}')\n",
    "\n",
    "# Verificar tamanhos dos arquivos\n",
    "csv_size = os.path.getsize(csv_path) / (1024**2)\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024**2)\n",
    "\n",
    "print(f'\\n📏 Tamanhos dos arquivos:')\n",
    "print(f'   • CSV: {csv_size:.1f} MB')\n",
    "print(f'   • Parquet: {parquet_size:.1f} MB ({parquet_size/csv_size*100:.1f}% do CSV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validação e Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('🔍 Validação final dos dados...')\n",
    "\n",
    "# Verificar estrutura dos dados\n",
    "print(f'\\n📋 Estrutura final:')\n",
    "print(f'   • Colunas: {list(submission_df.columns)}')\n",
    "print(f'   • Tipos: {submission_df.dtypes.to_dict()}')\n",
    "print(f'   • Shape: {submission_df.shape}')\n",
    "\n",
    "# Verificar completude\n",
    "print(f'\\n✅ Verificações:')\n",
    "print(f'   • Valores nulos: {submission_df.isnull().sum().sum()} (deve ser 0)')\n",
    "print(f'   • Valores negativos: {(submission_df[\"quantidade\"] < 0).sum()} (deve ser 0)')\n",
    "print(f'   • Semanas únicas: {sorted(submission_df[\"semana\"].dt.strftime(\"%Y-%m-%d\").unique())}')\n",
    "\n",
    "# Análise por semana\n",
    "print(f'\\n📊 Análise por semana:')\n",
    "weekly_stats = submission_df.groupby('semana')['quantidade'].agg({\n",
    "    'count': 'count',\n",
    "    'mean': 'mean',\n",
    "    'sum': 'sum',\n",
    "    'zeros': lambda x: (x == 0).sum()\n",
    "}).round(4)\n",
    "\n",
    "for semana, stats in weekly_stats.iterrows():\n",
    "    zeros_pct = (stats['zeros'] / stats['count']) * 100\n",
    "    print(f'   • {semana.strftime(\"%Y-%m-%d\")}: {stats[\"count\"]:,} registros, média={stats[\"mean\"]:.4f}, zeros={zeros_pct:.1f}%')\n",
    "\n",
    "print(f'\\n🎉 PIPELINE FINAL CONCLUÍDO COM SUCESSO!')\n",
    "print('=' * 60)\n",
    "print(f'✅ Modelo LightGBM treinado com {len(dados):,} registros de 2022')\n",
    "print(f'✅ Previsões iterativas geradas para 5 semanas de 2023')\n",
    "print(f'✅ {len(submission_df):,} previsões salvas em CSV e Parquet')\n",
    "print(f'✅ Arquivos prontos para submissão no hackathon!')\n",
    "\n",
    "print(f'\\n📁 Arquivos gerados:')\n",
    "print(f'   • {csv_path}')\n",
    "print(f'   • {parquet_path}')\n",
    "print(f'   • ../models/lightgbm_final.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
