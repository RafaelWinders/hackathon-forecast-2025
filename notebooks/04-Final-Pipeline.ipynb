{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final e Geração da Submissão\n",
    "\n",
    "**🎯 PROPÓSITO DESTE NOTEBOOK:**\n",
    "Este notebook contém o pipeline final para o Hackathon Forecast Big Data 2025. O processo consiste em:\n",
    "\n",
    "1. **Carregamento dos Dados Brutos:** Carregar os dados de 2022 (transações, produtos, pdvs)\n",
    "2. **Engenharia de Features:** Função completa para processar os dados e criar features\n",
    "3. **Treinamento do Modelo Final:** Treinar o LightGBM com 100% dos dados de 2022\n",
    "4. **Geração do Grid de Previsão:** Criar o dataframe base para as 5 semanas de Janeiro/2023\n",
    "5. **Previsão e Submissão:** Gerar previsões e salvar arquivos CSV e Parquet\n",
    "\n",
    "**🚀 PIPELINE COMPLETO:**\n",
    "Este notebook executa todo o processo de ponta a ponta, da engenharia de features até a geração da submissão final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Bibliotecas carregadas com sucesso!\n",
      "🎯 Iniciando Pipeline Final para Submissão\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Use o Polars para a engenharia de features, como no notebook 02\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('📚 Bibliotecas carregadas com sucesso!')\n",
    "print('🎯 Iniciando Pipeline Final para Submissão')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def engenharia_de_features(df_transacoes, df_produtos, df_pdvs):\n    \"\"\"\n    Aplica a engenharia de features completa usando Polars para eficiência.\n    Recebe os dataframes brutos e retorna um dataframe pandas com as features.\n    \n    Esta função implementa EXATAMENTE a mesma lógica do notebook 02-Feature-Engineering-Dask.ipynb\n    \"\"\"\n    print(\"🔧 Iniciando engenharia de features com Polars...\")\n    \n    # Converter para Polars para performance\n    transacoes_pl = pl.from_pandas(df_transacoes)\n    produtos_pl = pl.from_pandas(df_produtos)\n    pdvs_pl = pl.from_pandas(df_pdvs)\n    \n    print(f\"   • Transações: {len(transacoes_pl):,} registros\")\n    print(f\"   • Produtos: {len(produtos_pl):,} registros\")\n    print(f\"   • PDVs: {len(pdvs_pl):,} registros\")\n    \n    # === INÍCIO DA LÓGICA ADAPTADA DO NOTEBOOK 02 ===\n    \n    # 1. Renomear colunas para consistência (CORREÇÃO BASEADA NOS DADOS REAIS)\n    print(\"📝 Renomeando colunas para consistência...\")\n    \n    # Transações: renomear para nomes padronizados\n    transacoes_pl = transacoes_pl.rename({\n        'internal_product_id': 'produto_id',\n        'internal_store_id': 'pdv_id',\n        'reference_date': 'semana',\n        'quantity': 'quantidade'\n    })\n    \n    # Produtos: usar as colunas que existem realmente\n    produtos_pl = produtos_pl.rename({\n        'produto': 'produto_id'  # A coluna chave é 'produto', não 'internal_product_id'\n    })\n    \n    # PDVs: usar as colunas que existem realmente  \n    pdvs_pl = pdvs_pl.rename({\n        'pdv': 'pdv_id'  # A coluna chave é 'pdv', não 'internal_store_id'\n    })\n    \n    print(\"✅ Colunas renomeadas com sucesso!\")\n    \n    # 2. Joins dos dados (agora vai funcionar)\n    print(\"🔗 Fazendo joins dos dados...\")\n    dados = transacoes_pl.join(produtos_pl, on='produto_id', how='left').join(pdvs_pl, on='pdv_id', how='left')\n    \n    # 3. Conversão de data e ordenação\n    print(\"📅 Processando datas...\")\n    dados = dados.with_columns(pl.col(\"semana\").str.to_datetime(\"%Y-%m-%d\"))\n    dados = dados.sort([\"pdv_id\", \"produto_id\", \"semana\"])\n    \n    # 4. Features temporais (mês, semana do ano, sin/cos para sazonalidade)\n    print(\"🕒 Criando features temporais...\")\n    dados = dados.with_columns([\n        pl.col(\"semana\").dt.month().alias(\"mes\"),\n        pl.col(\"semana\").dt.week().alias(\"semana_ano\")\n    ])\n    \n    # Features cíclicas para capturar sazonalidade\n    dados = dados.with_columns([\n        (pl.col(\"mes\") * (2 * np.pi / 12)).sin().alias(\"mes_sin\"),\n        (pl.col(\"mes\") * (2 * np.pi / 12)).cos().alias(\"mes_cos\")\n    ])\n    \n    # 5. Features de Lag (valores passados)\n    print(\"⏪ Criando features de lag...\")\n    lags = [1, 2, 3, 4]\n    for lag in lags:\n        dados = dados.with_columns(\n            pl.col(\"quantidade\").shift(lag).over([\"pdv_id\", \"produto_id\"]).alias(f\"quantidade_lag_{lag}\")\n        )\n    \n    # 6. Features de Rolling Window (médias móveis, etc.)\n    print(\"📊 Criando features de rolling window...\")\n    dados = dados.with_columns([\n        pl.col(\"quantidade\").rolling_mean(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_media_4w\"),\n        pl.col(\"quantidade\").rolling_max(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_max_4w\"),\n        pl.col(\"quantidade\").rolling_min(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_min_4w\")\n    ])\n    \n    # 7. Features de Hash para reduzir dimensionalidade\n    print(\"🔢 Criando features de hash...\")\n    dados = dados.with_columns([\n        pl.col(\"pdv_id\").hash().cast(pl.Int8).alias(\"pdv_hash\"),\n        pl.col(\"produto_id\").hash().cast(pl.Int8).alias(\"produto_hash\"),\n        (pl.col(\"pdv_id\").cast(str) + \"_\" + pl.col(\"produto_id\").cast(str)).hash().cast(pl.Int16).alias(\"pdv_produto_hash\")\n    ])\n    \n    # 8. Features históricas por combinação PDV/Produto\n    print(\"📈 Criando features históricas...\")\n    dados = dados.with_columns([\n        pl.col(\"quantidade\").mean().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_mean\"),\n        pl.col(\"quantidade\").std().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_std\"),\n        pl.col(\"quantidade\").max().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_max\"),\n        pl.col(\"quantidade\").count().over([\"pdv_id\", \"produto_id\"]).cast(pl.Int8).alias(\"hist_count\")\n    ])\n    \n    # 9. Preencher NaNs que surgiram dos lags/rolling\n    print(\"🔧 Preenchendo valores missing...\")\n    dados = dados.fill_null(0)\n    \n    # === FIM DA LÓGICA ADAPTADA DO NOTEBOOK 02 ===\n    \n    print(\"✅ Engenharia de features concluída!\")\n    \n    # Converter de volta para pandas\n    df_final = dados.to_pandas()\n    print(f\"   • Shape final: {df_final.shape}\")\n    print(f\"   • Features criadas: {len(df_final.columns)}\")\n    \n    return df_final\n\nprint(\"🛠️ Função de engenharia de features definida com sucesso!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados brutos de 2022...\n",
      "   • Transações: (6560698, 11)\n",
      "   • Produtos: (7092, 8)\n",
      "   • PDVs: (14419, 4)\n",
      "\n",
      "🔧 Aplicando engenharia de features...\n",
      "🔧 Iniciando engenharia de features com Polars...\n",
      "   • Transações: 6,560,698 registros\n",
      "   • Produtos: 7,092 registros\n",
      "   • PDVs: 14,419 registros\n",
      "📝 Renomeando colunas para consistência...\n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "\"internal_product_id\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mColumnNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Aplicar engenharia de features usando a função criada\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔧 Aplicando engenharia de features...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m dados_treino_com_features = \u001b[43mengenharia_de_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_transacoes_2022\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_produtos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_pdvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m📊 Dados com features processados:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m   • Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdados_treino_com_features.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mengenharia_de_features\u001b[39m\u001b[34m(df_transacoes, df_produtos, df_pdvs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📝 Renomeando colunas para consistência...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m transacoes_pl = transacoes_pl.rename({\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minternal_product_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mproduto_id\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minternal_store_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mpdv_id\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreference_date\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msemana\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     27\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m produtos_pl = \u001b[43mprodutos_pl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minternal_product_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduto_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m pdvs_pl = pdvs_pl.rename({\u001b[33m'\u001b[39m\u001b[33minternal_store_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mpdv_id\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 2. Joins dos dados (agora vai funcionar)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\dataframe\\frame.py:5026\u001b[39m, in \u001b[36mDataFrame.rename\u001b[39m\u001b[34m(self, mapping, strict)\u001b[39m\n\u001b[32m   4980\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4981\u001b[39m \u001b[33;03mRename column names.\u001b[39;00m\n\u001b[32m   4982\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5019\u001b[39m \u001b[33;03m└─────┴─────┴─────┘\u001b[39;00m\n\u001b[32m   5020\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5021\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m   5023\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   5024\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m5026\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5027\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2407\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2405\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2406\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mColumnNotFoundError\u001b[39m: \"internal_product_id\" not found"
     ]
    }
   ],
   "source": [
    "# Carregar dados brutos de 2022 diretamente\n",
    "print(\"📂 Carregando dados brutos de 2022...\")\n",
    "\n",
    "# Carregar os dados brutos dos arquivos parquet originais\n",
    "df_transacoes_2022 = pd.read_parquet('../data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet')\n",
    "df_produtos = pd.read_parquet('../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet')\n",
    "df_pdvs = pd.read_parquet('../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet')\n",
    "\n",
    "print(f\"   • Transações: {df_transacoes_2022.shape}\")\n",
    "print(f\"   • Produtos: {df_produtos.shape}\") \n",
    "print(f\"   • PDVs: {df_pdvs.shape}\")\n",
    "\n",
    "# Aplicar engenharia de features usando a função criada\n",
    "print('\\n🔧 Aplicando engenharia de features...')\n",
    "dados_treino_com_features = engenharia_de_features(df_transacoes_2022, df_produtos, df_pdvs)\n",
    "\n",
    "print(f'\\n📊 Dados com features processados:')\n",
    "print(f'   • Shape: {dados_treino_com_features.shape}')\n",
    "print(f'   • Período: {dados_treino_com_features[\"semana\"].min()} até {dados_treino_com_features[\"semana\"].max()}')\n",
    "\n",
    "# Aplicar otimização de memória (downcasting) como no seu trabalho anterior\n",
    "print('\\n📊 Otimização de memória e tratamento de missing values...')\n",
    "\n",
    "# Downcasting para otimizar memória\n",
    "print('🔽 Aplicando downcasting...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_treino_com_features[col].dtype\n",
    "    if dados_treino_com_features[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='float')\n",
    "\n",
    "# Otimizar colunas categóricas\n",
    "print('📂 Otimizando categóricas...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_treino_com_features[col].nunique()\n",
    "        total_rows = len(dados_treino_com_features)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores únicos, usar category\n",
    "            dados_treino_com_features[col] = dados_treino_com_features[col].astype('category')\n",
    "\n",
    "# Tratamento inteligente de missing values para distributor_id\n",
    "if 'distributor_id' in dados_treino_com_features.columns:\n",
    "    print('🔧 Tratando missing values em distributor_id...')\n",
    "    if dados_treino_com_features['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados_treino_com_features['distributor_id'].cat.categories:\n",
    "            dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].cat.add_categories([-1])\n",
    "    dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].fillna(-1)\n",
    "\n",
    "print('✅ Otimização concluída!')\n",
    "\n",
    "# Definir features e target\n",
    "target = 'quantidade'\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana', 'quantidade',  # IDs e target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informação do futuro (se existirem)\n",
    "    'mes', 'semana_ano'  # Features temporais brutas (mantemos sin/cos)\n",
    "]\n",
    "\n",
    "all_features = [col for col in dados_treino_com_features.columns if col not in exclude_features]\n",
    "\n",
    "print(f'\\n🎯 Preparando dados para treinamento:')\n",
    "print(f'   • Target: {target}')\n",
    "print(f'   • Features disponíveis: {len(all_features)}')\n",
    "print(f'   • Features excluídas: {len(exclude_features)}')\n",
    "\n",
    "X_full = dados_treino_com_features[all_features]\n",
    "y_full = dados_treino_com_features[target]\n",
    "\n",
    "print(f'   • X_full shape: {X_full.shape}')\n",
    "print(f'   • y_full shape: {y_full.shape}')\n",
    "\n",
    "# Treinamento do modelo final\n",
    "print('\\n🚀 Treinando o modelo LightGBM final com todos os dados de 2022...')\n",
    "\n",
    "lgb_params_final = {\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "best_iteration = 200  # Usar a melhor iteração da validação anterior\n",
    "\n",
    "train_full_lgb = lgb.Dataset(X_full, label=y_full, free_raw_data=False)  # free_raw_data=False para reutilização\n",
    "final_model = lgb.train(lgb_params_final, train_full_lgb, num_boost_round=best_iteration)\n",
    "\n",
    "print(f'✅ Modelo final treinado com sucesso em {best_iteration} iterações!')\n",
    "print(f'   • Features utilizadas: {len(all_features)}')\n",
    "print(f'   • Lista de features: {all_features[:10]}...')  # Mostrar primeiras 10\n",
    "\n",
    "# Limpeza parcial de memória (manter dados necessários para teste)\n",
    "print('\\n🧹 Limpeza de memória...')\n",
    "del train_full_lgb\n",
    "gc.collect()\n",
    "\n",
    "print('🎉 Pipeline de treinamento concluído com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('📅 Preparando dados para as previsões de Janeiro/2023...')\n",
    "\n",
    "# 1. Criar o Grid de Teste para as 5 semanas de Janeiro/2023\n",
    "print('🎯 Criando grid de teste para Janeiro/2023...')\n",
    "\n",
    "# CORREÇÃO: Usar os nomes originais das colunas dos dados brutos\n",
    "unique_combinations = df_transacoes_2022[['internal_store_id', 'internal_product_id']].drop_duplicates()\n",
    "unique_combinations.rename(columns={'internal_store_id': 'pdv_id', 'internal_product_id': 'produto_id'}, inplace=True)\n",
    "\n",
    "semanas_2023 = pd.DataFrame({'semana_int': range(1, 6)})\n",
    "df_teste_base = unique_combinations.merge(semanas_2023, how='cross')\n",
    "\n",
    "# Converter semana_int para formato de data (começando em 2023-01-03, primeira terça de janeiro)\n",
    "df_teste_base['semana'] = df_teste_base['semana_int'].apply(\n",
    "    lambda s: pd.to_datetime('2023-01-03') + pd.to_timedelta(s-1, unit='W')\n",
    ")\n",
    "\n",
    "# Adicionar quantidade = 0 como placeholder (necessário para a função de features)\n",
    "df_teste_base['quantidade'] = 0\n",
    "\n",
    "print(f'   • Grid de teste: {df_teste_base.shape}')\n",
    "print(f'   • Combinações únicas: {len(unique_combinations):,}')\n",
    "print(f'   • Semanas: {sorted(df_teste_base[\"semana_int\"].unique())}')\n",
    "print(f'   • Período: {df_teste_base[\"semana\"].min()} a {df_teste_base[\"semana\"].max()}')\n",
    "\n",
    "# 2. ESTRATÉGIA CRUCIAL: Combinar histórico + grid de teste para features contínuas\n",
    "print('\\n🔧 Combinando dados históricos com grid de teste...')\n",
    "print('   💡 Estratégia: Esta combinação permite calcular lags e médias móveis corretas')\n",
    "\n",
    "# Preparar dados históricos com nomes de colunas consistentes\n",
    "df_historico = df_transacoes_2022[['reference_date', 'internal_store_id', 'internal_product_id', 'quantity']].copy()\n",
    "df_historico.rename(columns={\n",
    "    'reference_date': 'semana',\n",
    "    'internal_store_id': 'pdv_id', \n",
    "    'internal_product_id': 'produto_id',\n",
    "    'quantity': 'quantidade'\n",
    "}, inplace=True)\n",
    "\n",
    "# Garantir que as colunas sejam consistentes entre histórico e teste\n",
    "colunas_necessarias = ['semana', 'pdv_id', 'produto_id', 'quantidade']\n",
    "df_teste_grid = df_teste_base[colunas_necessarias].copy()\n",
    "\n",
    "# Combinar dados históricos de 2022 + grid de teste de 2023\n",
    "dados_para_features_teste = pd.concat([df_historico, df_teste_grid], ignore_index=True)\n",
    "dados_para_features_teste = dados_para_features_teste.sort_values(['pdv_id', 'produto_id', 'semana']).reset_index(drop=True)\n",
    "\n",
    "print(f'   • Dados históricos: {len(df_historico):,}')\n",
    "print(f'   • Grid de teste: {len(df_teste_grid):,}')\n",
    "print(f'   • Dados combinados: {dados_para_features_teste.shape}')\n",
    "\n",
    "# 3. Reutilizar a função de engenharia de features!\n",
    "print('\\n🛠️ Aplicando engenharia de features aos dados combinados...')\n",
    "dados_teste_com_features = engenharia_de_features(dados_para_features_teste, df_produtos, df_pdvs)\n",
    "\n",
    "print(f'   • Dados com features: {dados_teste_com_features.shape}')\n",
    "\n",
    "# 4. Filtrar apenas as semanas de 2023 para a previsão\n",
    "print('\\n🎯 Filtrando dados de teste (Janeiro/2023)...')\n",
    "mask_2023 = dados_teste_com_features['semana'].dt.year == 2023\n",
    "X_teste = dados_teste_com_features[mask_2023].copy()\n",
    "\n",
    "print(f'   • Dados de teste filtrados: {X_teste.shape}')\n",
    "print(f'   • Verificação: {X_teste[\"semana\"].min()} a {X_teste[\"semana\"].max()}')\n",
    "\n",
    "# 5. Aplicar mesma otimização de memória e tratamento de missing no X_teste\n",
    "print('\\n📊 Aplicando otimização aos dados de teste...')\n",
    "\n",
    "# Downcasting\n",
    "print('🔽 Downcasting dados de teste...')\n",
    "for col in X_teste.select_dtypes(include=[np.number]).columns:\n",
    "    if X_teste[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        X_teste[col] = pd.to_numeric(X_teste[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        X_teste[col] = pd.to_numeric(X_teste[col], downcast='float')\n",
    "\n",
    "# Otimizar categóricas\n",
    "print('📂 Otimizando categóricas teste...')\n",
    "for col in X_teste.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:\n",
    "        nunique = X_teste[col].nunique()\n",
    "        total_rows = len(X_teste)\n",
    "        if nunique / total_rows < 0.5:\n",
    "            X_teste[col] = X_teste[col].astype('category')\n",
    "\n",
    "# Tratamento de missing values\n",
    "if 'distributor_id' in X_teste.columns:\n",
    "    print('🔧 Tratando missing values em distributor_id (teste)...')\n",
    "    if X_teste['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in X_teste['distributor_id'].cat.categories:\n",
    "            X_teste['distributor_id'] = X_teste['distributor_id'].cat.add_categories([-1])\n",
    "    X_teste['distributor_id'] = X_teste['distributor_id'].fillna(-1)\n",
    "\n",
    "print('✅ Otimização dos dados de teste concluída!')\n",
    "\n",
    "# 6. Preparar features para previsão (garantir mesma ordem do treino)\n",
    "print('\\n🔮 Preparando features para previsão...')\n",
    "features_teste = X_teste[all_features]\n",
    "\n",
    "print(f'   • Shape das features de teste: {features_teste.shape}')\n",
    "print(f'   • Features esperadas: {len(all_features)}')\n",
    "print(f'   • Verificação de consistência: {list(features_teste.columns) == all_features}')\n",
    "\n",
    "# 7. FAZER A PREVISÃO REAL (não mais placeholder!)\n",
    "print('\\n🚀 Gerando previsões REAIS com o modelo treinado...')\n",
    "predictions = final_model.predict(features_teste)\n",
    "predictions = np.maximum(0, predictions)  # Não permitir previsões negativas\n",
    "\n",
    "print(f'   • Previsões geradas: {len(predictions):,}')\n",
    "print(f'   • Estatísticas das previsões:')\n",
    "print(f'     - Mínimo: {predictions.min():.2f}')\n",
    "print(f'     - Máximo: {predictions.max():.2f}')\n",
    "print(f'     - Média: {predictions.mean():.2f}')\n",
    "print(f'     - Mediana: {np.median(predictions):.2f}')\n",
    "print(f'     - Zeros: {(predictions == 0).sum():,} ({(predictions == 0).mean()*100:.1f}%)')\n",
    "\n",
    "# 8. Montar o arquivo de submissão\n",
    "print('\\n📋 Montando arquivo de submissão...')\n",
    "df_submissao = X_teste[['semana', 'pdv_id', 'produto_id']].copy()\n",
    "\n",
    "# Mapear a semana datetime de volta para o número da semana (1-5)\n",
    "week_map = {d: i+1 for i, d in enumerate(sorted(df_submissao['semana'].unique()))}\n",
    "df_submissao['semana'] = df_submissao['semana'].map(week_map)\n",
    "\n",
    "# Adicionar previsões (arredondadas para inteiros)\n",
    "df_submissao['quantidade'] = predictions.round().astype(int)\n",
    "\n",
    "# Renomear colunas para formato de submissão\n",
    "df_submissao.rename(columns={'pdv_id': 'pdv', 'produto_id': 'produto'}, inplace=True)\n",
    "\n",
    "print(f'   • Formato final: {df_submissao.shape}')\n",
    "print(f'   • Colunas: {list(df_submissao.columns)}')\n",
    "\n",
    "# 9. Salvar arquivos de submissão\n",
    "print('\\n💾 Salvando arquivos de submissão...')\n",
    "os.makedirs('../submissions', exist_ok=True)\n",
    "\n",
    "# Salvar em Parquet\n",
    "caminho_parquet = '../submissions/previsao_final.parquet'\n",
    "df_submissao.to_parquet(caminho_parquet, index=False)\n",
    "print(f'✅ Arquivo Parquet salvo: {caminho_parquet}')\n",
    "\n",
    "# Salvar em CSV\n",
    "caminho_csv = '../submissions/previsao_final.csv'\n",
    "df_submissao.to_csv(caminho_csv, index=False, sep=';', encoding='utf-8')\n",
    "print(f'✅ Arquivo CSV salvo: {caminho_csv}')\n",
    "\n",
    "# 10. Mostrar amostra e estatísticas finais\n",
    "print('\\n📋 Amostra da submissão final:')\n",
    "print(df_submissao.head(15))\n",
    "\n",
    "print('\\n🎉 PIPELINE COMPLETO - SUBMISSÃO GERADA COM SUCESSO!')\n",
    "print('=' * 70)\n",
    "print(f'📊 Estatísticas finais da submissão:')\n",
    "print(f'   • Total de previsões: {len(df_submissao):,}')\n",
    "print(f'   • Semanas cobertas: {sorted(df_submissao[\"semana\"].unique())}')\n",
    "print(f'   • Combinações únicas (PDV×Produto): {df_submissao.groupby([\"pdv\", \"produto\"]).size().count():,}')\n",
    "print(f'   • Estatísticas das quantidades previstas:')\n",
    "print(f'     - Total previsto: {df_submissao[\"quantidade\"].sum():,}')\n",
    "print(f'     - Média por previsão: {df_submissao[\"quantidade\"].mean():.2f}')\n",
    "print(f'     - Previsões não-zero: {(df_submissao[\"quantidade\"] > 0).sum():,} ({(df_submissao[\"quantidade\"] > 0).mean()*100:.1f}%)')\n",
    "\n",
    "print(f'\\n📁 Arquivos de submissão gerados:')\n",
    "print(f'   • CSV: {caminho_csv}')\n",
    "print(f'   • Parquet: {caminho_parquet}')\n",
    "\n",
    "print('\\n🏆 PIPELINE PONTA A PONTA CONCLUÍDO!')\n",
    "print('   ✅ Dados brutos → Features → Modelo → Previsões → Submissão')\n",
    "print('   ✅ Nenhuma dependência externa ou arquivo pré-processado')\n",
    "print('   ✅ Pronto para avaliação!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}