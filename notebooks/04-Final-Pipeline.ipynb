{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final e Gera√ß√£o da Submiss√£o\n",
    "\n",
    "**üéØ PROP√ìSITO DESTE NOTEBOOK:**\n",
    "Este notebook cont√©m o pipeline final para o Hackathon Forecast Big Data 2025. O processo consiste em:\n",
    "\n",
    "1. **Carregamento dos Dados Brutos:** Carregar os dados de 2022 (transa√ß√µes, produtos, pdvs)\n",
    "2. **Engenharia de Features:** Fun√ß√£o completa para processar os dados e criar features\n",
    "3. **Treinamento do Modelo Final:** Treinar o LightGBM com 100% dos dados de 2022\n",
    "4. **Gera√ß√£o do Grid de Previs√£o:** Criar o dataframe base para as 5 semanas de Janeiro/2023\n",
    "5. **Previs√£o e Submiss√£o:** Gerar previs√µes e salvar arquivos CSV e Parquet\n",
    "\n",
    "**üöÄ PIPELINE COMPLETO:**\n",
    "Este notebook executa todo o processo de ponta a ponta, da engenharia de features at√© a gera√ß√£o da submiss√£o final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Bibliotecas carregadas com sucesso!\n",
      "üéØ Iniciando Pipeline Final para Submiss√£o\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Use o Polars para a engenharia de features, como no notebook 02\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('üìö Bibliotecas carregadas com sucesso!')\n",
    "print('üéØ Iniciando Pipeline Final para Submiss√£o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def engenharia_de_features(df_transacoes, df_produtos, df_pdvs):\n    \"\"\"\n    Aplica a engenharia de features completa usando Polars para efici√™ncia.\n    Recebe os dataframes brutos e retorna um dataframe pandas com as features.\n    \n    Esta fun√ß√£o implementa EXATAMENTE a mesma l√≥gica do notebook 02-Feature-Engineering-Dask.ipynb\n    \"\"\"\n    print(\"üîß Iniciando engenharia de features com Polars...\")\n    \n    # Converter para Polars para performance\n    transacoes_pl = pl.from_pandas(df_transacoes)\n    produtos_pl = pl.from_pandas(df_produtos)\n    pdvs_pl = pl.from_pandas(df_pdvs)\n    \n    print(f\"   ‚Ä¢ Transa√ß√µes: {len(transacoes_pl):,} registros\")\n    print(f\"   ‚Ä¢ Produtos: {len(produtos_pl):,} registros\")\n    print(f\"   ‚Ä¢ PDVs: {len(pdvs_pl):,} registros\")\n    \n    # === IN√çCIO DA L√ìGICA ADAPTADA DO NOTEBOOK 02 ===\n    \n    # 1. Renomear colunas para consist√™ncia (CORRE√á√ÉO BASEADA NOS DADOS REAIS)\n    print(\"üìù Renomeando colunas para consist√™ncia...\")\n    \n    # Transa√ß√µes: renomear para nomes padronizados\n    transacoes_pl = transacoes_pl.rename({\n        'internal_product_id': 'produto_id',\n        'internal_store_id': 'pdv_id',\n        'reference_date': 'semana',\n        'quantity': 'quantidade'\n    })\n    \n    # Produtos: usar as colunas que existem realmente\n    produtos_pl = produtos_pl.rename({\n        'produto': 'produto_id'  # A coluna chave √© 'produto', n√£o 'internal_product_id'\n    })\n    \n    # PDVs: usar as colunas que existem realmente  \n    pdvs_pl = pdvs_pl.rename({\n        'pdv': 'pdv_id'  # A coluna chave √© 'pdv', n√£o 'internal_store_id'\n    })\n    \n    print(\"‚úÖ Colunas renomeadas com sucesso!\")\n    \n    # 2. Joins dos dados (agora vai funcionar)\n    print(\"üîó Fazendo joins dos dados...\")\n    dados = transacoes_pl.join(produtos_pl, on='produto_id', how='left').join(pdvs_pl, on='pdv_id', how='left')\n    \n    # 3. Convers√£o de data e ordena√ß√£o\n    print(\"üìÖ Processando datas...\")\n    dados = dados.with_columns(pl.col(\"semana\").str.to_datetime(\"%Y-%m-%d\"))\n    dados = dados.sort([\"pdv_id\", \"produto_id\", \"semana\"])\n    \n    # 4. Features temporais (m√™s, semana do ano, sin/cos para sazonalidade)\n    print(\"üïí Criando features temporais...\")\n    dados = dados.with_columns([\n        pl.col(\"semana\").dt.month().alias(\"mes\"),\n        pl.col(\"semana\").dt.week().alias(\"semana_ano\")\n    ])\n    \n    # Features c√≠clicas para capturar sazonalidade\n    dados = dados.with_columns([\n        (pl.col(\"mes\") * (2 * np.pi / 12)).sin().alias(\"mes_sin\"),\n        (pl.col(\"mes\") * (2 * np.pi / 12)).cos().alias(\"mes_cos\")\n    ])\n    \n    # 5. Features de Lag (valores passados)\n    print(\"‚è™ Criando features de lag...\")\n    lags = [1, 2, 3, 4]\n    for lag in lags:\n        dados = dados.with_columns(\n            pl.col(\"quantidade\").shift(lag).over([\"pdv_id\", \"produto_id\"]).alias(f\"quantidade_lag_{lag}\")\n        )\n    \n    # 6. Features de Rolling Window (m√©dias m√≥veis, etc.)\n    print(\"üìä Criando features de rolling window...\")\n    dados = dados.with_columns([\n        pl.col(\"quantidade\").rolling_mean(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_media_4w\"),\n        pl.col(\"quantidade\").rolling_max(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_max_4w\"),\n        pl.col(\"quantidade\").rolling_min(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_min_4w\")\n    ])\n    \n    # 7. Features de Hash para reduzir dimensionalidade\n    print(\"üî¢ Criando features de hash...\")\n    dados = dados.with_columns([\n        pl.col(\"pdv_id\").hash().cast(pl.Int8).alias(\"pdv_hash\"),\n        pl.col(\"produto_id\").hash().cast(pl.Int8).alias(\"produto_hash\"),\n        (pl.col(\"pdv_id\").cast(str) + \"_\" + pl.col(\"produto_id\").cast(str)).hash().cast(pl.Int16).alias(\"pdv_produto_hash\")\n    ])\n    \n    # 8. Features hist√≥ricas por combina√ß√£o PDV/Produto\n    print(\"üìà Criando features hist√≥ricas...\")\n    dados = dados.with_columns([\n        pl.col(\"quantidade\").mean().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_mean\"),\n        pl.col(\"quantidade\").std().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_std\"),\n        pl.col(\"quantidade\").max().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_max\"),\n        pl.col(\"quantidade\").count().over([\"pdv_id\", \"produto_id\"]).cast(pl.Int8).alias(\"hist_count\")\n    ])\n    \n    # 9. Preencher NaNs que surgiram dos lags/rolling\n    print(\"üîß Preenchendo valores missing...\")\n    dados = dados.fill_null(0)\n    \n    # === FIM DA L√ìGICA ADAPTADA DO NOTEBOOK 02 ===\n    \n    print(\"‚úÖ Engenharia de features conclu√≠da!\")\n    \n    # Converter de volta para pandas\n    df_final = dados.to_pandas()\n    print(f\"   ‚Ä¢ Shape final: {df_final.shape}\")\n    print(f\"   ‚Ä¢ Features criadas: {len(df_final.columns)}\")\n    \n    return df_final\n\nprint(\"üõ†Ô∏è Fun√ß√£o de engenharia de features definida com sucesso!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados brutos de 2022...\n",
      "   ‚Ä¢ Transa√ß√µes: (6560698, 11)\n",
      "   ‚Ä¢ Produtos: (7092, 8)\n",
      "   ‚Ä¢ PDVs: (14419, 4)\n",
      "\n",
      "üîß Aplicando engenharia de features...\n",
      "üîß Iniciando engenharia de features com Polars...\n",
      "   ‚Ä¢ Transa√ß√µes: 6,560,698 registros\n",
      "   ‚Ä¢ Produtos: 7,092 registros\n",
      "   ‚Ä¢ PDVs: 14,419 registros\n",
      "üìù Renomeando colunas para consist√™ncia...\n"
     ]
    },
    {
     "ename": "ColumnNotFoundError",
     "evalue": "\"internal_product_id\" not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mColumnNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Aplicar engenharia de features usando a fun√ß√£o criada\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîß Aplicando engenharia de features...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m dados_treino_com_features = \u001b[43mengenharia_de_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_transacoes_2022\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_produtos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_pdvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Dados com features processados:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m   ‚Ä¢ Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdados_treino_com_features.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mengenharia_de_features\u001b[39m\u001b[34m(df_transacoes, df_produtos, df_pdvs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìù Renomeando colunas para consist√™ncia...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m transacoes_pl = transacoes_pl.rename({\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minternal_product_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mproduto_id\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minternal_store_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mpdv_id\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreference_date\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msemana\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     27\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m produtos_pl = \u001b[43mprodutos_pl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minternal_product_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduto_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m pdvs_pl = pdvs_pl.rename({\u001b[33m'\u001b[39m\u001b[33minternal_store_id\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mpdv_id\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 2. Joins dos dados (agora vai funcionar)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\dataframe\\frame.py:5026\u001b[39m, in \u001b[36mDataFrame.rename\u001b[39m\u001b[34m(self, mapping, strict)\u001b[39m\n\u001b[32m   4980\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4981\u001b[39m \u001b[33;03mRename column names.\u001b[39;00m\n\u001b[32m   4982\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5019\u001b[39m \u001b[33;03m‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\u001b[39;00m\n\u001b[32m   5020\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5021\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m   5023\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   5024\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m5026\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5027\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rafae\\OneDrive\\Documentos\\Dev\\hackathon-forecast-2025\\venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2407\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2405\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2406\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mColumnNotFoundError\u001b[39m: \"internal_product_id\" not found"
     ]
    }
   ],
   "source": [
    "# Carregar dados brutos de 2022 diretamente\n",
    "print(\"üìÇ Carregando dados brutos de 2022...\")\n",
    "\n",
    "# Carregar os dados brutos dos arquivos parquet originais\n",
    "df_transacoes_2022 = pd.read_parquet('../data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet')\n",
    "df_produtos = pd.read_parquet('../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet')\n",
    "df_pdvs = pd.read_parquet('../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet')\n",
    "\n",
    "print(f\"   ‚Ä¢ Transa√ß√µes: {df_transacoes_2022.shape}\")\n",
    "print(f\"   ‚Ä¢ Produtos: {df_produtos.shape}\") \n",
    "print(f\"   ‚Ä¢ PDVs: {df_pdvs.shape}\")\n",
    "\n",
    "# Aplicar engenharia de features usando a fun√ß√£o criada\n",
    "print('\\nüîß Aplicando engenharia de features...')\n",
    "dados_treino_com_features = engenharia_de_features(df_transacoes_2022, df_produtos, df_pdvs)\n",
    "\n",
    "print(f'\\nüìä Dados com features processados:')\n",
    "print(f'   ‚Ä¢ Shape: {dados_treino_com_features.shape}')\n",
    "print(f'   ‚Ä¢ Per√≠odo: {dados_treino_com_features[\"semana\"].min()} at√© {dados_treino_com_features[\"semana\"].max()}')\n",
    "\n",
    "# Aplicar otimiza√ß√£o de mem√≥ria (downcasting) como no seu trabalho anterior\n",
    "print('\\nüìä Otimiza√ß√£o de mem√≥ria e tratamento de missing values...')\n",
    "\n",
    "# Downcasting para otimizar mem√≥ria\n",
    "print('üîΩ Aplicando downcasting...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_treino_com_features[col].dtype\n",
    "    if dados_treino_com_features[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='float')\n",
    "\n",
    "# Otimizar colunas categ√≥ricas\n",
    "print('üìÇ Otimizando categ√≥ricas...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_treino_com_features[col].nunique()\n",
    "        total_rows = len(dados_treino_com_features)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores √∫nicos, usar category\n",
    "            dados_treino_com_features[col] = dados_treino_com_features[col].astype('category')\n",
    "\n",
    "# Tratamento inteligente de missing values para distributor_id\n",
    "if 'distributor_id' in dados_treino_com_features.columns:\n",
    "    print('üîß Tratando missing values em distributor_id...')\n",
    "    if dados_treino_com_features['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados_treino_com_features['distributor_id'].cat.categories:\n",
    "            dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].cat.add_categories([-1])\n",
    "    dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].fillna(-1)\n",
    "\n",
    "print('‚úÖ Otimiza√ß√£o conclu√≠da!')\n",
    "\n",
    "# Definir features e target\n",
    "target = 'quantidade'\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana', 'quantidade',  # IDs e target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informa√ß√£o do futuro (se existirem)\n",
    "    'mes', 'semana_ano'  # Features temporais brutas (mantemos sin/cos)\n",
    "]\n",
    "\n",
    "all_features = [col for col in dados_treino_com_features.columns if col not in exclude_features]\n",
    "\n",
    "print(f'\\nüéØ Preparando dados para treinamento:')\n",
    "print(f'   ‚Ä¢ Target: {target}')\n",
    "print(f'   ‚Ä¢ Features dispon√≠veis: {len(all_features)}')\n",
    "print(f'   ‚Ä¢ Features exclu√≠das: {len(exclude_features)}')\n",
    "\n",
    "X_full = dados_treino_com_features[all_features]\n",
    "y_full = dados_treino_com_features[target]\n",
    "\n",
    "print(f'   ‚Ä¢ X_full shape: {X_full.shape}')\n",
    "print(f'   ‚Ä¢ y_full shape: {y_full.shape}')\n",
    "\n",
    "# Treinamento do modelo final\n",
    "print('\\nüöÄ Treinando o modelo LightGBM final com todos os dados de 2022...')\n",
    "\n",
    "lgb_params_final = {\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "best_iteration = 200  # Usar a melhor itera√ß√£o da valida√ß√£o anterior\n",
    "\n",
    "train_full_lgb = lgb.Dataset(X_full, label=y_full, free_raw_data=False)  # free_raw_data=False para reutiliza√ß√£o\n",
    "final_model = lgb.train(lgb_params_final, train_full_lgb, num_boost_round=best_iteration)\n",
    "\n",
    "print(f'‚úÖ Modelo final treinado com sucesso em {best_iteration} itera√ß√µes!')\n",
    "print(f'   ‚Ä¢ Features utilizadas: {len(all_features)}')\n",
    "print(f'   ‚Ä¢ Lista de features: {all_features[:10]}...')  # Mostrar primeiras 10\n",
    "\n",
    "# Limpeza parcial de mem√≥ria (manter dados necess√°rios para teste)\n",
    "print('\\nüßπ Limpeza de mem√≥ria...')\n",
    "del train_full_lgb\n",
    "gc.collect()\n",
    "\n",
    "print('üéâ Pipeline de treinamento conclu√≠do com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üìÖ Preparando dados para as previs√µes de Janeiro/2023...')\n",
    "\n",
    "# 1. Criar o Grid de Teste para as 5 semanas de Janeiro/2023\n",
    "print('üéØ Criando grid de teste para Janeiro/2023...')\n",
    "\n",
    "# CORRE√á√ÉO: Usar os nomes originais das colunas dos dados brutos\n",
    "unique_combinations = df_transacoes_2022[['internal_store_id', 'internal_product_id']].drop_duplicates()\n",
    "unique_combinations.rename(columns={'internal_store_id': 'pdv_id', 'internal_product_id': 'produto_id'}, inplace=True)\n",
    "\n",
    "semanas_2023 = pd.DataFrame({'semana_int': range(1, 6)})\n",
    "df_teste_base = unique_combinations.merge(semanas_2023, how='cross')\n",
    "\n",
    "# Converter semana_int para formato de data (come√ßando em 2023-01-03, primeira ter√ßa de janeiro)\n",
    "df_teste_base['semana'] = df_teste_base['semana_int'].apply(\n",
    "    lambda s: pd.to_datetime('2023-01-03') + pd.to_timedelta(s-1, unit='W')\n",
    ")\n",
    "\n",
    "# Adicionar quantidade = 0 como placeholder (necess√°rio para a fun√ß√£o de features)\n",
    "df_teste_base['quantidade'] = 0\n",
    "\n",
    "print(f'   ‚Ä¢ Grid de teste: {df_teste_base.shape}')\n",
    "print(f'   ‚Ä¢ Combina√ß√µes √∫nicas: {len(unique_combinations):,}')\n",
    "print(f'   ‚Ä¢ Semanas: {sorted(df_teste_base[\"semana_int\"].unique())}')\n",
    "print(f'   ‚Ä¢ Per√≠odo: {df_teste_base[\"semana\"].min()} a {df_teste_base[\"semana\"].max()}')\n",
    "\n",
    "# 2. ESTRAT√âGIA CRUCIAL: Combinar hist√≥rico + grid de teste para features cont√≠nuas\n",
    "print('\\nüîß Combinando dados hist√≥ricos com grid de teste...')\n",
    "print('   üí° Estrat√©gia: Esta combina√ß√£o permite calcular lags e m√©dias m√≥veis corretas')\n",
    "\n",
    "# Preparar dados hist√≥ricos com nomes de colunas consistentes\n",
    "df_historico = df_transacoes_2022[['reference_date', 'internal_store_id', 'internal_product_id', 'quantity']].copy()\n",
    "df_historico.rename(columns={\n",
    "    'reference_date': 'semana',\n",
    "    'internal_store_id': 'pdv_id', \n",
    "    'internal_product_id': 'produto_id',\n",
    "    'quantity': 'quantidade'\n",
    "}, inplace=True)\n",
    "\n",
    "# Garantir que as colunas sejam consistentes entre hist√≥rico e teste\n",
    "colunas_necessarias = ['semana', 'pdv_id', 'produto_id', 'quantidade']\n",
    "df_teste_grid = df_teste_base[colunas_necessarias].copy()\n",
    "\n",
    "# Combinar dados hist√≥ricos de 2022 + grid de teste de 2023\n",
    "dados_para_features_teste = pd.concat([df_historico, df_teste_grid], ignore_index=True)\n",
    "dados_para_features_teste = dados_para_features_teste.sort_values(['pdv_id', 'produto_id', 'semana']).reset_index(drop=True)\n",
    "\n",
    "print(f'   ‚Ä¢ Dados hist√≥ricos: {len(df_historico):,}')\n",
    "print(f'   ‚Ä¢ Grid de teste: {len(df_teste_grid):,}')\n",
    "print(f'   ‚Ä¢ Dados combinados: {dados_para_features_teste.shape}')\n",
    "\n",
    "# 3. Reutilizar a fun√ß√£o de engenharia de features!\n",
    "print('\\nüõ†Ô∏è Aplicando engenharia de features aos dados combinados...')\n",
    "dados_teste_com_features = engenharia_de_features(dados_para_features_teste, df_produtos, df_pdvs)\n",
    "\n",
    "print(f'   ‚Ä¢ Dados com features: {dados_teste_com_features.shape}')\n",
    "\n",
    "# 4. Filtrar apenas as semanas de 2023 para a previs√£o\n",
    "print('\\nüéØ Filtrando dados de teste (Janeiro/2023)...')\n",
    "mask_2023 = dados_teste_com_features['semana'].dt.year == 2023\n",
    "X_teste = dados_teste_com_features[mask_2023].copy()\n",
    "\n",
    "print(f'   ‚Ä¢ Dados de teste filtrados: {X_teste.shape}')\n",
    "print(f'   ‚Ä¢ Verifica√ß√£o: {X_teste[\"semana\"].min()} a {X_teste[\"semana\"].max()}')\n",
    "\n",
    "# 5. Aplicar mesma otimiza√ß√£o de mem√≥ria e tratamento de missing no X_teste\n",
    "print('\\nüìä Aplicando otimiza√ß√£o aos dados de teste...')\n",
    "\n",
    "# Downcasting\n",
    "print('üîΩ Downcasting dados de teste...')\n",
    "for col in X_teste.select_dtypes(include=[np.number]).columns:\n",
    "    if X_teste[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        X_teste[col] = pd.to_numeric(X_teste[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        X_teste[col] = pd.to_numeric(X_teste[col], downcast='float')\n",
    "\n",
    "# Otimizar categ√≥ricas\n",
    "print('üìÇ Otimizando categ√≥ricas teste...')\n",
    "for col in X_teste.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:\n",
    "        nunique = X_teste[col].nunique()\n",
    "        total_rows = len(X_teste)\n",
    "        if nunique / total_rows < 0.5:\n",
    "            X_teste[col] = X_teste[col].astype('category')\n",
    "\n",
    "# Tratamento de missing values\n",
    "if 'distributor_id' in X_teste.columns:\n",
    "    print('üîß Tratando missing values em distributor_id (teste)...')\n",
    "    if X_teste['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in X_teste['distributor_id'].cat.categories:\n",
    "            X_teste['distributor_id'] = X_teste['distributor_id'].cat.add_categories([-1])\n",
    "    X_teste['distributor_id'] = X_teste['distributor_id'].fillna(-1)\n",
    "\n",
    "print('‚úÖ Otimiza√ß√£o dos dados de teste conclu√≠da!')\n",
    "\n",
    "# 6. Preparar features para previs√£o (garantir mesma ordem do treino)\n",
    "print('\\nüîÆ Preparando features para previs√£o...')\n",
    "features_teste = X_teste[all_features]\n",
    "\n",
    "print(f'   ‚Ä¢ Shape das features de teste: {features_teste.shape}')\n",
    "print(f'   ‚Ä¢ Features esperadas: {len(all_features)}')\n",
    "print(f'   ‚Ä¢ Verifica√ß√£o de consist√™ncia: {list(features_teste.columns) == all_features}')\n",
    "\n",
    "# 7. FAZER A PREVIS√ÉO REAL (n√£o mais placeholder!)\n",
    "print('\\nüöÄ Gerando previs√µes REAIS com o modelo treinado...')\n",
    "predictions = final_model.predict(features_teste)\n",
    "predictions = np.maximum(0, predictions)  # N√£o permitir previs√µes negativas\n",
    "\n",
    "print(f'   ‚Ä¢ Previs√µes geradas: {len(predictions):,}')\n",
    "print(f'   ‚Ä¢ Estat√≠sticas das previs√µes:')\n",
    "print(f'     - M√≠nimo: {predictions.min():.2f}')\n",
    "print(f'     - M√°ximo: {predictions.max():.2f}')\n",
    "print(f'     - M√©dia: {predictions.mean():.2f}')\n",
    "print(f'     - Mediana: {np.median(predictions):.2f}')\n",
    "print(f'     - Zeros: {(predictions == 0).sum():,} ({(predictions == 0).mean()*100:.1f}%)')\n",
    "\n",
    "# 8. Montar o arquivo de submiss√£o\n",
    "print('\\nüìã Montando arquivo de submiss√£o...')\n",
    "df_submissao = X_teste[['semana', 'pdv_id', 'produto_id']].copy()\n",
    "\n",
    "# Mapear a semana datetime de volta para o n√∫mero da semana (1-5)\n",
    "week_map = {d: i+1 for i, d in enumerate(sorted(df_submissao['semana'].unique()))}\n",
    "df_submissao['semana'] = df_submissao['semana'].map(week_map)\n",
    "\n",
    "# Adicionar previs√µes (arredondadas para inteiros)\n",
    "df_submissao['quantidade'] = predictions.round().astype(int)\n",
    "\n",
    "# Renomear colunas para formato de submiss√£o\n",
    "df_submissao.rename(columns={'pdv_id': 'pdv', 'produto_id': 'produto'}, inplace=True)\n",
    "\n",
    "print(f'   ‚Ä¢ Formato final: {df_submissao.shape}')\n",
    "print(f'   ‚Ä¢ Colunas: {list(df_submissao.columns)}')\n",
    "\n",
    "# 9. Salvar arquivos de submiss√£o\n",
    "print('\\nüíæ Salvando arquivos de submiss√£o...')\n",
    "os.makedirs('../submissions', exist_ok=True)\n",
    "\n",
    "# Salvar em Parquet\n",
    "caminho_parquet = '../submissions/previsao_final.parquet'\n",
    "df_submissao.to_parquet(caminho_parquet, index=False)\n",
    "print(f'‚úÖ Arquivo Parquet salvo: {caminho_parquet}')\n",
    "\n",
    "# Salvar em CSV\n",
    "caminho_csv = '../submissions/previsao_final.csv'\n",
    "df_submissao.to_csv(caminho_csv, index=False, sep=';', encoding='utf-8')\n",
    "print(f'‚úÖ Arquivo CSV salvo: {caminho_csv}')\n",
    "\n",
    "# 10. Mostrar amostra e estat√≠sticas finais\n",
    "print('\\nüìã Amostra da submiss√£o final:')\n",
    "print(df_submissao.head(15))\n",
    "\n",
    "print('\\nüéâ PIPELINE COMPLETO - SUBMISS√ÉO GERADA COM SUCESSO!')\n",
    "print('=' * 70)\n",
    "print(f'üìä Estat√≠sticas finais da submiss√£o:')\n",
    "print(f'   ‚Ä¢ Total de previs√µes: {len(df_submissao):,}')\n",
    "print(f'   ‚Ä¢ Semanas cobertas: {sorted(df_submissao[\"semana\"].unique())}')\n",
    "print(f'   ‚Ä¢ Combina√ß√µes √∫nicas (PDV√óProduto): {df_submissao.groupby([\"pdv\", \"produto\"]).size().count():,}')\n",
    "print(f'   ‚Ä¢ Estat√≠sticas das quantidades previstas:')\n",
    "print(f'     - Total previsto: {df_submissao[\"quantidade\"].sum():,}')\n",
    "print(f'     - M√©dia por previs√£o: {df_submissao[\"quantidade\"].mean():.2f}')\n",
    "print(f'     - Previs√µes n√£o-zero: {(df_submissao[\"quantidade\"] > 0).sum():,} ({(df_submissao[\"quantidade\"] > 0).mean()*100:.1f}%)')\n",
    "\n",
    "print(f'\\nüìÅ Arquivos de submiss√£o gerados:')\n",
    "print(f'   ‚Ä¢ CSV: {caminho_csv}')\n",
    "print(f'   ‚Ä¢ Parquet: {caminho_parquet}')\n",
    "\n",
    "print('\\nüèÜ PIPELINE PONTA A PONTA CONCLU√çDO!')\n",
    "print('   ‚úÖ Dados brutos ‚Üí Features ‚Üí Modelo ‚Üí Previs√µes ‚Üí Submiss√£o')\n",
    "print('   ‚úÖ Nenhuma depend√™ncia externa ou arquivo pr√©-processado')\n",
    "print('   ‚úÖ Pronto para avalia√ß√£o!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}