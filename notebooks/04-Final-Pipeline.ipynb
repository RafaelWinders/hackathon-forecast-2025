{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final e Geração da Submissão\n",
    "\n",
    "**🎯 PROPÓSITO DESTE NOTEBOOK:**\n",
    "Este notebook contém o pipeline final para o Hackathon Forecast Big Data 2025. O processo consiste em:\n",
    "\n",
    "1. **Carregamento dos Dados Brutos:** Carregar os dados de 2022 (transações, produtos, pdvs)\n",
    "2. **Engenharia de Features:** Função completa para processar os dados e criar features\n",
    "3. **Treinamento do Modelo Final:** Treinar o LightGBM com 100% dos dados de 2022\n",
    "4. **Geração do Grid de Previsão:** Criar o dataframe base para as 5 semanas de Janeiro/2023\n",
    "5. **Previsão e Submissão:** Gerar previsões e salvar arquivos CSV e Parquet\n",
    "\n",
    "**🚀 PIPELINE COMPLETO:**\n",
    "Este notebook executa todo o processo de ponta a ponta, da engenharia de features até a geração da submissão final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Bibliotecas carregadas com sucesso!\n",
      "🎯 Iniciando Pipeline Final para Submissão\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Use o Polars para a engenharia de features, como no notebook 02\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('📚 Bibliotecas carregadas com sucesso!')\n",
    "print('🎯 Iniciando Pipeline Final para Submissão')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Função de engenharia de features definida com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def engenharia_de_features(df_transacoes, df_produtos, df_pdvs):\n",
    "    \"\"\"\n",
    "    Aplica a engenharia de features completa usando Polars para eficiência.\n",
    "    Recebe os dataframes brutos e retorna um dataframe pandas com as features.\n",
    "    \n",
    "    Esta função implementa EXATAMENTE a mesma lógica do notebook 02-Feature-Engineering-Dask.ipynb\n",
    "    \"\"\"\n",
    "    print(\"🔧 Iniciando engenharia de features com Polars...\")\n",
    "    \n",
    "    # Converter para Polars para performance\n",
    "    transacoes_pl = pl.from_pandas(df_transacoes)\n",
    "    produtos_pl = pl.from_pandas(df_produtos)\n",
    "    pdvs_pl = pl.from_pandas(df_pdvs)\n",
    "    \n",
    "    print(f\"   • Transações: {len(transacoes_pl):,} registros\")\n",
    "    print(f\"   • Produtos: {len(produtos_pl):,} registros\")\n",
    "    print(f\"   • PDVs: {len(pdvs_pl):,} registros\")\n",
    "    \n",
    "    # === INÍCIO DA LÓGICA ADAPTADA DO NOTEBOOK 02 ===\n",
    "    \n",
    "    # 1. Renomear colunas para consistência (CORREÇÃO BASEADA NOS DADOS REAIS)\n",
    "    print(\"📝 Renomeando colunas para consistência...\")\n",
    "    \n",
    "    # Transações: renomear para nomes padronizados\n",
    "    transacoes_pl = transacoes_pl.rename({\n",
    "        'internal_product_id': 'produto_id',\n",
    "        'internal_store_id': 'pdv_id',\n",
    "        'reference_date': 'semana',\n",
    "        'quantity': 'quantidade'\n",
    "    })\n",
    "    \n",
    "    # Produtos: usar as colunas que existem realmente\n",
    "    produtos_pl = produtos_pl.rename({\n",
    "        'produto': 'produto_id'  # A coluna chave é 'produto', não 'internal_product_id'\n",
    "    })\n",
    "    \n",
    "    # PDVs: usar as colunas que existem realmente  \n",
    "    pdvs_pl = pdvs_pl.rename({\n",
    "        'pdv': 'pdv_id'  # A coluna chave é 'pdv', não 'internal_store_id'\n",
    "    })\n",
    "    \n",
    "    print(\"✅ Colunas renomeadas com sucesso!\")\n",
    "    \n",
    "    # 2. Joins dos dados (agora vai funcionar)\n",
    "    print(\"🔗 Fazendo joins dos dados...\")\n",
    "    dados = transacoes_pl.join(produtos_pl, on='produto_id', how='left').join(pdvs_pl, on='pdv_id', how='left')\n",
    "    \n",
    "    # 3. Conversão de data e ordenação - CORREÇÃO AQUI\n",
    "    print(\"📅 Processando datas...\")\n",
    "    # Verificar o tipo da coluna semana antes de converter\n",
    "    semana_dtype = str(dados[\"semana\"].dtype)\n",
    "    print(f\"   • Tipo atual da coluna semana: {semana_dtype}\")\n",
    "    \n",
    "    if semana_dtype not in [\"Date\", \"Datetime\"]:\n",
    "        # Se não for Date nem Datetime, converter\n",
    "        dados = dados.with_columns(pl.col(\"semana\").str.to_datetime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    dados = dados.sort([\"pdv_id\", \"produto_id\", \"semana\"])\n",
    "    \n",
    "    # 4. Features temporais (mês, semana do ano, sin/cos para sazonalidade)\n",
    "    print(\"🕒 Criando features temporais...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"semana\").dt.month().alias(\"mes\"),\n",
    "        pl.col(\"semana\").dt.week().alias(\"semana_ano\")\n",
    "    ])\n",
    "    \n",
    "    # Features cíclicas para capturar sazonalidade\n",
    "    dados = dados.with_columns([\n",
    "        (pl.col(\"mes\") * (2 * np.pi / 12)).sin().alias(\"mes_sin\"),\n",
    "        (pl.col(\"mes\") * (2 * np.pi / 12)).cos().alias(\"mes_cos\")\n",
    "    ])\n",
    "    \n",
    "    # 5. Features de Lag (valores passados)\n",
    "    print(\"⏪ Criando features de lag...\")\n",
    "    lags = [1, 2, 3, 4]\n",
    "    for lag in lags:\n",
    "        dados = dados.with_columns(\n",
    "            pl.col(\"quantidade\").shift(lag).over([\"pdv_id\", \"produto_id\"]).alias(f\"quantidade_lag_{lag}\")\n",
    "        )\n",
    "    \n",
    "    # 6. Features de Rolling Window (médias móveis, etc.)\n",
    "    print(\"📊 Criando features de rolling window...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").rolling_mean(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_media_4w\"),\n",
    "        pl.col(\"quantidade\").rolling_max(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_max_4w\"),\n",
    "        pl.col(\"quantidade\").rolling_min(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_min_4w\")\n",
    "    ])\n",
    "    \n",
    "    # 7. Features de Hash para reduzir dimensionalidade - CORREÇÃO AQUI\n",
    "    print(\"🔢 Criando features de hash...\")\n",
    "    dados = dados.with_columns([\n",
    "        (pl.col(\"pdv_id\").hash() % 100).cast(pl.Int8).alias(\"pdv_hash\"),\n",
    "        (pl.col(\"produto_id\").hash() % 1000).cast(pl.Int16).alias(\"produto_hash\"),\n",
    "        ((pl.col(\"pdv_id\").cast(str) + \"_\" + pl.col(\"produto_id\").cast(str)).hash() % 10000).cast(pl.Int16).alias(\"pdv_produto_hash\")\n",
    "    ])\n",
    "    \n",
    "    # 8. Features históricas por combinação PDV/Produto - CORREÇÃO AQUI\n",
    "    print(\"📈 Criando features históricas...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").mean().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_mean\"),\n",
    "        pl.col(\"quantidade\").std().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_std\"),\n",
    "        pl.col(\"quantidade\").max().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_max\"),\n",
    "        pl.col(\"quantidade\").count().over([\"pdv_id\", \"produto_id\"]).cast(pl.Int16).alias(\"hist_count\")  # Int16 ao invés de Int8\n",
    "    ])\n",
    "    \n",
    "    # 9. Preencher NaNs que surgiram dos lags/rolling\n",
    "    print(\"🔧 Preenchendo valores missing...\")\n",
    "    dados = dados.fill_null(0)\n",
    "    \n",
    "    # === FIM DA LÓGICA ADAPTADA DO NOTEBOOK 02 ===\n",
    "    \n",
    "    print(\"✅ Engenharia de features concluída!\")\n",
    "    \n",
    "    # Converter de volta para pandas\n",
    "    df_final = dados.to_pandas()\n",
    "    print(f\"   • Shape final: {df_final.shape}\")\n",
    "    print(f\"   • Features criadas: {len(df_final.columns)}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "print(\"🛠️ Função de engenharia de features definida com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados brutos de 2022...\n",
      "   • Transações: (6560698, 11)\n",
      "   • Produtos: (7092, 8)\n",
      "   • PDVs: (14419, 4)\n",
      "\n",
      "🔧 Aplicando engenharia de features...\n",
      "🔧 Iniciando engenharia de features com Polars...\n",
      "   • Transações: 6,560,698 registros\n",
      "   • Produtos: 7,092 registros\n",
      "   • PDVs: 14,419 registros\n",
      "📝 Renomeando colunas para consistência...\n",
      "✅ Colunas renomeadas com sucesso!\n",
      "🔗 Fazendo joins dos dados...\n",
      "📅 Processando datas...\n",
      "   • Tipo atual da coluna semana: Date\n",
      "🕒 Criando features temporais...\n",
      "⏪ Criando features de lag...\n",
      "📊 Criando features de rolling window...\n",
      "🔢 Criando features de hash...\n",
      "📈 Criando features históricas...\n",
      "🔧 Preenchendo valores missing...\n",
      "✅ Engenharia de features concluída!\n",
      "   • Shape final: (6560698, 39)\n",
      "   • Features criadas: 39\n",
      "\n",
      "📊 Dados com features processados:\n",
      "   • Shape: (6560698, 39)\n",
      "   • Período: 2022-01-01 00:00:00 até 2022-12-01 00:00:00\n",
      "\n",
      "📊 Otimização de memória e tratamento de missing values...\n",
      "🔽 Aplicando downcasting...\n",
      "📂 Otimizando categóricas...\n",
      "🔧 Tratando missing values em distributor_id...\n",
      "✅ Otimização concluída!\n",
      "\n",
      "🎯 Preparando dados para treinamento:\n",
      "   • Target: quantidade\n",
      "   • Features disponíveis: 22\n",
      "   • Features excluídas: 6\n",
      "   • Tipos de dados únicos: float32           13\n",
      "float64            5\n",
      "int16              3\n",
      "int8               3\n",
      "datetime64[ms]     2\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "int32              1\n",
      "category           1\n",
      "Name: count, dtype: int64\n",
      "   • X_full shape: (6560698, 22)\n",
      "   • y_full shape: (6560698,)\n",
      "   • Tipos de dados em X_full: float32    12\n",
      "float64     5\n",
      "int16       3\n",
      "int32       1\n",
      "int8        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🚀 Treinando o modelo LightGBM final com todos os dados de 2022...\n",
      "✅ Modelo final treinado com sucesso em 200 iterações!\n",
      "   • Features utilizadas: 22\n",
      "   • Lista de features: ['gross_value', 'net_value', 'gross_profit', 'discount', 'taxes', 'zipcode', 'mes_sin', 'mes_cos', 'quantidade_lag_1', 'quantidade_lag_2']...\n",
      "\n",
      "🧹 Limpeza de memória...\n",
      "🎉 Pipeline de treinamento concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados brutos de 2022 diretamente\n",
    "print(\"📂 Carregando dados brutos de 2022...\")\n",
    "\n",
    "# Carregar os dados brutos dos arquivos parquet originais\n",
    "df_transacoes_2022 = pd.read_parquet('../data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet')\n",
    "df_produtos = pd.read_parquet('../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet')\n",
    "df_pdvs = pd.read_parquet('../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet')\n",
    "\n",
    "print(f\"   • Transações: {df_transacoes_2022.shape}\")\n",
    "print(f\"   • Produtos: {df_produtos.shape}\") \n",
    "print(f\"   • PDVs: {df_pdvs.shape}\")\n",
    "\n",
    "# Aplicar engenharia de features usando a função criada\n",
    "print('\\n🔧 Aplicando engenharia de features...')\n",
    "dados_treino_com_features = engenharia_de_features(df_transacoes_2022, df_produtos, df_pdvs)\n",
    "\n",
    "print(f'\\n📊 Dados com features processados:')\n",
    "print(f'   • Shape: {dados_treino_com_features.shape}')\n",
    "print(f'   • Período: {dados_treino_com_features[\"semana\"].min()} até {dados_treino_com_features[\"semana\"].max()}')\n",
    "\n",
    "# Aplicar otimização de memória (downcasting) como no seu trabalho anterior\n",
    "print('\\n📊 Otimização de memória e tratamento de missing values...')\n",
    "\n",
    "# Downcasting para otimizar memória\n",
    "print('🔽 Aplicando downcasting...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_treino_com_features[col].dtype\n",
    "    if dados_treino_com_features[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='float')\n",
    "\n",
    "# Otimizar colunas categóricas\n",
    "print('📂 Otimizando categóricas...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_treino_com_features[col].nunique()\n",
    "        total_rows = len(dados_treino_com_features)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores únicos, usar category\n",
    "            dados_treino_com_features[col] = dados_treino_com_features[col].astype('category')\n",
    "\n",
    "# Tratamento inteligente de missing values para distributor_id\n",
    "if 'distributor_id' in dados_treino_com_features.columns:\n",
    "    print('🔧 Tratando missing values em distributor_id...')\n",
    "    if dados_treino_com_features['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados_treino_com_features['distributor_id'].cat.categories:\n",
    "            dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].cat.add_categories([-1])\n",
    "    dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].fillna(-1)\n",
    "\n",
    "print('✅ Otimização concluída!')\n",
    "\n",
    "# Definir features e target - CORREÇÃO AQUI PARA EXCLUIR DATETIME\n",
    "target = 'quantidade'\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana', 'quantidade',  # IDs, datetime e target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informação do futuro (se existirem)\n",
    "    'mes', 'semana_ano'  # Features temporais brutas (mantemos sin/cos)\n",
    "]\n",
    "\n",
    "# Filtrar colunas que existem no DataFrame\n",
    "exclude_features = [col for col in exclude_features if col in dados_treino_com_features.columns]\n",
    "\n",
    "# Selecionar apenas colunas numéricas que não são datetime\n",
    "numeric_columns = dados_treino_com_features.select_dtypes(include=[np.number]).columns\n",
    "all_features = [col for col in numeric_columns if col not in exclude_features]\n",
    "\n",
    "print(f'\\n🎯 Preparando dados para treinamento:')\n",
    "print(f'   • Target: {target}')\n",
    "print(f'   • Features disponíveis: {len(all_features)}')\n",
    "print(f'   • Features excluídas: {len(exclude_features)}')\n",
    "print(f'   • Tipos de dados únicos: {dados_treino_com_features.dtypes.value_counts()}')\n",
    "\n",
    "X_full = dados_treino_com_features[all_features]\n",
    "y_full = dados_treino_com_features[target]\n",
    "\n",
    "print(f'   • X_full shape: {X_full.shape}')\n",
    "print(f'   • y_full shape: {y_full.shape}')\n",
    "print(f'   • Tipos de dados em X_full: {X_full.dtypes.value_counts()}')\n",
    "\n",
    "# Treinamento do modelo final\n",
    "print('\\n🚀 Treinando o modelo LightGBM final com todos os dados de 2022...')\n",
    "\n",
    "lgb_params_final = {\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "best_iteration = 200  # Usar a melhor iteração da validação anterior\n",
    "\n",
    "train_full_lgb = lgb.Dataset(X_full, label=y_full, free_raw_data=False)  # free_raw_data=False para reutilização\n",
    "final_model = lgb.train(lgb_params_final, train_full_lgb, num_boost_round=best_iteration)\n",
    "\n",
    "print(f'✅ Modelo final treinado com sucesso em {best_iteration} iterações!')\n",
    "print(f'   • Features utilizadas: {len(all_features)}')\n",
    "print(f'   • Lista de features: {all_features[:10]}...')  # Mostrar primeiras 10\n",
    "\n",
    "# Limpeza parcial de memória (manter dados necessários para teste)\n",
    "print('\\n🧹 Limpeza de memória...')\n",
    "del train_full_lgb\n",
    "gc.collect()\n",
    "\n",
    "print('🎉 Pipeline de treinamento concluído com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engenharia_de_features(df_transacoes, df_produtos, df_pdvs):\n",
    "    \"\"\"\n",
    "    Função para engenharia de features adaptada do notebook 02\n",
    "    \"\"\"\n",
    "    import polars as pl\n",
    "    \n",
    "    print(\"🚀 Iniciando engenharia de features...\")\n",
    "    \n",
    "    # 1. Renomear colunas para consistência com o pipeline original\n",
    "    print(\"🔄 Renomeando colunas...\")\n",
    "    transacoes_pl = df_transacoes.rename({\n",
    "        'internal_product_id': 'produto_id',\n",
    "        'internal_store_id': 'pdv_id', \n",
    "        'reference_date': 'semana',\n",
    "        'quantity': 'quantidade'\n",
    "    })\n",
    "    \n",
    "    produtos_pl = df_produtos.rename({'produto': 'produto_id'})\n",
    "    pdvs_pl = df_pdvs.rename({'pdv': 'pdv_id'})\n",
    "    \n",
    "    # 2. Joins para criar dataset base\n",
    "    print(\"🔗 Realizando joins...\")\n",
    "    dados = transacoes_pl.join(produtos_pl, on=\"produto_id\", how=\"left\")\n",
    "    dados = dados.join(pdvs_pl, on=\"pdv_id\", how=\"left\")\n",
    "    \n",
    "    # Filtrar apenas dados com vendas > 0 para otimização de memória\n",
    "    dados = dados.filter(pl.col(\"quantidade\") > 0)\n",
    "    \n",
    "    # 3. Conversão de data e ordenação\n",
    "    print(\"📅 Processando datas...\")\n",
    "    # Verificar se a coluna já está em formato datetime\n",
    "    if str(dados[\"semana\"].dtype) != \"Date\":\n",
    "        dados = dados.with_columns(pl.col(\"semana\").str.to_datetime(\"%Y-%m-%d\"))\n",
    "    dados = dados.sort([\"pdv_id\", \"produto_id\", \"semana\"])\n",
    "    \n",
    "    # 4. Features temporais (mês, semana do ano, sin/cos para sazonalidade)\n",
    "    print(\"📊 Criando features temporais...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"semana\").dt.month().alias(\"mes\"),\n",
    "        pl.col(\"semana\").dt.week().alias(\"semana_do_ano\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.month() / 12).sin().alias(\"mes_sin\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.month() / 12).cos().alias(\"mes_cos\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.week() / 52).sin().alias(\"semana_sin\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.week() / 52).cos().alias(\"semana_cos\")\n",
    "    ])\n",
    "    \n",
    "    # 5. Features de lag (1, 2, 3, 4 semanas)\n",
    "    print(\"⏮️ Criando features de lag...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").shift(1).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_1\"),\n",
    "        pl.col(\"quantidade\").shift(2).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_2\"),\n",
    "        pl.col(\"quantidade\").shift(3).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_3\"),\n",
    "        pl.col(\"quantidade\").shift(4).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_4\")\n",
    "    ])\n",
    "    \n",
    "    # 6. Features de rolling window (médias móveis, std, min, max)\n",
    "    print(\"📈 Criando features de rolling window...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").rolling_mean(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"media_4_semanas\"),\n",
    "        pl.col(\"quantidade\").rolling_std(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"std_4_semanas\"),\n",
    "        pl.col(\"quantidade\").rolling_min(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"min_4_semanas\"),\n",
    "        pl.col(\"quantidade\").rolling_max(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"max_4_semanas\")\n",
    "    ])\n",
    "    \n",
    "    # 7. Features de hash para PDV e produto (redução de dimensionalidade)\n",
    "    print(\"🏷️ Criando features de hash...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"pdv_id\").hash(seed=42) % 100,\n",
    "        pl.col(\"produto_id\").hash(seed=123) % 1000\n",
    "    ])\n",
    "    \n",
    "    # 8. Features de interação (PDV x produto)\n",
    "    print(\"🤝 Criando features de interação...\")\n",
    "    dados = dados.with_columns(\n",
    "        (pl.col(\"pdv_id\").cast(pl.Utf8) + \"_\" + pl.col(\"produto_id\").cast(pl.Utf8)).alias(\"pdv_produto_combo\")\n",
    "    )\n",
    "    dados = dados.with_columns(\n",
    "        pl.col(\"pdv_produto_combo\").hash(seed=456) % 10000\n",
    "    )\n",
    "    \n",
    "    # 9. Limpeza de valores nulos (preencher com 0 para lags iniciais)\n",
    "    print(\"🧹 Limpando valores nulos...\")\n",
    "    colunas_numericas = [\"lag_1\", \"lag_2\", \"lag_3\", \"lag_4\", \"media_4_semanas\", \"std_4_semanas\", \"min_4_semanas\", \"max_4_semanas\"]\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(col).fill_null(0) for col in colunas_numericas\n",
    "    ])\n",
    "    \n",
    "    # 10. Conversão para pandas para compatibilidade com sklearn\n",
    "    print(\"🔄 Convertendo para pandas...\")\n",
    "    dados_pandas = dados.to_pandas()\n",
    "    \n",
    "    print(f\"✅ Features criadas com sucesso! Shape: {dados_pandas.shape}\")\n",
    "    return dados_pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
