{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final e GeraÃ§Ã£o da SubmissÃ£o\n",
    "\n",
    "**ğŸ¯ PROPÃ“SITO DESTE NOTEBOOK:**\n",
    "Este notebook contÃ©m o pipeline final para o Hackathon Forecast Big Data 2025. O processo consiste em:\n",
    "\n",
    "1. **Carregamento dos Dados Brutos:** Carregar os dados de 2022 (transaÃ§Ãµes, produtos, pdvs)\n",
    "2. **Engenharia de Features:** FunÃ§Ã£o completa para processar os dados e criar features\n",
    "3. **Treinamento do Modelo Final:** Treinar o LightGBM com 100% dos dados de 2022\n",
    "4. **GeraÃ§Ã£o do Grid de PrevisÃ£o:** Criar o dataframe base para as 5 semanas de Janeiro/2023\n",
    "5. **PrevisÃ£o e SubmissÃ£o:** Gerar previsÃµes e salvar arquivos CSV e Parquet\n",
    "\n",
    "**ğŸš€ PIPELINE COMPLETO:**\n",
    "Este notebook executa todo o processo de ponta a ponta, da engenharia de features atÃ© a geraÃ§Ã£o da submissÃ£o final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Bibliotecas carregadas com sucesso!\n",
      "ğŸ¯ Iniciando Pipeline Final para SubmissÃ£o\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Use o Polars para a engenharia de features, como no notebook 02\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('ğŸ“š Bibliotecas carregadas com sucesso!')\n",
    "print('ğŸ¯ Iniciando Pipeline Final para SubmissÃ£o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ FunÃ§Ã£o de engenharia de features definida com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def engenharia_de_features(df_transacoes, df_produtos, df_pdvs):\n",
    "    \"\"\"\n",
    "    Aplica a engenharia de features completa usando Polars para eficiÃªncia.\n",
    "    Recebe os dataframes brutos e retorna um dataframe pandas com as features.\n",
    "    \n",
    "    Esta funÃ§Ã£o implementa EXATAMENTE a mesma lÃ³gica do notebook 02-Feature-Engineering-Dask.ipynb\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ Iniciando engenharia de features com Polars...\")\n",
    "    \n",
    "    # Converter para Polars para performance\n",
    "    transacoes_pl = pl.from_pandas(df_transacoes)\n",
    "    produtos_pl = pl.from_pandas(df_produtos)\n",
    "    pdvs_pl = pl.from_pandas(df_pdvs)\n",
    "    \n",
    "    print(f\"   â€¢ TransaÃ§Ãµes: {len(transacoes_pl):,} registros\")\n",
    "    print(f\"   â€¢ Produtos: {len(produtos_pl):,} registros\")\n",
    "    print(f\"   â€¢ PDVs: {len(pdvs_pl):,} registros\")\n",
    "    \n",
    "    # === INÃCIO DA LÃ“GICA ADAPTADA DO NOTEBOOK 02 ===\n",
    "    \n",
    "    # 1. Renomear colunas para consistÃªncia (CORREÃ‡ÃƒO BASEADA NOS DADOS REAIS)\n",
    "    print(\"ğŸ“ Renomeando colunas para consistÃªncia...\")\n",
    "    \n",
    "    # TransaÃ§Ãµes: renomear para nomes padronizados\n",
    "    transacoes_pl = transacoes_pl.rename({\n",
    "        'internal_product_id': 'produto_id',\n",
    "        'internal_store_id': 'pdv_id',\n",
    "        'reference_date': 'semana',\n",
    "        'quantity': 'quantidade'\n",
    "    })\n",
    "    \n",
    "    # Produtos: usar as colunas que existem realmente\n",
    "    produtos_pl = produtos_pl.rename({\n",
    "        'produto': 'produto_id'  # A coluna chave Ã© 'produto', nÃ£o 'internal_product_id'\n",
    "    })\n",
    "    \n",
    "    # PDVs: usar as colunas que existem realmente  \n",
    "    pdvs_pl = pdvs_pl.rename({\n",
    "        'pdv': 'pdv_id'  # A coluna chave Ã© 'pdv', nÃ£o 'internal_store_id'\n",
    "    })\n",
    "    \n",
    "    print(\"âœ… Colunas renomeadas com sucesso!\")\n",
    "    \n",
    "    # 2. Joins dos dados (agora vai funcionar)\n",
    "    print(\"ğŸ”— Fazendo joins dos dados...\")\n",
    "    dados = transacoes_pl.join(produtos_pl, on='produto_id', how='left').join(pdvs_pl, on='pdv_id', how='left')\n",
    "    \n",
    "    # 3. ConversÃ£o de data e ordenaÃ§Ã£o - CORREÃ‡ÃƒO AQUI\n",
    "    print(\"ğŸ“… Processando datas...\")\n",
    "    # Verificar o tipo da coluna semana antes de converter\n",
    "    semana_dtype = str(dados[\"semana\"].dtype)\n",
    "    print(f\"   â€¢ Tipo atual da coluna semana: {semana_dtype}\")\n",
    "    \n",
    "    if semana_dtype not in [\"Date\", \"Datetime\"]:\n",
    "        # Se nÃ£o for Date nem Datetime, converter\n",
    "        dados = dados.with_columns(pl.col(\"semana\").str.to_datetime(\"%Y-%m-%d\"))\n",
    "    \n",
    "    dados = dados.sort([\"pdv_id\", \"produto_id\", \"semana\"])\n",
    "    \n",
    "    # 4. Features temporais (mÃªs, semana do ano, sin/cos para sazonalidade)\n",
    "    print(\"ğŸ•’ Criando features temporais...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"semana\").dt.month().alias(\"mes\"),\n",
    "        pl.col(\"semana\").dt.week().alias(\"semana_ano\")\n",
    "    ])\n",
    "    \n",
    "    # Features cÃ­clicas para capturar sazonalidade\n",
    "    dados = dados.with_columns([\n",
    "        (pl.col(\"mes\") * (2 * np.pi / 12)).sin().alias(\"mes_sin\"),\n",
    "        (pl.col(\"mes\") * (2 * np.pi / 12)).cos().alias(\"mes_cos\")\n",
    "    ])\n",
    "    \n",
    "    # 5. Features de Lag (valores passados)\n",
    "    print(\"âª Criando features de lag...\")\n",
    "    lags = [1, 2, 3, 4]\n",
    "    for lag in lags:\n",
    "        dados = dados.with_columns(\n",
    "            pl.col(\"quantidade\").shift(lag).over([\"pdv_id\", \"produto_id\"]).alias(f\"quantidade_lag_{lag}\")\n",
    "        )\n",
    "    \n",
    "    # 6. Features de Rolling Window (mÃ©dias mÃ³veis, etc.)\n",
    "    print(\"ğŸ“Š Criando features de rolling window...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").rolling_mean(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_media_4w\"),\n",
    "        pl.col(\"quantidade\").rolling_max(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_max_4w\"),\n",
    "        pl.col(\"quantidade\").rolling_min(window_size=4, min_periods=1).over([\"pdv_id\", \"produto_id\"]).alias(\"quantidade_min_4w\")\n",
    "    ])\n",
    "    \n",
    "    # 7. Features de Hash para reduzir dimensionalidade - CORREÃ‡ÃƒO AQUI\n",
    "    print(\"ğŸ”¢ Criando features de hash...\")\n",
    "    dados = dados.with_columns([\n",
    "        (pl.col(\"pdv_id\").hash() % 100).cast(pl.Int8).alias(\"pdv_hash\"),\n",
    "        (pl.col(\"produto_id\").hash() % 1000).cast(pl.Int16).alias(\"produto_hash\"),\n",
    "        ((pl.col(\"pdv_id\").cast(str) + \"_\" + pl.col(\"produto_id\").cast(str)).hash() % 10000).cast(pl.Int16).alias(\"pdv_produto_hash\")\n",
    "    ])\n",
    "    \n",
    "    # 8. Features histÃ³ricas por combinaÃ§Ã£o PDV/Produto - CORREÃ‡ÃƒO AQUI\n",
    "    print(\"ğŸ“ˆ Criando features histÃ³ricas...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").mean().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_mean\"),\n",
    "        pl.col(\"quantidade\").std().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_std\"),\n",
    "        pl.col(\"quantidade\").max().over([\"pdv_id\", \"produto_id\"]).alias(\"hist_max\"),\n",
    "        pl.col(\"quantidade\").count().over([\"pdv_id\", \"produto_id\"]).cast(pl.Int16).alias(\"hist_count\")  # Int16 ao invÃ©s de Int8\n",
    "    ])\n",
    "    \n",
    "    # 9. Preencher NaNs que surgiram dos lags/rolling\n",
    "    print(\"ğŸ”§ Preenchendo valores missing...\")\n",
    "    dados = dados.fill_null(0)\n",
    "    \n",
    "    # === FIM DA LÃ“GICA ADAPTADA DO NOTEBOOK 02 ===\n",
    "    \n",
    "    print(\"âœ… Engenharia de features concluÃ­da!\")\n",
    "    \n",
    "    # Converter de volta para pandas\n",
    "    df_final = dados.to_pandas()\n",
    "    print(f\"   â€¢ Shape final: {df_final.shape}\")\n",
    "    print(f\"   â€¢ Features criadas: {len(df_final.columns)}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "print(\"ğŸ› ï¸ FunÃ§Ã£o de engenharia de features definida com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando dados brutos de 2022...\n",
      "   â€¢ TransaÃ§Ãµes: (6560698, 11)\n",
      "   â€¢ Produtos: (7092, 8)\n",
      "   â€¢ PDVs: (14419, 4)\n",
      "\n",
      "ğŸ”§ Aplicando engenharia de features...\n",
      "ğŸ”§ Iniciando engenharia de features com Polars...\n",
      "   â€¢ TransaÃ§Ãµes: 6,560,698 registros\n",
      "   â€¢ Produtos: 7,092 registros\n",
      "   â€¢ PDVs: 14,419 registros\n",
      "ğŸ“ Renomeando colunas para consistÃªncia...\n",
      "âœ… Colunas renomeadas com sucesso!\n",
      "ğŸ”— Fazendo joins dos dados...\n",
      "ğŸ“… Processando datas...\n",
      "   â€¢ Tipo atual da coluna semana: Date\n",
      "ğŸ•’ Criando features temporais...\n",
      "âª Criando features de lag...\n",
      "ğŸ“Š Criando features de rolling window...\n",
      "ğŸ”¢ Criando features de hash...\n",
      "ğŸ“ˆ Criando features histÃ³ricas...\n",
      "ğŸ”§ Preenchendo valores missing...\n",
      "âœ… Engenharia de features concluÃ­da!\n",
      "   â€¢ Shape final: (6560698, 39)\n",
      "   â€¢ Features criadas: 39\n",
      "\n",
      "ğŸ“Š Dados com features processados:\n",
      "   â€¢ Shape: (6560698, 39)\n",
      "   â€¢ PerÃ­odo: 2022-01-01 00:00:00 atÃ© 2022-12-01 00:00:00\n",
      "\n",
      "ğŸ“Š OtimizaÃ§Ã£o de memÃ³ria e tratamento de missing values...\n",
      "ğŸ”½ Aplicando downcasting...\n",
      "ğŸ“‚ Otimizando categÃ³ricas...\n",
      "ğŸ”§ Tratando missing values em distributor_id...\n",
      "âœ… OtimizaÃ§Ã£o concluÃ­da!\n",
      "\n",
      "ğŸ¯ Preparando dados para treinamento:\n",
      "   â€¢ Target: quantidade\n",
      "   â€¢ Features disponÃ­veis: 22\n",
      "   â€¢ Features excluÃ­das: 6\n",
      "   â€¢ Tipos de dados Ãºnicos: float32           13\n",
      "float64            5\n",
      "int16              3\n",
      "int8               3\n",
      "datetime64[ms]     2\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "category           1\n",
      "int32              1\n",
      "category           1\n",
      "Name: count, dtype: int64\n",
      "   â€¢ X_full shape: (6560698, 22)\n",
      "   â€¢ y_full shape: (6560698,)\n",
      "   â€¢ Tipos de dados em X_full: float32    12\n",
      "float64     5\n",
      "int16       3\n",
      "int32       1\n",
      "int8        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸš€ Treinando o modelo LightGBM final com todos os dados de 2022...\n",
      "âœ… Modelo final treinado com sucesso em 200 iteraÃ§Ãµes!\n",
      "   â€¢ Features utilizadas: 22\n",
      "   â€¢ Lista de features: ['gross_value', 'net_value', 'gross_profit', 'discount', 'taxes', 'zipcode', 'mes_sin', 'mes_cos', 'quantidade_lag_1', 'quantidade_lag_2']...\n",
      "\n",
      "ğŸ§¹ Limpeza de memÃ³ria...\n",
      "ğŸ‰ Pipeline de treinamento concluÃ­do com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados brutos de 2022 diretamente\n",
    "print(\"ğŸ“‚ Carregando dados brutos de 2022...\")\n",
    "\n",
    "# Carregar os dados brutos dos arquivos parquet originais\n",
    "df_transacoes_2022 = pd.read_parquet('../data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet')\n",
    "df_produtos = pd.read_parquet('../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet')\n",
    "df_pdvs = pd.read_parquet('../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet')\n",
    "\n",
    "print(f\"   â€¢ TransaÃ§Ãµes: {df_transacoes_2022.shape}\")\n",
    "print(f\"   â€¢ Produtos: {df_produtos.shape}\") \n",
    "print(f\"   â€¢ PDVs: {df_pdvs.shape}\")\n",
    "\n",
    "# Aplicar engenharia de features usando a funÃ§Ã£o criada\n",
    "print('\\nğŸ”§ Aplicando engenharia de features...')\n",
    "dados_treino_com_features = engenharia_de_features(df_transacoes_2022, df_produtos, df_pdvs)\n",
    "\n",
    "print(f'\\nğŸ“Š Dados com features processados:')\n",
    "print(f'   â€¢ Shape: {dados_treino_com_features.shape}')\n",
    "print(f'   â€¢ PerÃ­odo: {dados_treino_com_features[\"semana\"].min()} atÃ© {dados_treino_com_features[\"semana\"].max()}')\n",
    "\n",
    "# Aplicar otimizaÃ§Ã£o de memÃ³ria (downcasting) como no seu trabalho anterior\n",
    "print('\\nğŸ“Š OtimizaÃ§Ã£o de memÃ³ria e tratamento de missing values...')\n",
    "\n",
    "# Downcasting para otimizar memÃ³ria\n",
    "print('ğŸ”½ Aplicando downcasting...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_treino_com_features[col].dtype\n",
    "    if dados_treino_com_features[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_treino_com_features[col] = pd.to_numeric(dados_treino_com_features[col], downcast='float')\n",
    "\n",
    "# Otimizar colunas categÃ³ricas\n",
    "print('ğŸ“‚ Otimizando categÃ³ricas...')\n",
    "for col in dados_treino_com_features.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_treino_com_features[col].nunique()\n",
    "        total_rows = len(dados_treino_com_features)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores Ãºnicos, usar category\n",
    "            dados_treino_com_features[col] = dados_treino_com_features[col].astype('category')\n",
    "\n",
    "# Tratamento inteligente de missing values para distributor_id\n",
    "if 'distributor_id' in dados_treino_com_features.columns:\n",
    "    print('ğŸ”§ Tratando missing values em distributor_id...')\n",
    "    if dados_treino_com_features['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in dados_treino_com_features['distributor_id'].cat.categories:\n",
    "            dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].cat.add_categories([-1])\n",
    "    dados_treino_com_features['distributor_id'] = dados_treino_com_features['distributor_id'].fillna(-1)\n",
    "\n",
    "print('âœ… OtimizaÃ§Ã£o concluÃ­da!')\n",
    "\n",
    "# Definir features e target - CORREÃ‡ÃƒO AQUI PARA EXCLUIR DATETIME\n",
    "target = 'quantidade'\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana', 'quantidade',  # IDs, datetime e target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informaÃ§Ã£o do futuro (se existirem)\n",
    "    'mes', 'semana_ano'  # Features temporais brutas (mantemos sin/cos)\n",
    "]\n",
    "\n",
    "# Filtrar colunas que existem no DataFrame\n",
    "exclude_features = [col for col in exclude_features if col in dados_treino_com_features.columns]\n",
    "\n",
    "# Selecionar apenas colunas numÃ©ricas que nÃ£o sÃ£o datetime\n",
    "numeric_columns = dados_treino_com_features.select_dtypes(include=[np.number]).columns\n",
    "all_features = [col for col in numeric_columns if col not in exclude_features]\n",
    "\n",
    "print(f'\\nğŸ¯ Preparando dados para treinamento:')\n",
    "print(f'   â€¢ Target: {target}')\n",
    "print(f'   â€¢ Features disponÃ­veis: {len(all_features)}')\n",
    "print(f'   â€¢ Features excluÃ­das: {len(exclude_features)}')\n",
    "print(f'   â€¢ Tipos de dados Ãºnicos: {dados_treino_com_features.dtypes.value_counts()}')\n",
    "\n",
    "X_full = dados_treino_com_features[all_features]\n",
    "y_full = dados_treino_com_features[target]\n",
    "\n",
    "print(f'   â€¢ X_full shape: {X_full.shape}')\n",
    "print(f'   â€¢ y_full shape: {y_full.shape}')\n",
    "print(f'   â€¢ Tipos de dados em X_full: {X_full.dtypes.value_counts()}')\n",
    "\n",
    "# Treinamento do modelo final\n",
    "print('\\nğŸš€ Treinando o modelo LightGBM final com todos os dados de 2022...')\n",
    "\n",
    "lgb_params_final = {\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "best_iteration = 200  # Usar a melhor iteraÃ§Ã£o da validaÃ§Ã£o anterior\n",
    "\n",
    "train_full_lgb = lgb.Dataset(X_full, label=y_full, free_raw_data=False)  # free_raw_data=False para reutilizaÃ§Ã£o\n",
    "final_model = lgb.train(lgb_params_final, train_full_lgb, num_boost_round=best_iteration)\n",
    "\n",
    "print(f'âœ… Modelo final treinado com sucesso em {best_iteration} iteraÃ§Ãµes!')\n",
    "print(f'   â€¢ Features utilizadas: {len(all_features)}')\n",
    "print(f'   â€¢ Lista de features: {all_features[:10]}...')  # Mostrar primeiras 10\n",
    "\n",
    "# Limpeza parcial de memÃ³ria (manter dados necessÃ¡rios para teste)\n",
    "print('\\nğŸ§¹ Limpeza de memÃ³ria...')\n",
    "del train_full_lgb\n",
    "gc.collect()\n",
    "\n",
    "print('ğŸ‰ Pipeline de treinamento concluÃ­do com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engenharia_de_features(df_transacoes, df_produtos, df_pdvs):\n",
    "    \"\"\"\n",
    "    FunÃ§Ã£o para engenharia de features adaptada do notebook 02\n",
    "    \"\"\"\n",
    "    import polars as pl\n",
    "    \n",
    "    print(\"ğŸš€ Iniciando engenharia de features...\")\n",
    "    \n",
    "    # 1. Renomear colunas para consistÃªncia com o pipeline original\n",
    "    print(\"ğŸ”„ Renomeando colunas...\")\n",
    "    transacoes_pl = df_transacoes.rename({\n",
    "        'internal_product_id': 'produto_id',\n",
    "        'internal_store_id': 'pdv_id', \n",
    "        'reference_date': 'semana',\n",
    "        'quantity': 'quantidade'\n",
    "    })\n",
    "    \n",
    "    produtos_pl = df_produtos.rename({'produto': 'produto_id'})\n",
    "    pdvs_pl = df_pdvs.rename({'pdv': 'pdv_id'})\n",
    "    \n",
    "    # 2. Joins para criar dataset base\n",
    "    print(\"ğŸ”— Realizando joins...\")\n",
    "    dados = transacoes_pl.join(produtos_pl, on=\"produto_id\", how=\"left\")\n",
    "    dados = dados.join(pdvs_pl, on=\"pdv_id\", how=\"left\")\n",
    "    \n",
    "    # Filtrar apenas dados com vendas > 0 para otimizaÃ§Ã£o de memÃ³ria\n",
    "    dados = dados.filter(pl.col(\"quantidade\") > 0)\n",
    "    \n",
    "    # 3. ConversÃ£o de data e ordenaÃ§Ã£o\n",
    "    print(\"ğŸ“… Processando datas...\")\n",
    "    # Verificar se a coluna jÃ¡ estÃ¡ em formato datetime\n",
    "    if str(dados[\"semana\"].dtype) != \"Date\":\n",
    "        dados = dados.with_columns(pl.col(\"semana\").str.to_datetime(\"%Y-%m-%d\"))\n",
    "    dados = dados.sort([\"pdv_id\", \"produto_id\", \"semana\"])\n",
    "    \n",
    "    # 4. Features temporais (mÃªs, semana do ano, sin/cos para sazonalidade)\n",
    "    print(\"ğŸ“Š Criando features temporais...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"semana\").dt.month().alias(\"mes\"),\n",
    "        pl.col(\"semana\").dt.week().alias(\"semana_do_ano\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.month() / 12).sin().alias(\"mes_sin\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.month() / 12).cos().alias(\"mes_cos\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.week() / 52).sin().alias(\"semana_sin\"),\n",
    "        (2 * 3.14159 * pl.col(\"semana\").dt.week() / 52).cos().alias(\"semana_cos\")\n",
    "    ])\n",
    "    \n",
    "    # 5. Features de lag (1, 2, 3, 4 semanas)\n",
    "    print(\"â®ï¸ Criando features de lag...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").shift(1).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_1\"),\n",
    "        pl.col(\"quantidade\").shift(2).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_2\"),\n",
    "        pl.col(\"quantidade\").shift(3).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_3\"),\n",
    "        pl.col(\"quantidade\").shift(4).over([\"pdv_id\", \"produto_id\"]).alias(\"lag_4\")\n",
    "    ])\n",
    "    \n",
    "    # 6. Features de rolling window (mÃ©dias mÃ³veis, std, min, max)\n",
    "    print(\"ğŸ“ˆ Criando features de rolling window...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"quantidade\").rolling_mean(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"media_4_semanas\"),\n",
    "        pl.col(\"quantidade\").rolling_std(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"std_4_semanas\"),\n",
    "        pl.col(\"quantidade\").rolling_min(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"min_4_semanas\"),\n",
    "        pl.col(\"quantidade\").rolling_max(window_size=4).over([\"pdv_id\", \"produto_id\"]).alias(\"max_4_semanas\")\n",
    "    ])\n",
    "    \n",
    "    # 7. Features de hash para PDV e produto (reduÃ§Ã£o de dimensionalidade)\n",
    "    print(\"ğŸ·ï¸ Criando features de hash...\")\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(\"pdv_id\").hash(seed=42) % 100,\n",
    "        pl.col(\"produto_id\").hash(seed=123) % 1000\n",
    "    ])\n",
    "    \n",
    "    # 8. Features de interaÃ§Ã£o (PDV x produto)\n",
    "    print(\"ğŸ¤ Criando features de interaÃ§Ã£o...\")\n",
    "    dados = dados.with_columns(\n",
    "        (pl.col(\"pdv_id\").cast(pl.Utf8) + \"_\" + pl.col(\"produto_id\").cast(pl.Utf8)).alias(\"pdv_produto_combo\")\n",
    "    )\n",
    "    dados = dados.with_columns(\n",
    "        pl.col(\"pdv_produto_combo\").hash(seed=456) % 10000\n",
    "    )\n",
    "    \n",
    "    # 9. Limpeza de valores nulos (preencher com 0 para lags iniciais)\n",
    "    print(\"ğŸ§¹ Limpando valores nulos...\")\n",
    "    colunas_numericas = [\"lag_1\", \"lag_2\", \"lag_3\", \"lag_4\", \"media_4_semanas\", \"std_4_semanas\", \"min_4_semanas\", \"max_4_semanas\"]\n",
    "    dados = dados.with_columns([\n",
    "        pl.col(col).fill_null(0) for col in colunas_numericas\n",
    "    ])\n",
    "    \n",
    "    # 10. ConversÃ£o para pandas para compatibilidade com sklearn\n",
    "    print(\"ğŸ”„ Convertendo para pandas...\")\n",
    "    dados_pandas = dados.to_pandas()\n",
    "    \n",
    "    print(f\"âœ… Features criadas com sucesso! Shape: {dados_pandas.shape}\")\n",
    "    return dados_pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
