{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering com Dask (Otimizado para Big Data)\n",
    "\n",
    "Este notebook implementa feature engineering usando **Dask**, uma biblioteca projetada para processar datasets maiores que a RAM disponível.\n",
    "\n",
    "## Vantagens do Dask:\n",
    "- ✅ **Processamento Out-of-Core**: Datasets maiores que a RAM\n",
    "- ✅ **Paralelização Automática**: Usa todos os cores da CPU\n",
    "- ✅ **API Similar ao Pandas**: Fácil migração\n",
    "- ✅ **Computação Lazy**: Executa apenas quando necessário\n",
    "- ✅ **Escalabilidade**: Funciona em clusters distribuídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar Dask\n",
    "from dask.distributed import Client, progress\n",
    "import dask\n",
    "\n",
    "# Configurar para usar threads (melhor para I/O)\n",
    "dask.config.set(scheduler='threads')\n",
    "\n",
    "print('🚀 Dask configurado para processamento de Big Data')\n",
    "print('💡 Datasets maiores que RAM serão processados automaticamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados com Dask (lazy loading)\n",
    "print('📂 Carregando dados com Dask...')\n",
    "\n",
    "# Carregar apenas colunas essenciais para economizar memória\n",
    "colunas_essenciais = [\n",
    "    'internal_store_id', \n",
    "    'internal_product_id', \n",
    "    'transaction_date', \n",
    "    'quantity',\n",
    "    'gross_value',\n",
    "    'distributor_id'\n",
    "]\n",
    "\n",
    "# Dask lê o arquivo sem carregar na memória\n",
    "transacoes_dask = dd.read_parquet(\n",
    "    '../data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet',\n",
    "    columns=colunas_essenciais\n",
    ")\n",
    "\n",
    "print(f'📊 Dados carregados (lazy): {transacoes_dask.shape[0].compute():,} registros')\n",
    "print(f'📊 Colunas: {list(transacoes_dask.columns)}')\n",
    "print(f'🧠 Memória: Não carregado na RAM ainda (lazy evaluation)')\n",
    "\n",
    "# Renomear colunas\n",
    "transacoes_dask = transacoes_dask.rename(columns={\n",
    "    'internal_store_id': 'pdv_id',\n",
    "    'internal_product_id': 'produto_id',\n",
    "    'transaction_date': 'data',\n",
    "    'quantity': 'quantidade',\n",
    "    'gross_value': 'valor'\n",
    "})\n",
    "\n",
    "# Converter data para datetime\n",
    "transacoes_dask['data'] = dd.to_datetime(transacoes_dask['data'])\n",
    "\n",
    "print('✅ Estrutura de dados preparada (lazy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criação de Features Temporais com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar features temporais (ainda lazy)\n",
    "print('📅 Criando features temporais...')\n",
    "\n",
    "# Semana (usando map_partitions para operações customizadas)\n",
    "def add_week_features(df):\n",
    "    df = df.copy()\n",
    "    df['semana'] = df['data'].dt.to_period('W-MON').dt.start_time\n",
    "    df['mes'] = df['data'].dt.month\n",
    "    df['semana_ano'] = df['data'].dt.isocalendar().week\n",
    "    df['ano'] = df['data'].dt.year\n",
    "    return df\n",
    "\n",
    "transacoes_dask = transacoes_dask.map_partitions(\n",
    "    add_week_features, \n",
    "    meta=transacoes_dask._meta.assign(\n",
    "        semana=pd.Timestamp('2022-01-01'),\n",
    "        mes=1,\n",
    "        semana_ano=1,\n",
    "        ano=2022\n",
    "    )\n",
    ")\n",
    "\n",
    "print('✅ Features temporais adicionadas (lazy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agregação Semanal com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregação semanal usando Dask (processamento paralelo)\n",
    "print('🔄 Iniciando agregação semanal com Dask...')\n",
    "\n",
    "# Dask agrupa e processa em paralelo\n",
    "agregacao_semanal_dask = transacoes_dask.groupby(['semana', 'pdv_id', 'produto_id']).agg({\n",
    "    'quantidade': ['sum', 'count'],\n",
    "    'valor': 'sum',\n",
    "    'distributor_id': 'first'\n",
    "})\n",
    "\n",
    "# Flatten columns\n",
    "agregacao_semanal_dask.columns = [\n",
    "    'quantidade', 'num_transacoes', 'valor', 'distributor_id'\n",
    "]\n",
    "\n",
    "# Reset index\n",
    "agregacao_semanal_dask = agregacao_semanal_dask.reset_index()\n",
    "\n",
    "print('🔄 Agregação configurada (lazy). Executando...')\n",
    "\n",
    "# EXECUTAR a agregação (aqui que realmente processa)\n",
    "agregacao_semanal = agregacao_semanal_dask.compute()\n",
    "\n",
    "print(f'📊 Agregação semanal concluída: {agregacao_semanal.shape}')\n",
    "print(f'   • Combinações semana/PDV/produto: {len(agregacao_semanal):,}')\n",
    "print(f'   • PDVs únicos: {agregacao_semanal[\"pdv_id\"].nunique():,}')\n",
    "print(f'   • Produtos únicos: {agregacao_semanal[\"produto_id\"].nunique():,}')\n",
    "\n",
    "# Converter de volta para Dask para próximas operações\n",
    "agregacao_semanal_dask = dd.from_pandas(agregacao_semanal, npartitions=4)\n",
    "\n",
    "del transacoes_dask  # Liberar memória\n",
    "print('✅ Dados de transação liberados da memória')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid Inteligente com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estratégia Grid Inteligente usando Dask\n",
    "print('🎯 Criando Grid Inteligente com Dask...')\n",
    "\n",
    "# Obter combinações ativas e semanas únicas\n",
    "combinacoes_ativas = agregacao_semanal[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "semanas_unicas = sorted(agregacao_semanal['semana'].unique())\n",
    "\n",
    "print(f'   • Combinações ativas: {len(combinacoes_ativas):,}')\n",
    "print(f'   • Semanas: {len(semanas_unicas)}')\n",
    "print(f'   • Total registros no grid: {len(combinacoes_ativas) * len(semanas_unicas):,}')\n",
    "\n",
    "# Criar grid usando processamento em lotes otimizado\n",
    "def create_grid_batch(combo_batch, semanas):\n",
    "    \"\"\"Criar grid para um lote de combinações\"\"\"\n",
    "    import pandas as pd\n",
    "    from itertools import product\n",
    "    \n",
    "    # Criar produto cartesiano\n",
    "    grid_data = []\n",
    "    for _, row in combo_batch.iterrows():\n",
    "        for semana in semanas:\n",
    "            grid_data.append({\n",
    "                'semana': semana,\n",
    "                'pdv_id': row['pdv_id'],\n",
    "                'produto_id': row['produto_id']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(grid_data)\n",
    "\n",
    "# Processar em lotes usando Dask\n",
    "batch_size = 5000\n",
    "grid_parts = []\n",
    "\n",
    "for i in range(0, len(combinacoes_ativas), batch_size):\n",
    "    batch = combinacoes_ativas.iloc[i:i+batch_size]\n",
    "    print(f'   📦 Lote {i//batch_size + 1}: {len(batch)} combinações')\n",
    "    \n",
    "    # Criar grid para este lote\n",
    "    batch_grid = create_grid_batch(batch, semanas_unicas)\n",
    "    \n",
    "    # Converter para Dask DataFrame\n",
    "    batch_grid_dask = dd.from_pandas(batch_grid, npartitions=2)\n",
    "    \n",
    "    # Merge com vendas reais\n",
    "    batch_merged = batch_grid_dask.merge(\n",
    "        agregacao_semanal_dask,\n",
    "        on=['semana', 'pdv_id', 'produto_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Preencher zeros\n",
    "    batch_merged['quantidade'] = batch_merged['quantidade'].fillna(0)\n",
    "    batch_merged['valor'] = batch_merged['valor'].fillna(0)\n",
    "    batch_merged['num_transacoes'] = batch_merged['num_transacoes'].fillna(0)\n",
    "    \n",
    "    # Computar e armazenar\n",
    "    grid_parts.append(batch_merged.compute())\n",
    "\n",
    "# Concatenar todas as partes\n",
    "print('🔗 Concatenando grid completo...')\n",
    "dados_completos = pd.concat(grid_parts, ignore_index=True)\n",
    "\n",
    "print(f'✅ Grid Inteligente criado: {dados_completos.shape}')\n",
    "print(f'   • Zeros: {(dados_completos[\"quantidade\"] == 0).sum():,} ({(dados_completos[\"quantidade\"] == 0).mean()*100:.1f}%)')\n",
    "print(f'   • Não-zeros: {(dados_completos[\"quantidade\"] > 0).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Features Avançadas com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para Dask para features avançadas\n",
    "dados_dask = dd.from_pandas(dados_completos, npartitions=8)\n",
    "\n",
    "print('🚀 Criando features avançadas com Dask...')\n",
    "\n",
    "# Ordenar dados\n",
    "dados_dask = dados_dask.set_index(['pdv_id', 'produto_id', 'semana']).sort_index()\n",
    "\n",
    "# Features temporais\n",
    "def add_temporal_features(df):\n",
    "    df = df.copy()\n",
    "    df['mes'] = df.index.get_level_values('semana').month\n",
    "    df['semana_ano'] = df.index.get_level_values('semana').isocalendar().week\n",
    "    df['mes_sin'] = np.sin(2 * np.pi * df['mes'] / 12)\n",
    "    df['mes_cos'] = np.cos(2 * np.pi * df['mes'] / 12)\n",
    "    return df\n",
    "\n",
    "dados_dask = dados_dask.map_partitions(\n",
    "    add_temporal_features,\n",
    "    meta=dados_dask._meta.assign(\n",
    "        mes=1, semana_ano=1, mes_sin=0.0, mes_cos=1.0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Features de lag usando Dask\n",
    "print('⏰ Criando features de lag...')\n",
    "for lag in [1, 2, 3, 4]:\n",
    "    dados_dask[f'quantidade_lag_{lag}'] = (\n",
    "        dados_dask.groupby(level=[0, 1])['quantidade']\n",
    "        .shift(lag)\n",
    "    )\n",
    "\n",
    "# Rolling features\n",
    "print('📊 Criando rolling features...')\n",
    "dados_dask['quantidade_media_4w'] = (\n",
    "    dados_dask.groupby(level=[0, 1])['quantidade']\n",
    "    .rolling(window=4, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "dados_dask['quantidade_std_4w'] = (\n",
    "    dados_dask.groupby(level=[0, 1])['quantidade']\n",
    "    .rolling(window=4, min_periods=1)\n",
    "    .std()\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Features categóricas\n",
    "def add_categorical_features(df):\n",
    "    df = df.copy()\n",
    "    df['pdv_hash'] = df.index.get_level_values('pdv_id').astype(str).map(hash) % 100\n",
    "    df['produto_hash'] = df.index.get_level_values('produto_id').astype(str).map(hash) % 100\n",
    "    return df\n",
    "\n",
    "dados_dask = dados_dask.map_partitions(\n",
    "    add_categorical_features,\n",
    "    meta=dados_dask._meta.assign(pdv_hash=1, produto_hash=1)\n",
    ")\n",
    "\n",
    "print('✅ Features avançadas configuradas (lazy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execução e Limpeza Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar todas as transformações\n",
    "print('🔄 Executando todas as transformações...')\n",
    "dados_final = dados_dask.compute()\n",
    "\n",
    "# Reset index\n",
    "dados_final = dados_final.reset_index()\n",
    "\n",
    "print(f'📊 Dados processados: {dados_final.shape}')\n",
    "\n",
    "# Limpeza: remover registros sem lag_4\n",
    "print('🧹 Aplicando limpeza final...')\n",
    "dados_limpos = dados_final[dados_final['quantidade_lag_4'].notna()].copy()\n",
    "\n",
    "print(f'✅ Dados limpos: {dados_limpos.shape}')\n",
    "print(f'   • Período: {dados_limpos[\"semana\"].min()} até {dados_limpos[\"semana\"].max()}')\n",
    "print(f'   • Semanas: {dados_limpos[\"semana\"].nunique()}')\n",
    "print(f'   • Features: {len(dados_limpos.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Salvamento e Metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar dataset final\n",
    "print('💾 Salvando dataset final...')\n",
    "dados_limpos.to_csv('../data/dados_features_completo.csv', index=False)\n",
    "\n",
    "# Salvar também em parquet (mais eficiente)\n",
    "dados_limpos.to_parquet('../data/dados_features_completo.parquet', index=False)\n",
    "\n",
    "print('✅ Dataset salvo em CSV e Parquet')\n",
    "\n",
    "# Metadados\n",
    "import pickle\n",
    "\n",
    "metadata = {\n",
    "    'data_processamento': pd.Timestamp.now(),\n",
    "    'total_registros': len(dados_limpos),\n",
    "    'total_features': len(dados_limpos.columns),\n",
    "    'combinacoes_pdv_produto': dados_limpos[['pdv_id', 'produto_id']].drop_duplicates().shape[0],\n",
    "    'semanas_cobertas': dados_limpos['semana'].nunique(),\n",
    "    'periodo_treino': f\"{dados_limpos['semana'].min()} a {dados_limpos['semana'].max()}\",\n",
    "    'estrategia': 'Grid Inteligente com Dask - Big Data Optimized',\n",
    "    'features_criadas': list(dados_limpos.columns),\n",
    "    'tecnologia': 'Dask for Out-of-Core Processing',\n",
    "    'memoria_maxima_usada': 'Limitada pelo número de partições'\n",
    "}\n",
    "\n",
    "with open('../data/feature_engineering_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print('📋 Metadados salvos')\n",
    "\n",
    "# Estatísticas finais\n",
    "print('\\n🎉 FEATURE ENGINEERING COM DASK CONCLUÍDO!')\n",
    "print('=' * 60)\n",
    "print(f'📊 Dataset final: {dados_limpos.shape}')\n",
    "print(f'💾 Arquivos salvos:')\n",
    "print('   • dados_features_completo.csv')\n",
    "print('   • dados_features_completo.parquet')\n",
    "print('   • feature_engineering_metadata.pkl')\n",
    "print(f'\\n🏷️ Features principais:')\n",
    "features_importantes = ['quantidade', 'quantidade_lag_1', 'quantidade_lag_2', \n",
    "                       'quantidade_lag_4', 'quantidade_media_4w', 'mes_sin', 'mes_cos']\n",
    "for feat in features_importantes:\n",
    "    if feat in dados_limpos.columns:\n",
    "        print(f'   ✅ {feat}')\n",
    "\n",
    "print('\\n🚀 Pronto para Modelagem com dados otimizados!')\n",
    "print(f'📈 Distribuição target:')\n",
    "print(f'   • Zeros: {(dados_limpos[\"quantidade\"] == 0).sum():,} ({(dados_limpos[\"quantidade\"] == 0).mean()*100:.1f}%)')\n",
    "print(f'   • Não-zeros: {(dados_limpos[\"quantidade\"] > 0).sum():,} ({(dados_limpos[\"quantidade\"] > 0).mean()*100:.1f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}