{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering com Dask (Otimizado para Big Data)\n",
    "\n",
    "Este notebook implementa feature engineering usando **Dask**, uma biblioteca projetada para processar datasets maiores que a RAM disponÃ­vel.\n",
    "\n",
    "## Vantagens do Dask:\n",
    "- âœ… **Processamento Out-of-Core**: Datasets maiores que a RAM\n",
    "- âœ… **ParalelizaÃ§Ã£o AutomÃ¡tica**: Usa todos os cores da CPU\n",
    "- âœ… **API Similar ao Pandas**: FÃ¡cil migraÃ§Ã£o\n",
    "- âœ… **ComputaÃ§Ã£o Lazy**: Executa apenas quando necessÃ¡rio\n",
    "- âœ… **Escalabilidade**: Funciona em clusters distribuÃ­dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Dask configurado para processamento de Big Data\n",
      "ğŸ’¡ Datasets maiores que RAM serÃ£o processados automaticamente\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar Dask\n",
    "from dask.distributed import Client, progress\n",
    "import dask\n",
    "\n",
    "# Configurar para usar threads (melhor para I/O)\n",
    "dask.config.set(scheduler='threads')\n",
    "\n",
    "print('ğŸš€ Dask configurado para processamento de Big Data')\n",
    "print('ğŸ’¡ Datasets maiores que RAM serÃ£o processados automaticamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Carregando dados com Dask...\n",
      "ğŸ“Š Dados carregados (lazy): 6,560,698 registros\n",
      "ğŸ“Š Colunas: ['internal_store_id', 'internal_product_id', 'transaction_date', 'quantity', 'gross_value', 'distributor_id']\n",
      "ğŸ§  MemÃ³ria: NÃ£o carregado na RAM ainda (lazy evaluation)\n",
      "âœ… Estrutura de dados preparada (lazy)\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com Dask (lazy loading)\n",
    "print('ğŸ“‚ Carregando dados com Dask...')\n",
    "\n",
    "# Carregar apenas colunas essenciais para economizar memÃ³ria\n",
    "colunas_essenciais = [\n",
    "    'internal_store_id', \n",
    "    'internal_product_id', \n",
    "    'transaction_date', \n",
    "    'quantity',\n",
    "    'gross_value',\n",
    "    'distributor_id'\n",
    "]\n",
    "\n",
    "# Dask lÃª o arquivo sem carregar na memÃ³ria\n",
    "transacoes_dask = dd.read_parquet(\n",
    "    '../data/part-00000-tid-5196563791502273604-c90d3a24-52f2-4955-b4ec-fb143aae74d8-4-1-c000.snappy.parquet',\n",
    "    columns=colunas_essenciais\n",
    ")\n",
    "\n",
    "print(f'ğŸ“Š Dados carregados (lazy): {transacoes_dask.shape[0].compute():,} registros')\n",
    "print(f'ğŸ“Š Colunas: {list(transacoes_dask.columns)}')\n",
    "print(f'ğŸ§  MemÃ³ria: NÃ£o carregado na RAM ainda (lazy evaluation)')\n",
    "\n",
    "# Renomear colunas\n",
    "transacoes_dask = transacoes_dask.rename(columns={\n",
    "    'internal_store_id': 'pdv_id',\n",
    "    'internal_product_id': 'produto_id',\n",
    "    'transaction_date': 'data',\n",
    "    'quantity': 'quantidade',\n",
    "    'gross_value': 'valor'\n",
    "})\n",
    "\n",
    "# Converter data para datetime\n",
    "transacoes_dask['data'] = dd.to_datetime(transacoes_dask['data'])\n",
    "\n",
    "print('âœ… Estrutura de dados preparada (lazy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CriaÃ§Ã£o de Features Temporais com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… Criando features temporais...\n",
      "âœ… Features temporais adicionadas (lazy)\n"
     ]
    }
   ],
   "source": [
    "# Adicionar features temporais (ainda lazy)\n",
    "print('ğŸ“… Criando features temporais...')\n",
    "\n",
    "# Semana (usando map_partitions para operaÃ§Ãµes customizadas)\n",
    "def add_week_features(df):\n",
    "    df = df.copy()\n",
    "    df['semana'] = df['data'].dt.to_period('W-MON').dt.start_time\n",
    "    df['mes'] = df['data'].dt.month\n",
    "    df['semana_ano'] = df['data'].dt.isocalendar().week\n",
    "    df['ano'] = df['data'].dt.year\n",
    "    return df\n",
    "\n",
    "transacoes_dask = transacoes_dask.map_partitions(\n",
    "    add_week_features, \n",
    "    meta=transacoes_dask._meta.assign(\n",
    "        semana=pd.Timestamp('2022-01-01'),\n",
    "        mes=1,\n",
    "        semana_ano=1,\n",
    "        ano=2022\n",
    "    )\n",
    ")\n",
    "\n",
    "print('âœ… Features temporais adicionadas (lazy)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AgregaÃ§Ã£o Semanal com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Iniciando agregaÃ§Ã£o semanal com Dask...\n",
      "ğŸ”„ AgregaÃ§Ã£o configurada (lazy). Executando...\n",
      "ğŸ“Š AgregaÃ§Ã£o semanal concluÃ­da: (6241315, 7)\n",
      "   â€¢ CombinaÃ§Ãµes semana/PDV/produto: 6,241,315\n",
      "   â€¢ PDVs Ãºnicos: 15,086\n",
      "   â€¢ Produtos Ãºnicos: 7,092\n",
      "âœ… Dados de transaÃ§Ã£o liberados da memÃ³ria\n"
     ]
    }
   ],
   "source": [
    "# AgregaÃ§Ã£o semanal usando Dask (processamento paralelo)\n",
    "print('ğŸ”„ Iniciando agregaÃ§Ã£o semanal com Dask...')\n",
    "\n",
    "# Dask agrupa e processa em paralelo\n",
    "agregacao_semanal_dask = transacoes_dask.groupby(['semana', 'pdv_id', 'produto_id']).agg({\n",
    "    'quantidade': ['sum', 'count'],\n",
    "    'valor': 'sum',\n",
    "    'distributor_id': 'first'\n",
    "})\n",
    "\n",
    "# Flatten columns\n",
    "agregacao_semanal_dask.columns = [\n",
    "    'quantidade', 'num_transacoes', 'valor', 'distributor_id'\n",
    "]\n",
    "\n",
    "# Reset index\n",
    "agregacao_semanal_dask = agregacao_semanal_dask.reset_index()\n",
    "\n",
    "print('ğŸ”„ AgregaÃ§Ã£o configurada (lazy). Executando...')\n",
    "\n",
    "# EXECUTAR a agregaÃ§Ã£o (aqui que realmente processa)\n",
    "agregacao_semanal = agregacao_semanal_dask.compute()\n",
    "\n",
    "print(f'ğŸ“Š AgregaÃ§Ã£o semanal concluÃ­da: {agregacao_semanal.shape}')\n",
    "print(f'   â€¢ CombinaÃ§Ãµes semana/PDV/produto: {len(agregacao_semanal):,}')\n",
    "print(f'   â€¢ PDVs Ãºnicos: {agregacao_semanal[\"pdv_id\"].nunique():,}')\n",
    "print(f'   â€¢ Produtos Ãºnicos: {agregacao_semanal[\"produto_id\"].nunique():,}')\n",
    "\n",
    "# Converter de volta para Dask para prÃ³ximas operaÃ§Ãµes\n",
    "agregacao_semanal_dask = dd.from_pandas(agregacao_semanal, npartitions=4)\n",
    "\n",
    "del transacoes_dask  # Liberar memÃ³ria\n",
    "print('âœ… Dados de transaÃ§Ã£o liberados da memÃ³ria')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid Inteligente com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Criando Grid Inteligente com Dask...\n",
      "   â€¢ CombinaÃ§Ãµes ativas: 1,044,310\n",
      "   â€¢ Semanas: 53\n",
      "   â€¢ Total registros no grid: 55,348,430\n",
      "   ğŸ“¦ Lote 1: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 2: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 3: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 4: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 5: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 6: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 7: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 8: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 9: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 10: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 11: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 12: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 13: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 14: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 15: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 16: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 17: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 18: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 19: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 20: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 21: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 22: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 23: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 24: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 25: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 26: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 27: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 28: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 29: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 30: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 31: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 32: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 33: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 34: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 35: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 36: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 37: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 38: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 39: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 40: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 41: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 42: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 43: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 44: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 45: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 46: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 47: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 48: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 49: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 50: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 51: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 52: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 53: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 54: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 55: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 56: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 57: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 58: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 59: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 60: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 61: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 62: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 63: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 64: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 65: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 66: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 67: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 68: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 69: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 70: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 71: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 72: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 73: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 74: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 75: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 76: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 77: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 78: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 79: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 80: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 81: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 82: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 83: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 84: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 85: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 86: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 87: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 88: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 89: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 90: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 91: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 92: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 93: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 94: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 95: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 96: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 97: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 98: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 99: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 100: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 101: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 102: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 103: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 104: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 105: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 106: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 107: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 108: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 109: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 110: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 111: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 112: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 113: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 114: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 115: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 116: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 117: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 118: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 119: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 120: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 121: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 122: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 123: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 124: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 125: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 126: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 127: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 128: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 129: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 130: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 131: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 132: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 133: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 134: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 135: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 136: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 137: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 138: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 139: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 140: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 141: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 142: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 143: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 144: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 145: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 146: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 147: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 148: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 149: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 150: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 151: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 152: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 153: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 154: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 155: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 156: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 157: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 158: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 159: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 160: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 161: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 162: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 163: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 164: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 165: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 166: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 167: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 168: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 169: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 170: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 171: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 172: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 173: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 174: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 175: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 176: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 177: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 178: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 179: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 180: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 181: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 182: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 183: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 184: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 185: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 186: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 187: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 188: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 189: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 190: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 191: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 192: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 193: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 194: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 195: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 196: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 197: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 198: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 199: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 200: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 201: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 202: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 203: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 204: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 205: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 206: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 207: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 208: 5000 combinaÃ§Ãµes\n",
      "   ğŸ“¦ Lote 209: 4310 combinaÃ§Ãµes\n",
      "ğŸ”— Concatenando grid completo...\n",
      "âœ… Grid Inteligente criado: (55348430, 7)\n",
      "   â€¢ Zeros: 49,161,548 (88.8%)\n",
      "   â€¢ NÃ£o-zeros: 6,133,369\n"
     ]
    }
   ],
   "source": [
    "# EstratÃ©gia Grid Inteligente usando Dask\n",
    "print('ğŸ¯ Criando Grid Inteligente com Dask...')\n",
    "\n",
    "# Obter combinaÃ§Ãµes ativas e semanas Ãºnicas\n",
    "combinacoes_ativas = agregacao_semanal[['pdv_id', 'produto_id']].drop_duplicates()\n",
    "semanas_unicas = sorted(agregacao_semanal['semana'].unique())\n",
    "\n",
    "print(f'   â€¢ CombinaÃ§Ãµes ativas: {len(combinacoes_ativas):,}')\n",
    "print(f'   â€¢ Semanas: {len(semanas_unicas)}')\n",
    "print(f'   â€¢ Total registros no grid: {len(combinacoes_ativas) * len(semanas_unicas):,}')\n",
    "\n",
    "# Criar grid usando processamento em lotes otimizado\n",
    "def create_grid_batch(combo_batch, semanas):\n",
    "    \"\"\"Criar grid para um lote de combinaÃ§Ãµes\"\"\"\n",
    "    import pandas as pd\n",
    "    from itertools import product\n",
    "    \n",
    "    # Criar produto cartesiano\n",
    "    grid_data = []\n",
    "    for _, row in combo_batch.iterrows():\n",
    "        for semana in semanas:\n",
    "            grid_data.append({\n",
    "                'semana': semana,\n",
    "                'pdv_id': row['pdv_id'],\n",
    "                'produto_id': row['produto_id']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(grid_data)\n",
    "\n",
    "# Processar em lotes usando Dask\n",
    "batch_size = 5000\n",
    "grid_parts = []\n",
    "\n",
    "for i in range(0, len(combinacoes_ativas), batch_size):\n",
    "    batch = combinacoes_ativas.iloc[i:i+batch_size]\n",
    "    print(f'   ğŸ“¦ Lote {i//batch_size + 1}: {len(batch)} combinaÃ§Ãµes')\n",
    "    \n",
    "    # Criar grid para este lote\n",
    "    batch_grid = create_grid_batch(batch, semanas_unicas)\n",
    "    \n",
    "    # Converter para Dask DataFrame\n",
    "    batch_grid_dask = dd.from_pandas(batch_grid, npartitions=2)\n",
    "    \n",
    "    # Merge com vendas reais\n",
    "    batch_merged = batch_grid_dask.merge(\n",
    "        agregacao_semanal_dask,\n",
    "        on=['semana', 'pdv_id', 'produto_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Preencher zeros\n",
    "    batch_merged['quantidade'] = batch_merged['quantidade'].fillna(0)\n",
    "    batch_merged['valor'] = batch_merged['valor'].fillna(0)\n",
    "    batch_merged['num_transacoes'] = batch_merged['num_transacoes'].fillna(0)\n",
    "    \n",
    "    # Computar e armazenar\n",
    "    grid_parts.append(batch_merged.compute())\n",
    "\n",
    "# Concatenar todas as partes\n",
    "print('ğŸ”— Concatenando grid completo...')\n",
    "dados_completos = pd.concat(grid_parts, ignore_index=True)\n",
    "\n",
    "print(f'âœ… Grid Inteligente criado: {dados_completos.shape}')\n",
    "print(f'   â€¢ Zeros: {(dados_completos[\"quantidade\"] == 0).sum():,} ({(dados_completos[\"quantidade\"] == 0).mean()*100:.1f}%)')\n",
    "print(f'   â€¢ NÃ£o-zeros: {(dados_completos[\"quantidade\"] > 0).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Features AvanÃ§adas com Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Convertendo para Polars (mais eficiente que pandas para big data)...\n",
      "ğŸš€ Criando features avanÃ§adas com Polars...\n",
      "ğŸ”„ Ordenando dados por PDV, produto e semana...\n",
      "ğŸ“… Criando features temporais...\n",
      "â° Criando features de lag...\n",
      "   ğŸ“‹ Lag 1...\n",
      "   ğŸ“‹ Lag 2...\n",
      "   ğŸ“‹ Lag 3...\n",
      "   ğŸ“‹ Lag 4...\n",
      "ğŸ“Š Criando rolling features...\n",
      "ğŸ·ï¸ Criando features categÃ³ricas...\n",
      "ğŸ”— Criando features de interaÃ§Ã£o...\n",
      "ğŸ“ˆ Criando features estatÃ­sticas histÃ³ricas...\n",
      "âœ… Features avanÃ§adas criadas com Polars!\n",
      "ğŸ“Š Shape final: (55348430, 26)\n",
      "ğŸ’¾ MemÃ³ria otimizada pelo Polars\n"
     ]
    }
   ],
   "source": [
    "# Usando Polars para eficiÃªncia mÃ¡xima com big data\n",
    "print('ğŸš€ Convertendo para Polars (mais eficiente que pandas para big data)...')\n",
    "import polars as pl\n",
    "\n",
    "# Converter para Polars (muito mais eficiente)\n",
    "dados_polars = pl.from_pandas(dados_completos)\n",
    "\n",
    "print('ğŸš€ Criando features avanÃ§adas com Polars...')\n",
    "\n",
    "# Ordenar para garantir consistÃªncia das features temporais\n",
    "print('ğŸ”„ Ordenando dados por PDV, produto e semana...')\n",
    "dados_polars = dados_polars.sort(['pdv_id', 'produto_id', 'semana'])\n",
    "\n",
    "# Features temporais (Polars Ã© muito mais rÃ¡pido)\n",
    "print('ğŸ“… Criando features temporais...')\n",
    "dados_polars = dados_polars.with_columns([\n",
    "    pl.col('semana').dt.month().alias('mes'),\n",
    "    pl.col('semana').dt.week().alias('semana_ano'),\n",
    "    (2 * np.pi * pl.col('semana').dt.month() / 12).sin().alias('mes_sin'),\n",
    "    (2 * np.pi * pl.col('semana').dt.month() / 12).cos().alias('mes_cos')\n",
    "])\n",
    "\n",
    "# Features de lag (Polars faz isso de forma super eficiente)\n",
    "print('â° Criando features de lag...')\n",
    "lag_exprs = []\n",
    "for lag in [1, 2, 3, 4]:\n",
    "    print(f'   ğŸ“‹ Lag {lag}...')\n",
    "    lag_exprs.append(\n",
    "        pl.col('quantidade').shift(lag).over(['pdv_id', 'produto_id']).alias(f'quantidade_lag_{lag}')\n",
    "    )\n",
    "\n",
    "dados_polars = dados_polars.with_columns(lag_exprs)\n",
    "\n",
    "# Rolling features (Polars tem excelente suporte para rolling windows)\n",
    "print('ğŸ“Š Criando rolling features...')\n",
    "rolling_exprs = [\n",
    "    pl.col('quantidade').rolling_mean(window_size=4, min_periods=1).over(['pdv_id', 'produto_id']).alias('quantidade_media_4w'),\n",
    "    pl.col('quantidade').rolling_std(window_size=4, min_periods=1).over(['pdv_id', 'produto_id']).fill_null(0).alias('quantidade_std_4w'),\n",
    "    pl.col('quantidade').rolling_max(window_size=4, min_periods=1).over(['pdv_id', 'produto_id']).alias('quantidade_max_4w'),\n",
    "    pl.col('quantidade').rolling_min(window_size=4, min_periods=1).over(['pdv_id', 'produto_id']).alias('quantidade_min_4w'),\n",
    "]\n",
    "\n",
    "dados_polars = dados_polars.with_columns(rolling_exprs)\n",
    "\n",
    "# Features categÃ³ricas (usando hash de forma eficiente)\n",
    "print('ğŸ·ï¸ Criando features categÃ³ricas...')\n",
    "dados_polars = dados_polars.with_columns([\n",
    "    (pl.col('pdv_id').cast(pl.Utf8).hash() % 100).alias('pdv_hash'),\n",
    "    (pl.col('produto_id').cast(pl.Utf8).hash() % 100).alias('produto_hash')\n",
    "])\n",
    "\n",
    "# Features de interaÃ§Ã£o\n",
    "print('ğŸ”— Criando features de interaÃ§Ã£o...')\n",
    "dados_polars = dados_polars.with_columns([\n",
    "    (pl.col('pdv_hash') * 100 + pl.col('produto_hash')).alias('pdv_produto_hash')\n",
    "])\n",
    "\n",
    "# Features estatÃ­sticas histÃ³ricas (Polars Ã© muito eficiente em group_by)\n",
    "print('ğŸ“ˆ Criando features estatÃ­sticas histÃ³ricas...')\n",
    "stats_historicas = dados_polars.group_by(['pdv_id', 'produto_id']).agg([\n",
    "    pl.col('quantidade').mean().alias('hist_mean'),\n",
    "    pl.col('quantidade').std().alias('hist_std'),\n",
    "    pl.col('quantidade').max().alias('hist_max'),\n",
    "    pl.col('quantidade').count().alias('hist_count')\n",
    "])\n",
    "\n",
    "# Join com dataset principal (Polars otimiza joins automaticamente)\n",
    "dados_polars = dados_polars.join(\n",
    "    stats_historicas, \n",
    "    on=['pdv_id', 'produto_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print('âœ… Features avanÃ§adas criadas com Polars!')\n",
    "print(f'ğŸ“Š Shape final: {dados_polars.shape}')\n",
    "print(f'ğŸ’¾ MemÃ³ria otimizada pelo Polars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ExecuÃ§Ã£o e Limpeza Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Aplicando limpeza final com Polars...\n",
      "âœ… Dados limpos: (51171190, 26)\n",
      "   â€¢ PerÃ­odo: 2022-01-25 00:00:00 atÃ© 2022-12-27 00:00:00\n",
      "   â€¢ Semanas: 49\n",
      "   â€¢ Features: 26\n",
      "ğŸ§  Estimated memory usage: 9974.253155708313 MB\n"
     ]
    }
   ],
   "source": [
    "# Limpeza final com Polars (muito mais eficiente)\n",
    "print('ğŸ§¹ Aplicando limpeza final com Polars...')\n",
    "\n",
    "# Remover registros sem lag_4 (onde lag_4 Ã© null)\n",
    "dados_limpos = dados_polars.filter(pl.col('quantidade_lag_4').is_not_null())\n",
    "\n",
    "print(f'âœ… Dados limpos: {dados_limpos.shape}')\n",
    "print(f'   â€¢ PerÃ­odo: {dados_limpos.select(pl.col(\"semana\").min()).item()} atÃ© {dados_limpos.select(pl.col(\"semana\").max()).item()}')\n",
    "print(f'   â€¢ Semanas: {dados_limpos.select(pl.col(\"semana\").n_unique()).item()}')\n",
    "print(f'   â€¢ Features: {len(dados_limpos.columns)}')\n",
    "\n",
    "# Verificar memÃ³ria usada\n",
    "print(f'ğŸ§  Estimated memory usage: {dados_limpos.estimated_size(\"mb\")} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Salvamento e Metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Salvando dataset final com Polars...\n",
      "âœ… Dataset salvo em CSV e Parquet\n",
      "ğŸ“‹ Metadados salvos\n",
      "\n",
      "ğŸ‰ FEATURE ENGINEERING COM DASK + POLARS CONCLUÃDO!\n",
      "============================================================\n",
      "ğŸ“Š Dataset final: (51171190, 26)\n",
      "ğŸ’¾ Arquivos salvos:\n",
      "   â€¢ dados_features_completo.csv\n",
      "   â€¢ dados_features_completo.parquet\n",
      "   â€¢ feature_engineering_metadata.pkl\n",
      "\n",
      "ğŸ·ï¸ Features principais:\n",
      "   âœ… quantidade\n",
      "   âœ… quantidade_lag_1\n",
      "   âœ… quantidade_lag_2\n",
      "   âœ… quantidade_lag_4\n",
      "   âœ… quantidade_media_4w\n",
      "   âœ… mes_sin\n",
      "   âœ… mes_cos\n",
      "\n",
      "ğŸš€ Pronto para Modelagem com dados super-otimizados!\n",
      "ğŸ“ˆ DistribuiÃ§Ã£o target:\n",
      "   â€¢ Zeros: 45,254,351 (88.4%)\n",
      "   â€¢ NÃ£o-zeros: 5,916,839 (11.6%)\n",
      "ğŸ§  MemÃ³ria total usada: 9974.253155708313 MB\n",
      "ğŸ’¡ Polars otimizou automaticamente o uso de memÃ³ria!\n"
     ]
    }
   ],
   "source": [
    "# Salvar dataset final usando Polars (muito mais eficiente)\n",
    "print('ğŸ’¾ Salvando dataset final com Polars...')\n",
    "\n",
    "# Polars pode salvar diretamente para CSV e Parquet de forma otimizada\n",
    "dados_limpos.write_csv('../data/dados_features_completo.csv')\n",
    "dados_limpos.write_parquet('../data/dados_features_completo.parquet')\n",
    "\n",
    "print('âœ… Dataset salvo em CSV e Parquet')\n",
    "\n",
    "# Converter uma amostra para pandas para metadados compatÃ­veis\n",
    "dados_sample = dados_limpos.head(1000).to_pandas()\n",
    "\n",
    "# Metadados\n",
    "import pickle\n",
    "\n",
    "metadata = {\n",
    "    'data_processamento': pd.Timestamp.now(),\n",
    "    'total_registros': dados_limpos.shape[0],\n",
    "    'total_features': len(dados_limpos.columns),\n",
    "    'combinacoes_pdv_produto': dados_limpos.select([pl.col('pdv_id'), pl.col('produto_id')]).unique().shape[0],\n",
    "    'semanas_cobertas': dados_limpos.select(pl.col(\"semana\").n_unique()).item(),\n",
    "    'periodo_treino': f\"{dados_limpos.select(pl.col('semana').min()).item()} a {dados_limpos.select(pl.col('semana').max()).item()}\",\n",
    "    'estrategia': 'Grid Inteligente com Dask + Polars - Big Data Optimized',\n",
    "    'features_criadas': dados_limpos.columns,\n",
    "    'tecnologia': 'Dask + Polars for Maximum Performance',\n",
    "    'memoria_otimizada': f'{dados_limpos.estimated_size(\"mb\")} MB'\n",
    "}\n",
    "\n",
    "with open('../data/feature_engineering_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print('ğŸ“‹ Metadados salvos')\n",
    "\n",
    "# EstatÃ­sticas finais usando Polars (muito mais rÃ¡pido)\n",
    "print('\\nğŸ‰ FEATURE ENGINEERING COM DASK + POLARS CONCLUÃDO!')\n",
    "print('=' * 60)\n",
    "print(f'ğŸ“Š Dataset final: {dados_limpos.shape}')\n",
    "print(f'ğŸ’¾ Arquivos salvos:')\n",
    "print('   â€¢ dados_features_completo.csv')\n",
    "print('   â€¢ dados_features_completo.parquet')  \n",
    "print('   â€¢ feature_engineering_metadata.pkl')\n",
    "print(f'\\nğŸ·ï¸ Features principais:')\n",
    "features_importantes = ['quantidade', 'quantidade_lag_1', 'quantidade_lag_2', \n",
    "                       'quantidade_lag_4', 'quantidade_media_4w', 'mes_sin', 'mes_cos']\n",
    "for feat in features_importantes:\n",
    "    if feat in dados_limpos.columns:\n",
    "        print(f'   âœ… {feat}')\n",
    "\n",
    "print('\\nğŸš€ Pronto para Modelagem com dados super-otimizados!')\n",
    "print(f'ğŸ“ˆ DistribuiÃ§Ã£o target:')\n",
    "\n",
    "# Calcular estatÃ­sticas com Polars (muito mais eficiente)\n",
    "zeros = dados_limpos.select((pl.col(\"quantidade\") == 0).sum()).item()\n",
    "total = dados_limpos.shape[0]\n",
    "nao_zeros = total - zeros\n",
    "\n",
    "print(f'   â€¢ Zeros: {zeros:,} ({zeros/total*100:.1f}%)')\n",
    "print(f'   â€¢ NÃ£o-zeros: {nao_zeros:,} ({nao_zeros/total*100:.1f}%)')\n",
    "print(f'ğŸ§  MemÃ³ria total usada: {dados_limpos.estimated_size(\"mb\")} MB')\n",
    "print('ğŸ’¡ Polars otimizou automaticamente o uso de memÃ³ria!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
