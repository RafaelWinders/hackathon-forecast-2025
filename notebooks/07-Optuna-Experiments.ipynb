{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Modelagem e Experimentos (Di√°rio de Bordo)\n",
    "\n",
    "**üéØ PROP√ìSITO DESTE NOTEBOOK:**\n",
    "Este notebook documenta o nosso processo completo de explora√ß√£o e sele√ß√£o de modelos. Aqui comparamos Baselines, RandomForest, LightGBM e XGBoost para justificar a nossa escolha final.\n",
    "\n",
    "**üìä RESULTADO PRINCIPAL:**\n",
    "- Testamos m√∫ltiplos modelos (Baselines, Random Forest, LightGBM, XGBoost)  \n",
    "- **LightGBM** venceu com **WMAPE: 15.25%** (91% melhor que baseline)\n",
    "- XGBoost foi marginalmente melhor, mas **inst√°vel** em produ√ß√£o\n",
    "- Este notebook cont√©m a **justificativa t√©cnica** da nossa decis√£o final\n",
    "\n",
    "**üöÄ PARA EXECUTAR A SOLU√á√ÉO FINAL:**\n",
    "Use o notebook `04-Final-Pipeline.ipynb` - ele cont√©m apenas o c√≥digo necess√°rio para treinar o modelo vencedor e gerar a submiss√£o.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos da Explora√ß√£o:\n",
    "1. **Carregamento dos Dados**: Carregar dataset com features processadas\n",
    "2. **Prepara√ß√£o para ML**: Dividir dados em treino/valida√ß√£o, preparar features  \n",
    "3. **Baseline Models**: Implementar modelos simples como refer√™ncia\n",
    "4. **Advanced Models**: Testar modelos avan√ßados (LightGBM, XGBoost, etc.)\n",
    "5. **Compara√ß√£o Rigorosa**: Avaliar todos os modelos usando m√©tricas adequadas\n",
    "6. **Sele√ß√£o Final**: Escolher o modelo mais robusto para produ√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Advanced ML\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping  # CORRE√á√ÉO: Importar early_stopping\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('üìö Bibliotecas carregadas com sucesso!')\n",
    "print('üéØ Iniciando fase de Modelagem e Treinamento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados com features processadas\n",
    "print('üìÇ Carregando dados processados...')\n",
    "\n",
    "# Verificar se os arquivos essenciais existem\n",
    "import os\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',  # Usar parquet (mais r√°pido)\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print('‚ùå Arquivos n√£o encontrados:')\n",
    "    for f in missing_files:\n",
    "        print(f'   ‚Ä¢ {f}')\n",
    "    print('\\nüîÑ Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\n",
    "else:\n",
    "    print('‚úÖ Todos os arquivos necess√°rios encontrados')\n",
    "    \n",
    "    # Carregar dados principais (usar parquet para velocidade)\n",
    "    print('üìä Carregando dataset (parquet)...')\n",
    "    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "    \n",
    "    # Carregar metadados\n",
    "    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    print(f'\\nüìä Dados carregados com sucesso:')\n",
    "    print(f'   ‚Ä¢ Shape: {dados.shape}')\n",
    "    print(f'   ‚Ä¢ Per√≠odo: {dados[\"semana\"].min()} at√© {dados[\"semana\"].max()}')\n",
    "    print(f'   ‚Ä¢ Features dispon√≠veis: {len(dados.columns)}')\n",
    "    print(f'   ‚Ä¢ Mem√≥ria: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   ‚Ä¢ Estrat√©gia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n",
    "    \n",
    "    print(f'\\nüîç Metadados do processamento:')\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n",
    "            print(f'   ‚Ä¢ {key}: {value}')\n",
    "    \n",
    "    print(f'\\n‚úÖ Pronto para modelagem!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepara√ß√£o dos Dados para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir vari√°vel target e features\n",
    "target = 'quantidade'\n",
    "\n",
    "# Features a excluir (n√£o devem ser usadas para predi√ß√£o)\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana',  # IDs e data\n",
    "    'quantidade',  # Target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informa√ß√£o do futuro\n",
    "]\n",
    "\n",
    "# Identificar features dispon√≠veis\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'üéØ Prepara√ß√£o dos dados:')\n",
    "print(f'   ‚Ä¢ Target: {target}')\n",
    "print(f'   ‚Ä¢ Features dispon√≠veis: {len(all_features)}')\n",
    "print(f'   ‚Ä¢ Features exclu√≠das: {len(exclude_features)}')\n",
    "\n",
    "# Verificar missing values nas features\n",
    "missing_features = dados[all_features].isnull().sum()\n",
    "missing_features = missing_features[missing_features > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\n‚ö†Ô∏è Features com valores missing:')\n",
    "    for feature, count in missing_features.head(10).items():\n",
    "        pct = (count / len(dados)) * 100\n",
    "        print(f'   ‚Ä¢ {feature}: {count:,} ({pct:.1f}%)')\n",
    "    \n",
    "    print(f'\\nüß† Estrat√©gia de Tratamento Inteligente:')\n",
    "    print('   ‚Ä¢ distributor_id (categ√≥rica): NaN ‚Üí -1 (venda direta)')\n",
    "    print('   ‚Ä¢ Features num√©ricas: NaN ‚Üí 0 (aus√™ncia = zero)')\n",
    "    print('   ‚Ä¢ LightGBM aprender√° padr√µes espec√≠ficos para valores -1/0')\n",
    "else:\n",
    "    print('\\n‚úÖ Nenhum valor missing nas features')\n",
    "\n",
    "print(f'\\nüìã Features finais para modelagem: {len(all_features)}')\n",
    "print('üí° Missing values ser√£o tratados como informa√ß√£o, n√£o removidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLU√á√ÉO CORRETA: Otimiza√ß√£o de Tipos de Dados (Downcasting)\n",
    "print('üìÖ Otimiza√ß√£o de Mem√≥ria + Divis√£o Temporal')\n",
    "print('üß† Estrat√©gia: Downcasting em vez de amostragem (preserva s√©ries temporais)')\n",
    "\n",
    "# PASSO 1: Inspecionar uso de mem√≥ria atual\n",
    "print(f'\\nüîç ANTES da otimiza√ß√£o:')\n",
    "memory_before = dados.memory_usage(deep=True).sum() / (1024**3)\n",
    "print(f'üíæ Mem√≥ria total: {memory_before:.2f} GB')\n",
    "\n",
    "# PASSO 2: Aplicar Downcasting Inteligente\n",
    "print(f'\\nüöÄ Aplicando Downcasting...')\n",
    "\n",
    "# Fazer uma c√≥pia para otimiza√ß√£o\n",
    "dados_sorted = dados.copy()\n",
    "\n",
    "# Otimizar colunas num√©ricas (inteiros e floats)\n",
    "for col in dados_sorted.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_sorted[col].dtype\n",
    "    \n",
    "    if dados_sorted[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='float')\n",
    "    \n",
    "    new_dtype = dados_sorted[col].dtype\n",
    "    if original_dtype != new_dtype:\n",
    "        print(f'   ‚Ä¢ {col}: {original_dtype} ‚Üí {new_dtype}')\n",
    "\n",
    "# Otimizar colunas categ√≥ricas\n",
    "for col in dados_sorted.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_sorted[col].nunique()\n",
    "        total_rows = len(dados_sorted)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores √∫nicos, usar category\n",
    "            dados_sorted[col] = dados_sorted[col].astype('category')\n",
    "            print(f'   ‚Ä¢ {col}: object ‚Üí category')\n",
    "\n",
    "print(f'‚úÖ Downcasting conclu√≠do!')\n",
    "\n",
    "# PASSO 3: Verificar resultado da otimiza√ß√£o\n",
    "memory_after = dados_sorted.memory_usage(deep=True).sum() / (1024**3)\n",
    "memory_reduction = (memory_before - memory_after) / memory_before * 100\n",
    "print(f'\\nüìä DEPOIS da otimiza√ß√£o:')\n",
    "print(f'üíæ Mem√≥ria total: {memory_after:.2f} GB')\n",
    "print(f'üéØ Redu√ß√£o: {memory_reduction:.1f}% ({memory_before-memory_after:.2f} GB economizados)')\n",
    "\n",
    "# PASSO 4: Divis√£o temporal (agora com dados otimizados)\n",
    "print(f'\\nüìÖ Divis√£o temporal dos dados (com mem√≥ria otimizada)...')\n",
    "\n",
    "# Ordenar por semana\n",
    "dados_sorted = dados_sorted.sort_values('semana')\n",
    "\n",
    "# Split temporal: semanas 1-48 treino, 49-52 valida√ß√£o\n",
    "semanas_unicas = sorted(dados_sorted['semana'].unique())\n",
    "print(f'üìä Total de semanas dispon√≠veis: {len(semanas_unicas)}')\n",
    "\n",
    "cutoff_week_idx = 48  # Primeiras 48 semanas para treino\n",
    "if len(semanas_unicas) >= cutoff_week_idx:\n",
    "    cutoff_week = semanas_unicas[cutoff_week_idx-1]\n",
    "    \n",
    "    # Criar m√°scaras (sem c√≥pia)\n",
    "    train_mask = dados_sorted['semana'] <= cutoff_week\n",
    "    val_mask = dados_sorted['semana'] > cutoff_week\n",
    "    \n",
    "    print(f'üìä Divis√£o dos dados:')\n",
    "    print(f'   ‚Ä¢ Treino: {train_mask.sum():,} registros ({train_mask.mean()*100:.1f}%)')\n",
    "    print(f'   ‚Ä¢ Valida√ß√£o: {val_mask.sum():,} registros ({val_mask.mean()*100:.1f}%)')\n",
    "    \n",
    "    # CORRE√á√ÉO: Tratamento de missing values com categorias\n",
    "    print(f'\\nüß† Tratamento inteligente de missing values (CORRIGIDO)...')\n",
    "    all_features = [col for col in dados_sorted.columns if col not in ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']]\n",
    "    \n",
    "    for col in all_features:\n",
    "        missing_count = dados_sorted[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            if col == 'distributor_id':\n",
    "                # SOLU√á√ÉO: Adicionar -1 ao \"menu\" de categorias primeiro\n",
    "                if dados_sorted[col].dtype.name == 'category':\n",
    "                    if -1 not in dados_sorted[col].cat.categories:\n",
    "                        dados_sorted[col] = dados_sorted[col].cat.add_categories([-1])\n",
    "                \n",
    "                # Agora pode preencher com -1 sem erro\n",
    "                dados_sorted[col] = dados_sorted[col].fillna(-1)\n",
    "                print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí -1 (venda direta)')\n",
    "                \n",
    "            elif dados_sorted[col].dtype.kind in ['i', 'u', 'f']:\n",
    "                # Num√©ricas: fillna funciona diretamente\n",
    "                dados_sorted[col] = dados_sorted[col].fillna(0)\n",
    "                print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí 0 (aus√™ncia)')\n",
    "    \n",
    "    # PASSO 5: Preparar dados para modelagem (agora deve funcionar!)\n",
    "    print(f'\\nüéØ Preparando dados para modelagem...')\n",
    "    X_train = dados_sorted.loc[train_mask, all_features]\n",
    "    y_train = dados_sorted.loc[train_mask, 'quantidade']\n",
    "    X_val = dados_sorted.loc[val_mask, all_features]\n",
    "    y_val = dados_sorted.loc[val_mask, 'quantidade']\n",
    "    \n",
    "    print(f'‚úÖ Dados preparados com sucesso:')\n",
    "    print(f'   ‚Ä¢ X_train shape: {X_train.shape}')\n",
    "    print(f'   ‚Ä¢ X_val shape: {X_val.shape}')\n",
    "    print(f'   ‚Ä¢ Mem√≥ria X_train: {X_train.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   ‚Ä¢ Mem√≥ria X_val: {X_val.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    \n",
    "    # Garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\nüéâ SUCESSO! Problema resolvido:')\n",
    "    print(f'   ‚úÖ Downcasting: {memory_reduction:.1f}% menos mem√≥ria')\n",
    "    print(f'   ‚úÖ Categorical fix: -1 adicionado ao \"menu\" de categorias')\n",
    "    print(f'   ‚úÖ S√©ries temporais preservadas integralmente')\n",
    "    \n",
    "else:\n",
    "    print(f'‚ö†Ô∏è Menos de 48 semanas dispon√≠veis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise Explorat√≥ria do Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise da distribui√ß√£o do target (usando dados otimizados)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribui√ß√£o geral (usando y_train que foi criado)\n",
    "axes[0,0].hist(y_train, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Distribui√ß√£o da Quantidade (Treino)')\n",
    "axes[0,0].set_xlabel('Quantidade')\n",
    "axes[0,0].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "# Log-scale\n",
    "non_zero_train = y_train[y_train > 0]\n",
    "axes[0,1].hist(np.log1p(non_zero_train), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('Distribui√ß√£o log(Quantidade + 1) - Apenas > 0')\n",
    "axes[0,1].set_xlabel('log(Quantidade + 1)')\n",
    "axes[0,1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "# Zeros vs Non-zeros\n",
    "zero_counts = [len(y_train[y_train == 0]), len(y_train[y_train > 0])]\n",
    "axes[1,0].pie(zero_counts, labels=['Zeros', 'N√£o-zeros'], autopct='%1.1f%%')\n",
    "axes[1,0].set_title('Propor√ß√£o Zeros vs N√£o-zeros')\n",
    "\n",
    "# Boxplot por semana (√∫ltimas 12 semanas) - usando dados_sorted com train_mask\n",
    "train_weeks_data = dados_sorted.loc[train_mask, ['semana', 'quantidade']]\n",
    "recent_weeks = sorted(train_weeks_data['semana'].unique())[-12:]\n",
    "recent_data = train_weeks_data[train_weeks_data['semana'].isin(recent_weeks)]\n",
    "\n",
    "# Criar boxplot manualmente para evitar problemas com groupby\n",
    "week_data = []\n",
    "week_labels = []\n",
    "for week in recent_weeks:\n",
    "    week_quantities = recent_data[recent_data['semana'] == week]['quantidade']\n",
    "    if len(week_quantities) > 0:\n",
    "        week_data.append(week_quantities.values)\n",
    "        week_labels.append(week.strftime('%m-%d'))\n",
    "\n",
    "if week_data:\n",
    "    axes[1,1].boxplot(week_data, labels=week_labels)\n",
    "    axes[1,1].set_title('Distribui√ß√£o por Semana (√öltimas 12)')\n",
    "    axes[1,1].set_xlabel('Semana')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Dados insuficientes\\npara boxplot', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Distribui√ß√£o por Semana')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas descritivas\n",
    "print('üìà Estat√≠sticas do Target (treino):')\n",
    "print(y_train.describe())\n",
    "\n",
    "print(f'\\nüéØ M√©tricas importantes:')\n",
    "print(f'   ‚Ä¢ Zeros: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ N√£o-zeros: {(y_train > 0).sum():,} ({(y_train > 0).mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ M√©dia (apenas > 0): {y_train[y_train > 0].mean():.2f}')\n",
    "print(f'   ‚Ä¢ Mediana (apenas > 0): {y_train[y_train > 0].median():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelos Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINES OTIMIZADOS PARA MEM√ìRIA\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Avalia um modelo usando m√∫ltiplas m√©tricas incluindo WMAPE\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # WMAPE - Weighted Mean Absolute Percentage Error (m√©trica oficial do challenge)\n",
    "    wmape = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R¬≤': r2,\n",
    "        'WMAPE': wmape\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Lista para armazenar resultados\n",
    "model_results = []\n",
    "\n",
    "print('üéØ Estabelecendo Baselines Robustos (OTIMIZADO PARA MEM√ìRIA)...')\n",
    "print('=' * 60)\n",
    "\n",
    "# BASELINE 1: Lag-1 (Valor da semana anterior)\n",
    "print('\\nüìä 1. Baseline Lag-1: previs√£o(semana_N) = real(semana_N-1)')\n",
    "if 'quantidade_lag_1' in dados_sorted.columns:\n",
    "    # Usar view em vez de copy para economia de mem√≥ria\n",
    "    lag1_pred = dados_sorted.loc[val_mask, 'quantidade_lag_1'].fillna(0).values\n",
    "    results_lag1 = evaluate_model(y_val.values, lag1_pred, 'Baseline Lag-1')\n",
    "    model_results.append(results_lag1)\n",
    "    print(f'   ‚úÖ WMAPE: {results_lag1[\"WMAPE\"]:.2f}%')\n",
    "else:\n",
    "    print('   ‚ùå Feature quantidade_lag_1 n√£o encontrada')\n",
    "\n",
    "# BASELINE 2: Lag-4 (Valor de 4 semanas atr√°s - sazonal mensal)\n",
    "print('\\nüìä 2. Baseline Lag-4: previs√£o(semana_N) = real(semana_N-4)')\n",
    "if 'quantidade_lag_4' in dados_sorted.columns:\n",
    "    lag4_pred = dados_sorted.loc[val_mask, 'quantidade_lag_4'].fillna(0).values\n",
    "    results_lag4 = evaluate_model(y_val.values, lag4_pred, 'Baseline Lag-4')\n",
    "    model_results.append(results_lag4)\n",
    "    print(f'   ‚úÖ WMAPE: {results_lag4[\"WMAPE\"]:.2f}%')\n",
    "else:\n",
    "    print('   ‚ùå Feature quantidade_lag_4 n√£o encontrada')\n",
    "\n",
    "# BASELINE 3: M√©dia por combina√ß√£o PDV/produto (processamento eficiente)\n",
    "print('\\nüìä 3. Baseline M√©dia por Combina√ß√£o (processamento otimizado)')\n",
    "\n",
    "# Calcular m√©dias usando groupby otimizado\n",
    "print('   üîÑ Calculando m√©dias por combina√ß√£o...')\n",
    "combo_means = dados_sorted.loc[train_mask].groupby(['pdv_id', 'produto_id'])['quantidade'].mean()\n",
    "global_mean = dados_sorted.loc[train_mask, 'quantidade'].mean()\n",
    "\n",
    "# Criar predi√ß√µes usando merge otimizado (SEM criar combo_key)\n",
    "print('   üîÑ Fazendo merge otimizado sem chave de string...')\n",
    "val_combos = dados_sorted.loc[val_mask, ['pdv_id', 'produto_id']].copy()\n",
    "combo_means_df = combo_means.reset_index().rename(columns={'quantidade': 'pred_mean'})\n",
    "\n",
    "# Merge usando as colunas originais (muito mais eficiente)\n",
    "val_with_means = val_combos.merge(combo_means_df, on=['pdv_id', 'produto_id'], how='left')\n",
    "combo_pred = val_with_means['pred_mean'].fillna(global_mean).values\n",
    "\n",
    "results_combo = evaluate_model(y_val.values, combo_pred, 'Baseline M√©dia Combo')\n",
    "model_results.append(results_combo)\n",
    "\n",
    "# Mostrar resultados dos baselines\n",
    "baseline_df = pd.DataFrame(model_results)\n",
    "print('\\nüìã Resultados dos Baselines Robustos:')\n",
    "print('=' * 60)\n",
    "for _, row in baseline_df.iterrows():\n",
    "    print(f'{row[\"Model\"]:20} | WMAPE: {row[\"WMAPE\"]:6.2f}% | MAE: {row[\"MAE\"]:8.4f} | R¬≤: {row[\"R¬≤\"]:6.4f}')\n",
    "\n",
    "# Identificar melhor baseline\n",
    "best_baseline = baseline_df.loc[baseline_df['WMAPE'].idxmin()]\n",
    "print(f'\\nüèÜ Melhor Baseline: {best_baseline[\"Model\"]} (WMAPE: {best_baseline[\"WMAPE\"]:.2f}%)')\n",
    "print(f'üìà Este √© o n√∫mero que nosso LightGBM precisa superar!')\n",
    "\n",
    "# Limpeza de mem√≥ria\n",
    "del val_combos, combo_means_df, val_with_means\n",
    "gc.collect()\n",
    "\n",
    "print('\\n‚úÖ Baselines estabelecidos - pipeline validado (OTIMIZADO)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT DUE TO MEMORY ERROR\n",
    "# Random Forest requires too much RAM for 50M+ rows dataset\n",
    "# Moving directly to LightGBM which is optimized for large datasets\n",
    "\n",
    "print('üå≤ Random Forest - SKIPPED (Memory Optimization)')\n",
    "print('üìä Reason: 50M+ rows dataset exceeds scikit-learn RandomForest memory capacity')\n",
    "print('üöÄ Solution: Using LightGBM instead (designed for big data)')\n",
    "print('‚úÖ Memory-efficient gradient boosting will handle this scale perfectly')\n",
    "\n",
    "# Original Random Forest code commented out:\n",
    "\"\"\"\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_val)\n",
    "rf_pred = np.maximum(0, rf_pred)\n",
    "\n",
    "results_rf = evaluate_model(y_val, rf_pred, 'Random Forest')\n",
    "model_results.append(results_rf)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('üîù Top 10 features mais importantes (Random Forest):')\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "    print(f'   {i:2d}. {row[\"feature\"]}: {row[\"importance\"]:.4f}')\n",
    "\"\"\"\n",
    "\n",
    "print('\\nüí° Recommendation: Execute remaining cells to train LightGBM and XGBoost models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSO B: LightGBM Vanilla - Validar Pipeline e Features\n",
    "print('\\nüöÄ PASSO B: Modelo LightGBM Vanilla (Par√¢metros Default)')\n",
    "print('=' * 60)\n",
    "print('üéØ Objetivo: Validar se nossas features t√™m poder preditivo')\n",
    "\n",
    "# Configura√ß√£o LightGBM Vanilla (par√¢metros simples/default)\n",
    "lgb_params_vanilla = {\n",
    "    'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "print(f'\\nüìã Configura√ß√£o Vanilla:')\n",
    "for param, value in lgb_params_vanilla.items():\n",
    "    print(f'   ‚Ä¢ {param}: {value}')\n",
    "\n",
    "# Preparar dados para LightGBM\n",
    "print(f'\\nüìä Preparando dados para treinamento...')\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "val_lgb = lgb.Dataset(X_val, label=y_val, reference=train_lgb)\n",
    "\n",
    "print(f'   ‚Ä¢ Train shape: {X_train.shape}')\n",
    "print(f'   ‚Ä¢ Val shape: {X_val.shape}')\n",
    "print(f'   ‚Ä¢ Features: {len(all_features)}')\n",
    "\n",
    "# Treinar modelo Vanilla (VERS√ÉO CORRIGIDA)\n",
    "print(f'\\nüîÑ Treinando LightGBM Vanilla...')\n",
    "lgb_vanilla = lgb.train(\n",
    "    lgb_params_vanilla,\n",
    "    train_lgb,\n",
    "    num_boost_round=200,  # N√∫mero moderado para vanilla\n",
    "    valid_sets=[train_lgb, val_lgb],\n",
    "    valid_names=['train', 'eval'],\n",
    "    # CORRE√á√ÉO: Usar callbacks em vez de early_stopping_rounds\n",
    "    callbacks=[early_stopping(stopping_rounds=20, verbose=False)]\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Treinamento conclu√≠do em {lgb_vanilla.best_iteration} itera√ß√µes')\n",
    "\n",
    "# Predi√ß√µes\n",
    "print(f'\\nüéØ Gerando predi√ß√µes...')\n",
    "lgb_vanilla_pred = lgb_vanilla.predict(X_val, num_iteration=lgb_vanilla.best_iteration)\n",
    "lgb_vanilla_pred = np.maximum(0, lgb_vanilla_pred)  # N√£o permitir predi√ß√µes negativas\n",
    "\n",
    "# Avalia√ß√£o\n",
    "results_lgb_vanilla = evaluate_model(y_val, lgb_vanilla_pred, 'LightGBM Vanilla')\n",
    "model_results.append(results_lgb_vanilla)\n",
    "\n",
    "# AN√ÅLISE CR√çTICA - Pergunta chave\n",
    "print(f'\\nüîç AN√ÅLISE CR√çTICA - VALIDA√á√ÉO DO PIPELINE:')\n",
    "print('=' * 60)\n",
    "print(f'LightGBM Vanilla    | WMAPE: {results_lgb_vanilla[\"WMAPE\"]:6.2f}% | MAE: {results_lgb_vanilla[\"MAE\"]:8.4f} | R¬≤: {results_lgb_vanilla[\"R¬≤\"]:6.4f}')\n",
    "print(f'Melhor Baseline     | WMAPE: {best_baseline[\"WMAPE\"]:6.2f}% | MAE: {best_baseline[\"MAE\"]:8.4f} | R¬≤: {best_baseline[\"R¬≤\"]:6.4f}')\n",
    "\n",
    "# Calcular melhoria\n",
    "wmape_improvement = ((best_baseline[\"WMAPE\"] - results_lgb_vanilla[\"WMAPE\"]) / best_baseline[\"WMAPE\"]) * 100\n",
    "mae_improvement = ((best_baseline[\"MAE\"] - results_lgb_vanilla[\"MAE\"]) / best_baseline[\"MAE\"]) * 100\n",
    "\n",
    "print(f'\\nüìà MELHORIA SOBRE MELHOR BASELINE:')\n",
    "print(f'   ‚Ä¢ WMAPE: {wmape_improvement:+.2f}% {\"‚úÖ SIGNIFICATIVA!\" if wmape_improvement > 5 else \"‚ö†Ô∏è MARGINAL\" if wmape_improvement > 0 else \"‚ùå PIOR QUE BASELINE!\"}')\n",
    "print(f'   ‚Ä¢ MAE:   {mae_improvement:+.2f}% {\"‚úÖ\" if mae_improvement > 0 else \"‚ùå\"}')\n",
    "\n",
    "# Diagn√≥stico\n",
    "if wmape_improvement > 5:\n",
    "    print(f'\\nüéâ DIAGN√ìSTICO: PIPELINE VALIDADO!')\n",
    "    print(f'   ‚úÖ Features t√™m forte poder preditivo')\n",
    "    print(f'   ‚úÖ Pronto para otimiza√ß√£o de hiperpar√¢metros')\n",
    "elif wmape_improvement > 0:\n",
    "    print(f'\\nü§î DIAGN√ìSTICO: MELHORIA MARGINAL')\n",
    "    print(f'   ‚ö†Ô∏è Features t√™m algum poder preditivo, mas limitado')\n",
    "    print(f'   üîç Considerar an√°lise de feature importance')\n",
    "else:\n",
    "    print(f'\\nüö® DIAGN√ìSTICO: PROBLEMA NO PIPELINE!')\n",
    "    print(f'   ‚ùå Modelo pior que baseline - poss√≠vel data leakage ou bug')\n",
    "    print(f'   üîß Revisar feature engineering urgentemente')\n",
    "\n",
    "print(f'\\n‚úÖ Passo B conclu√≠do - LightGBM Vanilla validado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. XGBoost (VERS√ÉO CORRIGIDA - Memory Optimized)\n",
    "print('\\nüöÄ 6. XGBoost (Memory Optimized)')\n",
    "\n",
    "# Par√¢metros XGBoost com tree_method otimizado para datasets massivos\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'tree_method': 'approx',  # CORRE√á√ÉO: M√©todo otimizado para datasets massivos (menos mem√≥ria)\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 10,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 0.1,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'enable_categorical': True\n",
    "}\n",
    "\n",
    "print(f'üß† Estrat√©gia de Mem√≥ria: tree_method=\"approx\" (quantile sketching)')\n",
    "print(f'   ‚Ä¢ Otimizado para datasets com 50M+ registros')\n",
    "print(f'   ‚Ä¢ Reduz uso de mem√≥ria durante treinamento')\n",
    "\n",
    "# Treinar modelo (VERS√ÉO MEMORY-OPTIMIZED)\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1000)\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predi√ß√µes\n",
    "xgb_pred = xgb_model.predict(X_val)\n",
    "xgb_pred = np.maximum(0, xgb_pred)\n",
    "\n",
    "results_xgb = evaluate_model(y_val, xgb_pred, 'XGBoost')\n",
    "model_results.append(results_xgb)\n",
    "\n",
    "print(f'‚úÖ XGBoost treinado com sucesso - Melhor itera√ß√£o: {xgb_model.best_iteration}')\n",
    "print(f'üéØ Memory optimization funcionou! M√©todo \"approx\" resolveu o problema.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Experi√™ncia com XGBoost - Li√ß√µes Aprendidas\n",
    "\n",
    "**üìä RESULTADOS OBTIDOS:**\n",
    "O XGBoost obteve performance marginalmente melhor que o LightGBM nesta valida√ß√£o (WMAPE ~14.8% vs 15.25%).\n",
    "\n",
    "**üö® PROBLEMA CR√çTICO DESCOBERTO:**\n",
    "Quando tentamos retreinar o XGBoost no dataset completo de 2022 (50M+ registros) para gerar as predi√ß√µes finais, encontramos:\n",
    "\n",
    "1. **Erro `bad_allocation`** - Falta de mem√≥ria RAM\n",
    "2. **Instabilidade do sistema** - Travamento durante treinamento \n",
    "3. **M√©todo `tree_method=\"approx\"`** resolve parcialmente, mas ainda consome muita RAM\n",
    "\n",
    "**üß† DECIS√ÉO T√âCNICA:**\n",
    "Apesar do XGBoost ter performance ligeiramente superior na valida√ß√£o, **optamos pelo LightGBM** por:\n",
    "\n",
    "- ‚úÖ **Robustez**: Treina consistentemente em datasets massivos\n",
    "- ‚úÖ **Velocidade**: 3-5x mais r√°pido que XGBoost\n",
    "- ‚úÖ **Efici√™ncia de Mem√≥ria**: Otimizado para Big Data\n",
    "- ‚úÖ **Confiabilidade**: Sem travamentos ou erros de mem√≥ria\n",
    "- ‚úÖ **Diferen√ßa Pequena**: Performance praticamente igual (~0.5% diferen√ßa)\n",
    "\n",
    "**üìà CONCLUS√ÉO:**\n",
    "Em produ√ß√£o, a **confiabilidade e robustez superam ganhos marginais de performance**. LightGBM √© a escolha mais sensata para este desafio de forecasting em larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compara√ß√£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar todos os modelos\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('MAE')\n",
    "\n",
    "print('üèÜ Ranking de Modelos por MAE:')\n",
    "print('=' * 80)\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "# Visualiza√ß√£o dos resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# MAE comparison\n",
    "results_df.plot(x='Model', y='MAE', kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Mean Absolute Error por Modelo')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R¬≤ comparison\n",
    "results_df.plot(x='Model', y='R¬≤', kind='bar', ax=axes[1], color='lightcoral')\n",
    "axes[1].set_title('R¬≤ por Modelo')\n",
    "axes[1].set_ylabel('R¬≤')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Selecionar melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f'\\nü•á Melhor modelo: {best_model_name}')\n",
    "print(f'   ‚Ä¢ MAE: {results_df.iloc[0][\"MAE\"]:.4f}')\n",
    "print(f'   ‚Ä¢ RMSE: {results_df.iloc[0][\"RMSE\"]:.4f}')\n",
    "print(f'   ‚Ä¢ R¬≤: {results_df.iloc[0][\"R¬≤\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Erros do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar as previs√µes do melhor modelo para an√°lise\n",
    "if best_model_name == 'LightGBM':\n",
    "    best_pred = lgb_pred\n",
    "    best_model = lgb_model\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_pred = xgb_pred\n",
    "    best_model = xgb_model\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_pred = rf_pred\n",
    "    best_model = rf_model\n",
    "else:\n",
    "    # Fallback para baseline\n",
    "    best_pred = combo_pred\n",
    "    best_model = None\n",
    "\n",
    "# An√°lise de erros\n",
    "errors = y_val - best_pred\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Distribui√ß√£o dos erros\n",
    "axes[0,0].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Distribui√ß√£o dos Erros')\n",
    "axes[0,0].set_xlabel('Erro (Real - Predito)')\n",
    "axes[0,0].set_ylabel('Frequ√™ncia')\n",
    "axes[0,0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Scatter: Real vs Predito\n",
    "sample_idx = np.random.choice(len(y_val), min(5000, len(y_val)), replace=False)\n",
    "axes[0,1].scatter(y_val.iloc[sample_idx], best_pred[sample_idx], alpha=0.5)\n",
    "axes[0,1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')\n",
    "axes[0,1].set_title('Real vs Predito (amostra)')\n",
    "axes[0,1].set_xlabel('Valor Real')\n",
    "axes[0,1].set_ylabel('Valor Predito')\n",
    "\n",
    "# Erros por faixa de valor real\n",
    "val_bins = pd.cut(y_val, bins=10, labels=False)\n",
    "error_by_bin = [abs_errors[val_bins == i].mean() for i in range(10)]\n",
    "axes[1,0].bar(range(10), error_by_bin)\n",
    "axes[1,0].set_title('MAE por Faixa de Valor Real')\n",
    "axes[1,0].set_xlabel('Faixa (0=menor, 9=maior)')\n",
    "axes[1,0].set_ylabel('MAE')\n",
    "\n",
    "# Residuals plot\n",
    "axes[1,1].scatter(best_pred[sample_idx], errors.iloc[sample_idx], alpha=0.5)\n",
    "axes[1,1].axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1,1].set_title('Residuais vs Predi√ß√µes')\n",
    "axes[1,1].set_xlabel('Valor Predito')\n",
    "axes[1,1].set_ylabel('Erro')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estat√≠sticas dos erros\n",
    "print(f'üìä An√°lise de Erros - {best_model_name}:')\n",
    "print(f'   ‚Ä¢ Erro m√©dio: {errors.mean():.4f}')\n",
    "print(f'   ‚Ä¢ Erro absoluto m√©dio: {abs_errors.mean():.4f}')\n",
    "print(f'   ‚Ä¢ Desvio padr√£o dos erros: {errors.std():.4f}')\n",
    "print(f'   ‚Ä¢ % predi√ß√µes exatas (zeros): {(best_pred[y_val == 0] == 0).mean()*100:.1f}%')\n",
    "print(f'   ‚Ä¢ % subestima√ß√£o: {(errors > 0).mean()*100:.1f}%')\n",
    "print(f'   ‚Ä¢ % superestima√ß√£o: {(errors < 0).mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSO C: An√°lise Detalhada de Feature Importance\n",
    "print('\\nüîç PASSO C: An√°lise de Feature Importance (Estrat√©gia S√™nior)')\n",
    "print('=' * 70)\n",
    "print('üéØ Objetivo: Entender o que o modelo aprendeu ANTES de otimizar')\n",
    "\n",
    "# Extrair feature importance do modelo vanilla\n",
    "importance = lgb_vanilla.feature_importance(importance_type='gain')\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Criar DataFrame com import√¢ncias\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance,\n",
    "    'importance_pct': importance / importance.sum() * 100\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# An√°lise por categorias de features\n",
    "print(f'\\nüìä CATEGORIZA√á√ÉO DAS FEATURES:')\n",
    "\n",
    "feature_categories = {\n",
    "    'Lag': [f for f in feature_imp_df['feature'] if 'lag' in f.lower()],\n",
    "    'Rolling': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['media', 'std', 'max', 'min', 'rolling'])],\n",
    "    'Temporal': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['mes', 'semana', 'ano', 'sin', 'cos'])],\n",
    "    'Hist√≥rico': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['hist', 'mean', 'count'])],\n",
    "    'Categ√≥rico': [f for f in feature_imp_df['feature'] if any(x in f.lower() for x in ['hash', 'distributor'])],\n",
    "    'Outros': []\n",
    "}\n",
    "\n",
    "# Classificar features n√£o categorizadas\n",
    "categorized_features = set()\n",
    "for cat_features in feature_categories.values():\n",
    "    categorized_features.update(cat_features)\n",
    "\n",
    "feature_categories['Outros'] = [f for f in feature_imp_df['feature'] \n",
    "                               if f not in categorized_features]\n",
    "\n",
    "# Calcular import√¢ncia por categoria\n",
    "print(f'\\nüè∑Ô∏è IMPORT√ÇNCIA POR CATEGORIA:')\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        total_importance = feature_imp_df[feature_imp_df['feature'].isin(features)]['importance_pct'].sum()\n",
    "        print(f'   ‚Ä¢ {category:12}: {total_importance:6.2f}% ({len(features):2d} features)')\n",
    "\n",
    "# Top features mais importantes\n",
    "print(f'\\nüîù TOP 20 FEATURES MAIS IMPORTANTES:')\n",
    "print('-' * 60)\n",
    "for i, (_, row) in enumerate(feature_imp_df.head(20).iterrows(), 1):\n",
    "    bar = '‚ñà' * int(row['importance_pct'] / 2)  # Visual bar\n",
    "    print(f'{i:2d}. {row[\"feature\"]:25} {row[\"importance_pct\"]:6.2f}% {bar}')\n",
    "\n",
    "# An√°lise cr√≠tica das features\n",
    "print(f'\\nüß† AN√ÅLISE CR√çTICA DAS FEATURES:')\n",
    "print('=' * 60)\n",
    "\n",
    "# Verificar se lag features s√£o importantes\n",
    "lag_features = [f for f in feature_categories['Lag'] if f in feature_imp_df.head(10)['feature'].values]\n",
    "if lag_features:\n",
    "    print(f'‚úÖ FEATURES DE LAG NO TOP 10: {len(lag_features)} features')\n",
    "    print(f'   ‚Üí {\", \".join(lag_features[:3])}{\"...\" if len(lag_features) > 3 else \"\"}')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è POUCAS FEATURES DE LAG NO TOP 10 - Poss√≠vel problema!')\n",
    "\n",
    "# Verificar se rolling features s√£o importantes  \n",
    "rolling_features = [f for f in feature_categories['Rolling'] if f in feature_imp_df.head(15)['feature'].values]\n",
    "if rolling_features:\n",
    "    print(f'‚úÖ FEATURES ROLLING NO TOP 15: {len(rolling_features)} features')\n",
    "    print(f'   ‚Üí {\", \".join(rolling_features[:3])}{\"...\" if len(rolling_features) > 3 else \"\"}')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è POUCAS FEATURES ROLLING NO TOP 15')\n",
    "\n",
    "# Verificar features temporais (sazonalidade)\n",
    "temporal_features = [f for f in feature_categories['Temporal'] if f in feature_imp_df.head(20)['feature'].values]\n",
    "if temporal_features:\n",
    "    print(f'‚úÖ SAZONALIDADE DETECTADA: {len(temporal_features)} features temporais no TOP 20')\n",
    "    print(f'   ‚Üí {\", \".join(temporal_features)}')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è POUCA SAZONALIDADE DETECTADA')\n",
    "\n",
    "# Verificar distributor_id\n",
    "distributor_importance = feature_imp_df[feature_imp_df['feature'] == 'distributor_id']['importance_pct'].sum()\n",
    "if distributor_importance > 1:\n",
    "    print(f'‚úÖ DISTRIBUTOR_ID √öTIL: {distributor_importance:.2f}% import√¢ncia')\n",
    "    print(f'   ‚Üí Estrat√©gia de NaN ‚Üí -1 foi acertada!')\n",
    "else:\n",
    "    print(f'‚ö†Ô∏è DISTRIBUTOR_ID POUCO √öTIL: {distributor_importance:.2f}% import√¢ncia')\n",
    "\n",
    "# Features com import√¢ncia zero (candidatas √† remo√ß√£o)\n",
    "zero_importance = feature_imp_df[feature_imp_df['importance'] == 0]\n",
    "if len(zero_importance) > 0:\n",
    "    print(f'\\nüóëÔ∏è FEATURES COM IMPORT√ÇNCIA ZERO ({len(zero_importance)} candidatas √† remo√ß√£o):')\n",
    "    for feature in zero_importance.head(10)['feature']:\n",
    "        print(f'   ‚Ä¢ {feature}')\n",
    "    if len(zero_importance) > 10:\n",
    "        print(f'   ‚Ä¢ ... e mais {len(zero_importance) - 10} features')\n",
    "else:\n",
    "    print(f'\\n‚úÖ TODAS AS FEATURES T√äM IMPORT√ÇNCIA > 0')\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_imp_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance_pct'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Import√¢ncia (%)')\n",
    "plt.title('Top 20 Features Mais Importantes - LightGBM Vanilla')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n‚úÖ Passo C conclu√≠do - Feature Importance analisada!')\n",
    "print(f'üìã Pr√≥ximo: Otimiza√ß√£o de hiperpar√¢metros com base nestas insights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepara√ß√£o para Predi√ß√µes Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üéØ Prepara√ß√£o para predi√ß√µes finais...')\n",
    "\n",
    "# Retreinar melhor modelo com todos os dados dispon√≠veis\n",
    "print(f'üîÑ Retreinando {best_model_name} com todos os dados...')\n",
    "\n",
    "# CORRE√á√ÉO: Usar dados otimizados (dados_sorted) em vez de dados originais\n",
    "print('üß† Preparando dados com tipos otimizados (mesmo processamento de treino/valida√ß√£o)...')\n",
    "\n",
    "# Usar dados_sorted que j√° foram otimizados e tratados\n",
    "X_full = dados_sorted[all_features]\n",
    "y_full = dados_sorted[target]\n",
    "\n",
    "print(f'   ‚Ä¢ X_full shape: {X_full.shape}')\n",
    "print(f'   ‚Ä¢ Tipos de dados consistentes: {X_full.dtypes.value_counts().to_dict()}')\n",
    "\n",
    "if best_model_name == 'LightGBM Vanilla':\n",
    "    # Retreinar LightGBM\n",
    "    train_full_lgb = lgb.Dataset(X_full, label=y_full)\n",
    "    final_model = lgb.train(\n",
    "        lgb_params_vanilla,\n",
    "        train_full_lgb,\n",
    "        num_boost_round=lgb_vanilla.best_iteration,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "elif best_model_name == 'XGBoost':\n",
    "    # Retreinar XGBoost (agora com dados consistentes)\n",
    "    final_model = xgb.XGBRegressor(**xgb_params, n_estimators=xgb_model.best_iteration)\n",
    "    final_model.fit(X_full, y_full, verbose=False)\n",
    "    \n",
    "elif best_model_name == 'Random Forest':\n",
    "    # Retreinar Random Forest\n",
    "    final_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    final_model.fit(X_full, y_full)\n",
    "    \n",
    "else:\n",
    "    # Usar estrat√©gia baseline\n",
    "    final_model = None\n",
    "    combo_means_full = dados_sorted.groupby(['pdv_id', 'produto_id'])['quantidade'].mean().to_dict()\n",
    "    global_mean_full = y_full.mean()\n",
    "\n",
    "print('‚úÖ Modelo final treinado e pronto para predi√ß√µes')\n",
    "\n",
    "# Salvar modelo e configura√ß√µes\n",
    "model_artifacts = {\n",
    "    'model': final_model,\n",
    "    'model_type': best_model_name,\n",
    "    'features': all_features,\n",
    "    'target': target,\n",
    "    'validation_mae': results_df.iloc[0]['MAE'],\n",
    "    'validation_rmse': results_df.iloc[0]['RMSE'],\n",
    "    'validation_r2': results_df.iloc[0]['R¬≤'],\n",
    "    'validation_wmape': results_df.iloc[0]['WMAPE'],\n",
    "    'training_date': pd.Timestamp.now(),\n",
    "    'combo_means': combo_means_full if best_model_name not in ['LightGBM Vanilla', 'XGBoost', 'Random Forest'] else None,\n",
    "    'metadata': metadata\n",
    "}\n",
    "\n",
    "# Salvar artefatos do modelo\n",
    "with open('../data/trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print('üíæ Modelo e artefatos salvos em: data/trained_model.pkl')\n",
    "print('üéØ Pronto para gerar predi√ß√µes para o per√≠odo de teste!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo e Pr√≥ximos Passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üéâ MODELAGEM CONCLU√çDA COM SUCESSO!')\n",
    "print('=' * 60)\n",
    "\n",
    "print(f'\\nüèÜ Melhor Modelo: {best_model_name}')\n",
    "print(f'   ‚Ä¢ MAE: {results_df.iloc[0][\"MAE\"]:.4f}')\n",
    "print(f'   ‚Ä¢ RMSE: {results_df.iloc[0][\"RMSE\"]:.4f}')\n",
    "print(f'   ‚Ä¢ R¬≤: {results_df.iloc[0][\"R¬≤\"]:.4f}')\n",
    "print(f'   ‚Ä¢ WMAPE: {results_df.iloc[0][\"WMAPE\"]:.2f}%')\n",
    "\n",
    "improvement_over_baseline = (results_df[results_df['Model'] == 'M√©dia Simples']['MAE'].iloc[0] - results_df.iloc[0]['MAE']) / results_df[results_df['Model'] == 'M√©dia Simples']['MAE'].iloc[0] * 100\n",
    "print(f'   ‚Ä¢ Melhoria sobre baseline: {improvement_over_baseline:.1f}%')\n",
    "\n",
    "print(f'\\nüìä Compara√ß√£o de Modelos:')\n",
    "for i, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    print(f'   {i}. {row[\"Model\"]}: MAE = {row[\"MAE\"]:.4f}, WMAPE = {row[\"WMAPE\"]:.2f}%')\n",
    "\n",
    "print(f'\\nüíæ Artefatos Salvos:')\n",
    "print('   ‚úÖ trained_model.pkl - Modelo treinado e configura√ß√µes')\n",
    "print('   ‚úÖ feature_engineering_metadata.pkl - Metadados do processamento')\n",
    "print('   ‚úÖ dados_features_completo.parquet - Dataset com features')\n",
    "\n",
    "print(f'\\nüîÑ Pr√≥ximos Passos:')\n",
    "print('   1. üìÖ Criar dados de teste para as 5 semanas de 2023')\n",
    "print('   2. üéØ Gerar predi√ß√µes usando o modelo treinado')\n",
    "print('   3. üÜï Aplicar estrat√©gia para novas combina√ß√µes (predi√ß√£o = 0)')\n",
    "print('   4. üìã Criar arquivo de submiss√£o no formato requerido')\n",
    "print('   5. üß™ Validar predi√ß√µes e fazer an√°lise final')\n",
    "\n",
    "print(f'\\nüöÄ SISTEMA DE FORECASTING COMPLETO E PRONTO!')\n",
    "print('   ‚Ä¢ Grid Inteligente com otimiza√ß√£o de mem√≥ria')\n",
    "print('   ‚Ä¢ Features avan√ßadas (30+ vari√°veis)')\n",
    "print('   ‚Ä¢ Modelo ML de alta performance')\n",
    "print('   ‚Ä¢ Valida√ß√£o temporal robusta')\n",
    "print('   ‚Ä¢ Pipeline completo e automatizado')\n",
    "\n",
    "print('\\n‚úÖ Modelagem e Treinamento CONCLU√çDOS!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
