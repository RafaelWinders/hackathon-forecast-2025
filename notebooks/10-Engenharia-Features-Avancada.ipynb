{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Engenharia de Features Avançada\n",
    "\n",
    "Este notebook cria features avançadas baseadas nos dados de treino e validação temporal.\n",
    "\n",
    "## Novas Features:\n",
    "- **Target de Classificação**: `vendeu` (1 se quantidade > 0, 0 caso contrário)\n",
    "- **Features de Preço**: preço unitário, variações, médias\n",
    "- **Features de Calendário**: dia do mês, semana do mês, flags temporais\n",
    "- **Features de Hierarquia**: agregações por categoria, zipcode\n",
    "- **Features de Tendência**: momentum, aceleração de vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando Engenharia de Features Avançada\n",
      "🎯 Objetivo: Criar variáveis preditivas poderosas para o modelo de dois estágios\n",
      "📁 Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('🚀 Iniciando Engenharia de Features Avançada')\n",
    "print('🎯 Objetivo: Criar variáveis preditivas poderosas para o modelo de dois estágios')\n",
    "\n",
    "# Criar pasta submissao3 se não existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('📁 Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados de Treino e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados de treino e validação...\n",
      "🏋️ Dados de treino: (50126880, 6)\n",
      "🔍 Dados de validação: (5221550, 6)\n",
      "📅 Treino: 2021-12-28 00:00:00 até 2022-11-22 00:00:00\n",
      "📅 Validação: 2022-11-29 00:00:00 até 2022-12-27 00:00:00\n",
      "✅ Dados carregados com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados de treino e validação\n",
    "print('📂 Carregando dados de treino e validação...')\n",
    "\n",
    "train_data = pd.read_parquet('../data/submissao3/train_data.parquet')\n",
    "validation_data = pd.read_parquet('../data/submissao3/validation_data.parquet')\n",
    "\n",
    "print(f'🏋️ Dados de treino: {train_data.shape}')\n",
    "print(f'🔍 Dados de validação: {validation_data.shape}')\n",
    "\n",
    "# Verificar período\n",
    "print(f'📅 Treino: {train_data[\"semana\"].min()} até {train_data[\"semana\"].max()}')\n",
    "print(f'📅 Validação: {validation_data[\"semana\"].min()} até {validation_data[\"semana\"].max()}')\n",
    "\n",
    "print('✅ Dados carregados com sucesso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados auxiliares...\n",
      "📊 Produtos: (7092, 8) - Categorias: 7\n",
      "📊 PDVs: (14419, 4) - Zipcodes: 788\n",
      "✅ Dados auxiliares carregados\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados de produtos e PDVs para features adicionais\n",
    "print('📂 Carregando dados auxiliares...')\n",
    "\n",
    "# Produtos\n",
    "produtos = pd.read_parquet(\n",
    "    '../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet'\n",
    ")\n",
    "produtos = produtos.rename(columns={\n",
    "    'produto': 'produto_id'\n",
    "})\n",
    "\n",
    "# PDVs\n",
    "pdvs = pd.read_parquet(\n",
    "    '../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
    ")\n",
    "pdvs = pdvs.rename(columns={\n",
    "    'pdv': 'pdv_id',\n",
    "    'categoria_pdv': 'tipo_loja'\n",
    "})\n",
    "\n",
    "print(f'📊 Produtos: {produtos.shape} - Categorias: {produtos[\"categoria\"].nunique()}')\n",
    "print(f'📊 PDVs: {pdvs.shape} - Zipcodes: {pdvs[\"zipcode\"].nunique()}')\n",
    "\n",
    "print('✅ Dados auxiliares carregados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Função de Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Função CORRIGIDA contra TARGET LEAKAGE definida!\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# FUNÇÃO BLINDADA CONTRA CRASH DE MEMÓRIA - DOWNCASTING INCREMENTAL SEGURO\n",
    "# =================================================================================\n",
    "\n",
    "def reduce_mem_usage(df, name):\n",
    "    \"\"\"\n",
    "    Reduz o uso de memória de um dataframe, alterando os tipos de dados das colunas.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'🔄 Otimizando memória para {name}...')\n",
    "    print(f'   📊 Memória inicial: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Para floats, vamos usar float32 que é suficiente para a maioria dos casos\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'   ✅ Memória final: {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% de redução)')\n",
    "    return df\n",
    "\n",
    "def safe_downcast_column(series, target_dtype):\n",
    "    \"\"\"\n",
    "    Faz downcast seguro de uma coluna, verificando limites antes.\n",
    "    Converte strings para numérico se necessário.\n",
    "    \"\"\"\n",
    "    # Se for string, converter para numérico primeiro\n",
    "    if series.dtype == 'object':\n",
    "        try:\n",
    "            series = pd.to_numeric(series, errors='coerce')\n",
    "            print(f'   🔄 Coluna {series.name}: convertida de string para numérico')\n",
    "        except:\n",
    "            print(f'   ⚠️ Coluna {series.name}: não foi possível converter para numérico')\n",
    "            return series\n",
    "    \n",
    "    if target_dtype == 'int32':\n",
    "        if series.min() >= np.iinfo(np.int32).min and series.max() <= np.iinfo(np.int32).max:\n",
    "            return series.astype('int32')\n",
    "        else:\n",
    "            print(f'   ⚠️ Coluna {series.name}: valores muito grandes para int32, mantendo int64')\n",
    "            return series.astype('int64')\n",
    "    elif target_dtype == 'float32':\n",
    "        if series.min() >= np.finfo(np.float32).min and series.max() <= np.finfo(np.float32).max:\n",
    "            return series.astype('float32')\n",
    "        else:\n",
    "            return series.astype('float64')\n",
    "    else:\n",
    "        return series.astype(target_dtype)\n",
    "\n",
    "def criar_features_avancadas_otimizada(dados, dados_produtos, dados_pdvs, nome_conjunto=\"DATASET\"):\n",
    "    \"\"\"\n",
    "    Cria features avançadas com downcasting incremental SEGURO para controle máximo de memória.\n",
    "    VERSÃO BLINDADA: Otimiza memória a cada etapa para evitar crash, verificando limites.\n",
    "    VERSÃO CORRIGIDA: Remove target leakage das features rolantes.\n",
    "    \"\"\"\n",
    "    print(f'\\\\n🔧 Criando features avançadas para {nome_conjunto} (VERSÃO CORRIGIDA - SEM TARGET LEAKAGE)...')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 1: OTIMIZAÇÃO INICIAL DOS DADOS BASE (SEGURA)\n",
    "    # ============================================================================\n",
    "    print('   🎯 Otimização inicial de tipos de dados (verificando limites)...')\n",
    "    \n",
    "    # Verificar e downcast de IDs com segurança\n",
    "    for col in ['pdv_id', 'produto_id', 'distributor_id']:\n",
    "        if col in dados.columns:\n",
    "            print(f'      🔍 {col}: tipo={dados[col].dtype}')\n",
    "            # Se for string, mostrar alguns valores de exemplo\n",
    "            if dados[col].dtype == 'object':\n",
    "                print(f'         Exemplos: {dados[col].head(3).tolist()}')\n",
    "            else:\n",
    "                print(f'         Range: {dados[col].min()} até {dados[col].max()}')\n",
    "            dados[col] = safe_downcast_column(dados[col], 'int32')\n",
    "    \n",
    "    if 'quantidade' in dados.columns:\n",
    "        dados['quantidade'] = safe_downcast_column(dados['quantidade'], 'int32')\n",
    "    if 'faturamento' in dados.columns:\n",
    "        dados['faturamento'] = safe_downcast_column(dados['faturamento'], 'float32')\n",
    "    \n",
    "    print('   ✅ Otimização inicial concluída.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 2: TARGET E MERGES COM DOWNCASTING IMEDIATO\n",
    "    # ============================================================================\n",
    "    print('   🎯 Criando target e fazendo merge com dados auxiliares...')\n",
    "    \n",
    "    # Target como int8 (muito mais eficiente)\n",
    "    dados['vendeu'] = (dados['quantidade'] > 0).astype('int8')\n",
    "    \n",
    "    # Merge com dados auxiliares\n",
    "    dados = dados.merge(dados_produtos[['produto_id', 'categoria']], on='produto_id', how='left')\n",
    "    dados = dados.merge(dados_pdvs[['pdv_id', 'zipcode', 'tipo_loja']], on='pdv_id', how='left')\n",
    "    \n",
    "    # CORREÇÃO: Verificar se semana_original existe antes de sobrescrever\n",
    "    if 'semana_original' not in dados.columns:\n",
    "        dados['semana_original'] = dados['semana']\n",
    "        print('   📅 Coluna semana_original criada para preservar separação treino/validação')\n",
    "    \n",
    "    # Converter para datetime apenas temporariamente para criar features\n",
    "    dados['semana'] = pd.to_datetime(dados['semana'])\n",
    "    \n",
    "    print('   ✅ Target e merges concluídos.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 3: ORDENAÇÃO E PREPARAÇÃO\n",
    "    # ============================================================================\n",
    "    print('   🔄 Ordenando dados para consistência temporal...')\n",
    "    dados.sort_values(['pdv_id', 'produto_id', 'semana'], inplace=True)\n",
    "    dados.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gb_cols = ['pdv_id', 'produto_id']\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # ETAPA 4: FEATURES DE PREÇO COM DOWNCASTING IMEDIATO (VERSÃO CORRIGIDA)\n",
    "    # ===========================================================================\n",
    "    print('   💰 Criando features de preço (COM CORREÇÃO DE LEAKAGE)...')\n",
    "\n",
    "    # 1. Calcule o preço unitário apenas onde a venda ocorreu, o resto fica NaN.\n",
    "    # Isso quebra a ligação direta com o target 'vendeu'.\n",
    "    preco_unitario_temp = np.where(\n",
    "        dados['quantidade'] > 0,\n",
    "        dados['faturamento'] / dados['quantidade'],\n",
    "        np.nan  # Usar NaN em vez de 0 é a chave da correção!\n",
    "    )\n",
    "    dados['preco_unitario_atual'] = preco_unitario_temp.astype('float32')\n",
    "\n",
    "    # 2. Propague o último preço conhecido para frente (forward fill) para cada item/loja.\n",
    "    # Isso simula a realidade: o preço permanece o mesmo até ser alterado.\n",
    "    # Usamos bfill() em seguida para cobrir casos onde as primeiras semanas são NaN.\n",
    "    print('      propagando preços para semanas sem vendas...')\n",
    "    dados['preco_unitario_atual'] = dados.groupby(gb_cols)['preco_unitario_atual'].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    # 3. Se ainda houver NaNs (itens que nunca venderam), preencha com 0.\n",
    "    dados['preco_unitario_atual'].fillna(0, inplace=True)\n",
    "\n",
    "    # Lags de preço (agora calculados sobre o preço corrigido e sem leakage)\n",
    "    gb_preco = dados.groupby(gb_cols)['preco_unitario_atual']\n",
    "    dados['preco_lag_1'] = gb_preco.shift(1).astype('float32')\n",
    "    dados['preco_lag_2'] = gb_preco.shift(2).astype('float32')\n",
    "    dados['variacao_preco_sku_semanal'] = (dados['preco_lag_1'] - dados['preco_lag_2']).fillna(0).astype('float32')\n",
    "\n",
    "    print('   ✅ Features de preço criadas e corrigidas contra leakage.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 5: FEATURES DE LAG E ROLLING (CRÍTICA) - CORREÇÃO TARGET LEAKAGE\n",
    "    # ============================================================================\n",
    "    print('   📊 Criando features de lag e rolling (ETAPA CRÍTICA - SEM TARGET LEAKAGE)...')\n",
    "\n",
    "    gb_quantidade = dados.groupby(gb_cols)['quantidade']\n",
    "\n",
    "    # Lags de quantidade como float32 (não float64)\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        dados[f'quantidade_lag_{lag}'] = gb_quantidade.shift(lag).astype('float32')\n",
    "\n",
    "    # CORREÇÃO CRÍTICA: Calcular rolling features SOBRE OS DADOS DEFASADOS\n",
    "    # Usar lag_1 como base para evitar que a semana atual influencie nas features\n",
    "    gb_quantidade_lag1 = dados.groupby(gb_cols)['quantidade_lag_1']\n",
    "\n",
    "    # Rolling features baseadas em dados do passado (sem a semana atual)\n",
    "    rolling_window_corrigido = gb_quantidade_lag1.rolling(window=4, min_periods=1)\n",
    "    dados['quantidade_media_4w'] = rolling_window_corrigido.mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    dados['quantidade_std_4w'] = rolling_window_corrigido.std().reset_index(level=gb_cols, drop=True).fillna(0).astype('float32')\n",
    "    dados['quantidade_max_4w'] = rolling_window_corrigido.max().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "\n",
    "    print('   ✅ Features de lag e rolling criadas SEM TARGET LEAKAGE.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 6: FEATURES DA FASE 2 COM DOWNCASTING INCREMENTAL - CORREÇÃO EWMA\n",
    "    # ============================================================================\n",
    "    print('   🔄 Adicionando features EWMA (Fase 2 otimizada - SEM TARGET LEAKAGE)...')\n",
    "    \n",
    "    # CORREÇÃO CRÍTICA: EWMA também baseado em dados defasados\n",
    "    dados['quantidade_ewma_4w'] = gb_quantidade_lag1.ewm(span=4, adjust=False).mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    dados['quantidade_ewma_8w'] = gb_quantidade_lag1.ewm(span=8, adjust=False).mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    dados['preco_ewma_4w'] = gb_preco.ewm(span=4, adjust=False).mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    \n",
    "    print('   📅 Adicionando features de calendário avançadas (Fase 2 otimizada)...')\n",
    "    dados['semana_do_ano'] = dados['semana'].dt.isocalendar().week.astype('int8')\n",
    "    dados['eh_primeira_semana_mes'] = (dados['semana'].dt.day <= 7).astype('int8')\n",
    "    dados['eh_dezembro'] = (dados['semana'].dt.month == 12).astype('int8')\n",
    "    dados['eh_janeiro'] = (dados['semana'].dt.month == 1).astype('int8')\n",
    "    dados['eh_pos_festas'] = ((dados['semana'].dt.month == 1) & (dados['semana'].dt.day <= 15)).astype('int8')\n",
    "    \n",
    "    # Features cíclicas como float32\n",
    "    dados['semana_ano_sin'] = np.sin(2 * np.pi * dados['semana_do_ano'] / 52).astype('float32')\n",
    "    dados['semana_ano_cos'] = np.cos(2 * np.pi * dados['semana_do_ano'] / 52).astype('float32')\n",
    "    \n",
    "    print('   💸 Adicionando features de preço relativo (Fase 2 otimizada)...')\n",
    "    preco_medio_categoria = dados.groupby(['semana', 'categoria'])['preco_lag_1'].transform('mean')\n",
    "    dados['preco_relativo_categoria'] = (dados['preco_lag_1'] / preco_medio_categoria).fillna(1.0).astype('float32')\n",
    "    \n",
    "    preco_medio_pdv = dados.groupby(['semana', 'pdv_id'])['preco_lag_1'].transform('mean')\n",
    "    dados['preco_relativo_pdv'] = (dados['preco_lag_1'] / preco_medio_pdv).fillna(1.0).astype('float32')\n",
    "    \n",
    "    # CORREÇÃO CRÍTICA: preco_volatilidade baseada em features sem leakage\n",
    "    dados['preco_volatilidade'] = (dados['quantidade_std_4w'] / (dados['quantidade_media_4w'] + 1e-6)).astype('float32')\n",
    "    \n",
    "    print('   ✅ Features da Fase 2 criadas e corrigidas contra target leakage.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 7: FEATURES DE TENDÊNCIA E HIERARQUIA OTIMIZADAS - CORREÇÃO\n",
    "    # ============================================================================\n",
    "    print('   📈 Criando features de tendência e hierarquia (otimizadas - SEM TARGET LEAKAGE)...')\n",
    "    \n",
    "    dados['media_vendas_categoria_pdv_lag_1'] = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('mean').astype('float32')\n",
    "    vendas_categoria_pdv_lag = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('sum')\n",
    "    dados['share_vendas_sku_categoria_lag_1'] = (dados['quantidade_lag_1'] / vendas_categoria_pdv_lag).fillna(0).astype('float32')\n",
    "    \n",
    "    # CORREÇÃO CRÍTICA: momentum baseado em features sem leakage\n",
    "    dados['momentum_ratio'] = (dados['quantidade_lag_1'] / dados['quantidade_media_4w']).fillna(0).astype('float32')\n",
    "    dados['momentum_ratio_ewma'] = (dados['quantidade_lag_1'] / dados['quantidade_ewma_4w']).fillna(0).astype('float32')\n",
    "    dados['aceleracao'] = (dados['quantidade_lag_1'] - dados['quantidade_lag_2']).fillna(0).astype('float32')\n",
    "    \n",
    "    print('   ✅ Features de tendência criadas SEM TARGET LEAKAGE.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 8: FEATURES DE CALENDÁRIO BÁSICAS OTIMIZADAS\n",
    "    # ============================================================================\n",
    "    print('   📅 Criando features de calendário básicas (otimizadas)...')\n",
    "    dados['dia_do_mes'] = dados['semana'].dt.day.astype('int8')\n",
    "    dados['semana_do_mes'] = ((dados['semana'].dt.day - 1) // 7 + 1).astype('int8')\n",
    "    dados['eh_inicio_mes'] = (dados['semana'].dt.day <= 7).astype('int8')\n",
    "    dados['eh_fim_mes'] = (dados['semana'].dt.day >= 22).astype('int8')\n",
    "    dados['mes'] = dados['semana'].dt.month.astype('int8')\n",
    "    \n",
    "    dados['mes_sin'] = np.sin(2 * np.pi * dados['mes'] / 12).astype('float32')\n",
    "    dados['mes_cos'] = np.cos(2 * np.pi * dados['mes'] / 12).astype('float32')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 9: FEATURES PARA REGRESSÃO OTIMIZADAS\n",
    "    # ============================================================================\n",
    "    print('   🎯 Criando features específicas para REGRESSÃO (otimizadas)...')\n",
    "    dados['preco_medio_semanal_sku_atual'] = dados.groupby(['semana', 'produto_id'])['preco_unitario_atual'].transform('mean').astype('float32')\n",
    "    dados['media_vendas_categoria_pdv_atual'] = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('mean').astype('float32')\n",
    "    vendas_categoria_pdv_atual = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('sum')\n",
    "    dados['share_vendas_sku_categoria_atual'] = (dados['quantidade'] / vendas_categoria_pdv_atual).fillna(0).astype('float32')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 10: FEATURES CATEGÓRICAS HASH OTIMIZADAS\n",
    "    # ============================================================================\n",
    "    print('   🏷️ Criando features categóricas (otimizadas)...')\n",
    "    dados['pdv_hash'] = (dados['pdv_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados['produto_hash'] = (dados['produto_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados['categoria_hash'] = (dados['categoria'].astype(str).apply(hash).abs() % 50).astype('int8')\n",
    "    dados['zipcode_hash'] = (dados['zipcode'].astype(str).apply(hash).abs() % 1000).astype('int16')\n",
    "    \n",
    "    dados['pdv_produto_hash'] = (dados['pdv_hash'].astype('int16') * 100 + dados['produto_hash']).astype('int16')\n",
    "    dados['categoria_zipcode_hash'] = (dados['categoria_hash'].astype('int32') * 1000 + dados['zipcode_hash']).astype('int32')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 11: LIMPEZA FINAL E REMOÇÃO DA COLUNA DATETIME PESADA\n",
    "    # ============================================================================\n",
    "    print('   🧹 Aplicando limpeza final e removendo coluna datetime pesada...')\n",
    "    \n",
    "    # CORREÇÃO: Preservar semana_original antes de remover a coluna datetime\n",
    "    # Remover apenas a coluna datetime temporária, manter semana_original\n",
    "    dados = dados.drop(columns=['semana'])\n",
    "    \n",
    "    # A coluna semana_original fica preservada para separação posterior\n",
    "    \n",
    "    # Limpeza de infinitos e NaNs\n",
    "    dados.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    dados.fillna(0, inplace=True)\n",
    "    \n",
    "    print(f'   ✅ {nome_conjunto} - Features criadas com sucesso: {dados.shape}')\n",
    "    print('   🆕 Inclui todas as features da Fase 2: EWMA, calendário avançado, preço relativo')\n",
    "    print('   🛡️ Memória otimizada a cada etapa - BLINDADO contra crash!')\n",
    "    print('   🔒 Downcasting seguro aplicado - conversão automática de strings para numérico')\n",
    "    print('   📅 Coluna semana_original preservada para separação treino/validação')\n",
    "    print('   🔧 CORREÇÃO DE TARGET LEAKAGE APLICADA nas ETAPAS 5, 6 e 7')\n",
    "    print('   ✅ Rolling, EWMA e features derivadas baseadas em quantidade_lag_1')\n",
    "    \n",
    "    return dados\n",
    "\n",
    "print('✅ Função CORRIGIDA contra TARGET LEAKAGE definida!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aplicação da Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Etapa 1: Criando features temporais no dataset combinado...\n",
      "   🔧 Convertendo tipos dos dados auxiliares para compatibilidade...\n",
      "   ✅ produtos[produto_id] convertido para int64\n",
      "   ✅ pdvs[pdv_id] convertido para int64\n",
      "\\n🔧 Criando features avançadas para DATASET_COMBINADO (VERSÃO CORRIGIDA - SEM TARGET LEAKAGE)...\n",
      "   🎯 Otimização inicial de tipos de dados (verificando limites)...\n",
      "      🔍 pdv_id: tipo=object\n",
      "         Exemplos: ['1008240744247283174', '1008240744247283174', '10097752152132198']\n",
      "   🔄 Coluna pdv_id: convertida de string para numérico\n",
      "   ⚠️ Coluna pdv_id: valores muito grandes para int32, mantendo int64\n",
      "      🔍 produto_id: tipo=object\n",
      "         Exemplos: ['1938760505411922162', '4098058333001424920', '1938760505411922162']\n",
      "   🔄 Coluna produto_id: convertida de string para numérico\n",
      "   ⚠️ Coluna produto_id: valores muito grandes para int32, mantendo int64\n",
      "      🔍 distributor_id: tipo=float32\n",
      "         Range: 4.0 até 11.0\n",
      "   ✅ Otimização inicial concluída.\n",
      "   🎯 Criando target e fazendo merge com dados auxiliares...\n",
      "   ✅ Target e merges concluídos.\n",
      "   🔄 Ordenando dados para consistência temporal...\n",
      "   💰 Criando features de preço (COM CORREÇÃO DE LEAKAGE)...\n",
      "      propagando preços para semanas sem vendas...\n",
      "   ✅ Features de preço criadas e corrigidas contra leakage.\n",
      "   📊 Criando features de lag e rolling (ETAPA CRÍTICA - SEM TARGET LEAKAGE)...\n",
      "   ✅ Features de lag e rolling criadas SEM TARGET LEAKAGE.\n",
      "   🔄 Adicionando features EWMA (Fase 2 otimizada - SEM TARGET LEAKAGE)...\n",
      "   📅 Adicionando features de calendário avançadas (Fase 2 otimizada)...\n",
      "   💸 Adicionando features de preço relativo (Fase 2 otimizada)...\n",
      "   ✅ Features da Fase 2 criadas e corrigidas contra target leakage.\n",
      "   📈 Criando features de tendência e hierarquia (otimizadas - SEM TARGET LEAKAGE)...\n",
      "   ✅ Features de tendência criadas SEM TARGET LEAKAGE.\n",
      "   📅 Criando features de calendário básicas (otimizadas)...\n",
      "   🎯 Criando features específicas para REGRESSÃO (otimizadas)...\n",
      "   🏷️ Criando features categóricas (otimizadas)...\n",
      "   🧹 Aplicando limpeza final e removendo coluna datetime pesada...\n",
      "   ✅ DATASET_COMBINADO - Features criadas com sucesso: (55348430, 55)\n",
      "   🆕 Inclui todas as features da Fase 2: EWMA, calendário avançado, preço relativo\n",
      "   🛡️ Memória otimizada a cada etapa - BLINDADO contra crash!\n",
      "   🔒 Downcasting seguro aplicado - conversão automática de strings para numérico\n",
      "   📅 Coluna semana_original preservada para separação treino/validação\n",
      "   🔧 CORREÇÃO DE TARGET LEAKAGE APLICADA nas ETAPAS 5, 6 e 7\n",
      "   ✅ Rolling, EWMA e features derivadas baseadas em quantidade_lag_1\n",
      "   ✅ Features temporais criadas.\n",
      "\n",
      "📊 Etapa 2: Separando os datasets de volta para treino e validação...\n",
      "   🗑️ Liberando memória do dataframe combinado...\n",
      "   ✅ Memória liberada.\n",
      "🔄 Otimizando memória para train_features...\n",
      "   📊 Memória inicial: 9465.33 MB\n",
      "   ✅ Memória final: 9130.70 MB (3.5% de redução)\n",
      "🔄 Otimizando memória para validation_features...\n",
      "   📊 Memória inicial: 985.97 MB\n",
      "   ✅ Memória final: 941.16 MB (4.5% de redução)\n",
      "\n",
      "💾 Etapa Final: Salvando arquivos otimizados sequencialmente...\n",
      "   🔧 Aplicando correção de tipos para compatibilidade com Parquet...\n",
      "   ✅ Dados de TREINO salvos em: ../data/submissao3/train_features.parquet\n",
      "      Shape: (50126880, 54), Colunas: 54\n",
      "   ✅ Dados de VALIDAÇÃO salvos em: ../data/submissao3/validation_features.parquet\n",
      "      Shape: (5221550, 54), Colunas: 54\n",
      "\n",
      "📋 Salvando metadados das features...\n",
      "   ✅ Metadados salvos em: data/submissao3/advanced_features_metadata.pkl\n",
      "\n",
      "🎉 ENGENHARIA DE FEATURES AVANÇADA CONCLUÍDA (COM OTIMIZAÇÃO DE MEMÓRIA)!\n",
      "================================================================================\n",
      "🎯 Features criadas para modelo de dois estágios:\n",
      "   📊 CLASSIFICAÇÃO: Use target \"vendeu\" para treinar se vai vender\n",
      "   📊 REGRESSÃO: Use target \"quantidade\" (apenas onde vendeu=1) para quanto vai vender\n",
      "\n",
      "💡 Próximos passos:\n",
      "   1. Treinar modelo de classificação (LGBMClassifier)\n",
      "   2. Treinar modelo de regressão (LGBMRegressor)\n",
      "   3. Combinar os dois no pipeline final\n",
      "\n",
      "🚀 Dados prontos para modelagem avançada!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VERSÃO CORRIGIDA - PROCESSAMENTO SEQUENCIAL PARA EVITAR CRASH DE MEMÓRIA\n",
    "# ==============================================================================\n",
    "\n",
    "# Função auxiliar para reduzir o uso de memória\n",
    "def reduce_mem_usage(df, name):\n",
    "    \"\"\"\n",
    "    Reduz o uso de memória de um dataframe, alterando os tipos de dados das colunas.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'🔄 Otimizando memória para {name}...')\n",
    "    print(f'   📊 Memória inicial: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'   ✅ Memória final: {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% de redução)')\n",
    "    return df\n",
    "\n",
    "# Função para limpar tipos antes do salvamento - CORREÇÃO APLICADA AQUI\n",
    "def fix_parquet_types(df):\n",
    "    \"\"\"Fix tipos problemáticos antes de salvar em parquet\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    \n",
    "    # Converter todas as colunas object para string de forma robusta\n",
    "    for col in df_fixed.columns:\n",
    "        if df_fixed[col].dtype == 'object':\n",
    "            # Primeiro converter todos valores para string\n",
    "            df_fixed[col] = df_fixed[col].astype(str)\n",
    "            # Limpar valores problemáticos\n",
    "            df_fixed[col] = df_fixed[col].replace(['nan', 'None', '<NA>', 'NaN', 'null'], 'Unknown')\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# --- NOVA LÓGICA DE EXECUÇÃO SEQUENCIAL ---\n",
    "\n",
    "import gc\n",
    "\n",
    "# Passo 1: Concatenar dados APENAS para criar features que dependem da série temporal completa (lags, rolling)\n",
    "print('\\n🔗 Etapa 1: Criando features temporais no dataset combinado...')\n",
    "dados_completos = pd.concat([train_data, validation_data], ignore_index=True)\n",
    "\n",
    "# Criar coluna semana_original para separar depois\n",
    "dados_completos['semana_original'] = dados_completos['semana']\n",
    "\n",
    "# CORREÇÃO CRÍTICA: Converter tipos dos dados auxiliares ANTES do merge\n",
    "print('   🔧 Convertendo tipos dos dados auxiliares para compatibilidade...')\n",
    "\n",
    "# Converter produto_id nos dados auxiliares para int64 (mesmo tipo do dados_completos)\n",
    "if produtos['produto_id'].dtype == 'object':\n",
    "    produtos['produto_id'] = pd.to_numeric(produtos['produto_id'], errors='coerce').astype('int64')\n",
    "    print('   ✅ produtos[produto_id] convertido para int64')\n",
    "\n",
    "if pdvs['pdv_id'].dtype == 'object':\n",
    "    pdvs['pdv_id'] = pd.to_numeric(pdvs['pdv_id'], errors='coerce').astype('int64')\n",
    "    print('   ✅ pdvs[pdv_id] convertido para int64')\n",
    "\n",
    "# Limpar memória\n",
    "del train_data, validation_data\n",
    "gc.collect()\n",
    "\n",
    "# Aplicar a função de engenharia de features\n",
    "dados_completos_com_features = criar_features_avancadas_otimizada(dados_completos, produtos, pdvs, \"DATASET_COMBINADO\")\n",
    "\n",
    "# Limpar memória\n",
    "del dados_completos, produtos, pdvs\n",
    "gc.collect()\n",
    "\n",
    "print('   ✅ Features temporais criadas.')\n",
    "\n",
    "# Passo 2: Separar novamente os conjuntos de treino e validação\n",
    "print('\\n📊 Etapa 2: Separando os datasets de volta para treino e validação...')\n",
    "semanas_validacao = sorted(dados_completos_com_features['semana_original'].unique())[-5:]\n",
    "train_features = dados_completos_com_features[~dados_completos_com_features['semana_original'].isin(semanas_validacao)].copy()\n",
    "validation_features = dados_completos_com_features[dados_completos_com_features['semana_original'].isin(semanas_validacao)].copy()\n",
    "\n",
    "# Remover coluna auxiliar\n",
    "train_features = train_features.drop('semana_original', axis=1)\n",
    "validation_features = validation_features.drop('semana_original', axis=1)\n",
    "\n",
    "# Passo CRÍTICO: Deletar o dataframe gigante e forçar a limpeza de memória\n",
    "print('   🗑️ Liberando memória do dataframe combinado...')\n",
    "del dados_completos_com_features\n",
    "gc.collect()\n",
    "print('   ✅ Memória liberada.')\n",
    "\n",
    "# Passo 3: Otimizar a memória de cada dataframe SEPARADAMENTE\n",
    "train_features = reduce_mem_usage(train_features, \"train_features\")\n",
    "validation_features = reduce_mem_usage(validation_features, \"validation_features\")\n",
    "\n",
    "# Passo 4: Salvar os arquivos um de cada vez - COM CORREÇÃO DE TIPOS\n",
    "print('\\n💾 Etapa Final: Salvando arquivos otimizados sequencialmente...')\n",
    "\n",
    "# CORREÇÃO: Aplicar fix de tipos antes de salvar\n",
    "print('   🔧 Aplicando correção de tipos para compatibilidade com Parquet...')\n",
    "train_features = fix_parquet_types(train_features)\n",
    "validation_features = fix_parquet_types(validation_features)\n",
    "\n",
    "# Salvar dados de treino\n",
    "output_path_train = '../data/submissao3/train_features.parquet'\n",
    "train_features.to_parquet(output_path_train, index=False)\n",
    "print(f'   ✅ Dados de TREINO salvos em: {output_path_train}')\n",
    "print(f'      Shape: {train_features.shape}, Colunas: {len(train_features.columns)}')\n",
    "\n",
    "# Liberar memória do treino antes de salvar a validação\n",
    "del train_features\n",
    "gc.collect()\n",
    "\n",
    "# Salvar dados de validação\n",
    "output_path_validation = '../data/submissao3/validation_features.parquet'\n",
    "validation_features.to_parquet(output_path_validation, index=False)\n",
    "print(f'   ✅ Dados de VALIDAÇÃO salvos em: {output_path_validation}')\n",
    "print(f'      Shape: {validation_features.shape}, Colunas: {len(validation_features.columns)}')\n",
    "\n",
    "# Liberar memória da validação\n",
    "del validation_features\n",
    "gc.collect()\n",
    "\n",
    "# Salvar metadados (código original mantido)\n",
    "print('\\n📋 Salvando metadados das features...')\n",
    "import pickle\n",
    "\n",
    "metadados_features = {\n",
    "    'data_criacao': pd.Timestamp.now(),\n",
    "    'estrategia': 'Engenharia de Features Avançada para Modelo de Dois Estágios',\n",
    "    'total_features': len(pd.read_parquet(output_path_train).columns),\n",
    "    'target_classificacao': 'vendeu (1 se quantidade > 0, 0 caso contrário)',\n",
    "    'target_regressao': 'quantidade (apenas onde vendeu = 1)',\n",
    "    'tipos_features': [\n",
    "        'Target \"vendeu\" para classificação',\n",
    "        'Features de preço (unitário, lags, médias)',\n",
    "        'Features de calendário (semana, mês)',\n",
    "        'Features de lag de vendas (1 a 4 semanas)',\n",
    "        'Features de rolling window (médias, std dev)',\n",
    "        'Features EWMA (médias ponderadas exponencialmente)',\n",
    "        'Features de hierarquia (agregados por categoria/zipcode)',\n",
    "        'Features de tendência como momentum e aceleração',\n",
    "        'Features hash criam embeddings para variáveis categóricas'\n",
    "    ],\n",
    "    'observacoes': [\n",
    "        'Processamento sequencial para evitar esgotamento de memória',\n",
    "        'Otimização de tipos de dados aplicada',\n",
    "        'Features de Fase 2 incluídas: EWMA, calendário avançado, preço relativo',\n",
    "        'Compatibilidade de tipos garantida para merges',\n",
    "        'Correção de tipos aplicada para salvamento em Parquet'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../data/submissao3/advanced_features_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_features, f)\n",
    "\n",
    "print('   ✅ Metadados salvos em: data/submissao3/advanced_features_metadata.pkl')\n",
    "\n",
    "print('\\n🎉 ENGENHARIA DE FEATURES AVANÇADA CONCLUÍDA (COM OTIMIZAÇÃO DE MEMÓRIA)!')\n",
    "print('=' * 80)\n",
    "print('🎯 Features criadas para modelo de dois estágios:')\n",
    "print('   📊 CLASSIFICAÇÃO: Use target \"vendeu\" para treinar se vai vender')\n",
    "print('   📊 REGRESSÃO: Use target \"quantidade\" (apenas onde vendeu=1) para quanto vai vender')\n",
    "print('\\n💡 Próximos passos:')\n",
    "print('   1. Treinar modelo de classificação (LGBMClassifier)')\n",
    "print('   2. Treinar modelo de regressão (LGBMRegressor)')\n",
    "print('   3. Combinar os dois no pipeline final')\n",
    "print('\\n🚀 Dados prontos para modelagem avançada!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Analisando features criadas SEM TARGET LEAKAGE...\n",
      "📂 Carregando dados salvos para análise...\n",
      "\n",
      "📋 Features disponíveis (54):\n",
      "\n",
      "🏷️ Básicas (6):\n",
      "   • pdv_id\n",
      "   • produto_id\n",
      "   • quantidade\n",
      "   • faturamento\n",
      "   • distributor_id\n",
      "\n",
      "🏷️ Target (1):\n",
      "   • vendeu\n",
      "\n",
      "🏷️ Auxiliares (3):\n",
      "   • categoria\n",
      "   • zipcode\n",
      "   • tipo_loja\n",
      "\n",
      "🏷️ Preço_SEM_Leakage (2):\n",
      "   • preco_lag_1\n",
      "   • preco_lag_2\n",
      "\n",
      "🏷️ Preço_PARA_Regressão (2):\n",
      "   • preco_unitario_atual\n",
      "   • preco_medio_semanal_sku_atual\n",
      "\n",
      "🏷️ Calendário (12):\n",
      "   • quantidade_media_4w\n",
      "   • semana_do_ano\n",
      "   • eh_primeira_semana_mes\n",
      "   • media_vendas_categoria_pdv_lag_1\n",
      "   • dia_do_mes\n",
      "   • semana_do_mes\n",
      "   • eh_inicio_mes\n",
      "   • eh_fim_mes\n",
      "   • mes\n",
      "   • mes_sin\n",
      "   • mes_cos\n",
      "   • media_vendas_categoria_pdv_atual\n",
      "\n",
      "🏷️ Lag (6):\n",
      "   • quantidade_lag_1\n",
      "   • quantidade_lag_2\n",
      "   • quantidade_lag_3\n",
      "   • quantidade_lag_4\n",
      "   • media_vendas_categoria_pdv_lag_1\n",
      "   • share_vendas_sku_categoria_lag_1\n",
      "\n",
      "🏷️ Rolling (3):\n",
      "   • quantidade_media_4w\n",
      "   • quantidade_std_4w\n",
      "   • quantidade_max_4w\n",
      "\n",
      "🏷️ Hierarquia_SEM_Leakage (2):\n",
      "   • media_vendas_categoria_pdv_lag_1\n",
      "   • share_vendas_sku_categoria_lag_1\n",
      "\n",
      "🏷️ Hierarquia_PARA_Regressão (2):\n",
      "   • media_vendas_categoria_pdv_atual\n",
      "   • share_vendas_sku_categoria_atual\n",
      "\n",
      "🏷️ Tendência (3):\n",
      "   • momentum_ratio\n",
      "   • momentum_ratio_ewma\n",
      "   • aceleracao\n",
      "\n",
      "🏷️ Hash (6):\n",
      "   • pdv_hash\n",
      "   • produto_hash\n",
      "   • categoria_hash\n",
      "   • zipcode_hash\n",
      "   • pdv_produto_hash\n",
      "   • categoria_zipcode_hash\n",
      "\n",
      "🚨 ANÁLISE DE TARGET LEAKAGE:\n",
      "✅ Features SEGURAS para classificação: 44\n",
      "   • categoria\n",
      "   • zipcode\n",
      "   • tipo_loja\n",
      "   • preco_lag_1\n",
      "   • preco_lag_2\n",
      "   • variacao_preco_sku_semanal\n",
      "   • quantidade_lag_1\n",
      "   • quantidade_lag_2\n",
      "   • quantidade_lag_3\n",
      "   • quantidade_lag_4\n",
      "   • ... e mais 34 features\n",
      "\n",
      "⚠️ Features COM LEAKAGE (apenas para regressão): 4\n",
      "   • preco_unitario_atual\n",
      "   • preco_medio_semanal_sku_atual\n",
      "   • media_vendas_categoria_pdv_atual\n",
      "   • share_vendas_sku_categoria_atual\n",
      "\n",
      "📊 Estatísticas do conjunto de treino (SEM LEAKAGE):\n",
      "   • Shape: (50126880, 54)\n",
      "   • Target vendeu: 5,543,608 positivos (11.1%)\n",
      "   • Quantidade média: 1.00\n",
      "   • Registros com lag_4: 5,077,534\n",
      "\n",
      "📊 Estatísticas do conjunto de validação (SEM LEAKAGE):\n",
      "   • Shape: (5221550, 54)\n",
      "   • Target vendeu: 571,729 positivos (10.9%)\n",
      "   • Quantidade média: 0.57\n",
      "   • Registros com lag_4: 587,975\n",
      "✅ Análise das features SEM TARGET LEAKAGE concluída\n"
     ]
    }
   ],
   "source": [
    "# Analisar features criadas SEM TARGET LEAKAGE\n",
    "print('📊 Analisando features criadas SEM TARGET LEAKAGE...')\n",
    "\n",
    "# Carregar os dados salvos para análise (já que foram deletados para economizar memória)\n",
    "print('📂 Carregando dados salvos para análise...')\n",
    "train_features = pd.read_parquet('../data/submissao3/train_features.parquet')\n",
    "validation_features = pd.read_parquet('../data/submissao3/validation_features.parquet')\n",
    "\n",
    "# Listar todas as features\n",
    "features_disponiveis = list(train_features.columns)\n",
    "print(f'\\n📋 Features disponíveis ({len(features_disponiveis)}):')\n",
    "\n",
    "# Categorizar features CORRIGIDAS\n",
    "features_categorias = {\n",
    "    'Básicas': ['semana', 'pdv_id', 'produto_id', 'quantidade', 'faturamento', 'distributor_id'],\n",
    "    'Target': ['vendeu'],\n",
    "    'Auxiliares': ['categoria', 'zipcode', 'tipo_loja'],\n",
    "    'Preço_SEM_Leakage': [col for col in features_disponiveis if 'preco' in col and 'lag' in col],\n",
    "    'Preço_PARA_Regressão': [col for col in features_disponiveis if 'preco' in col and 'atual' in col],\n",
    "    'Calendário': [col for col in features_disponiveis if any(x in col for x in ['dia', 'semana_do', 'mes', 'inicio', 'fim'])],\n",
    "    'Lag': [col for col in features_disponiveis if 'lag_' in col and 'preco' not in col],\n",
    "    'Rolling': [col for col in features_disponiveis if any(x in col for x in ['media_4w', 'std_4w', 'max_4w'])],\n",
    "    'Hierarquia_SEM_Leakage': [col for col in features_disponiveis if any(x in col for x in ['media_vendas', 'share']) and 'lag' in col],\n",
    "    'Hierarquia_PARA_Regressão': [col for col in features_disponiveis if any(x in col for x in ['media_vendas', 'share']) and 'atual' in col],\n",
    "    'Tendência': [col for col in features_disponiveis if any(x in col for x in ['momentum', 'aceleracao'])],\n",
    "    'Hash': [col for col in features_disponiveis if 'hash' in col]\n",
    "}\n",
    "\n",
    "for categoria, features in features_categorias.items():\n",
    "    if features:\n",
    "        print(f'\\n🏷️ {categoria} ({len(features)}):')\n",
    "        for feat in features:\n",
    "            if feat in features_disponiveis:\n",
    "                print(f'   • {feat}')\n",
    "\n",
    "print('\\n🚨 ANÁLISE DE TARGET LEAKAGE:')\n",
    "features_sem_leakage = []\n",
    "features_com_leakage = []\n",
    "\n",
    "for feat in features_disponiveis:\n",
    "    if feat in ['semana', 'pdv_id', 'produto_id', 'quantidade', 'faturamento', 'distributor_id', 'vendeu']:\n",
    "        continue  # Básicas, não contar\n",
    "    elif 'atual' in feat:\n",
    "        features_com_leakage.append(feat)  # Para regressão apenas\n",
    "    else:\n",
    "        features_sem_leakage.append(feat)  # Seguras para classificação\n",
    "\n",
    "print(f'✅ Features SEGURAS para classificação: {len(features_sem_leakage)}')\n",
    "for feat in features_sem_leakage[:10]:  # Mostrar apenas primeiras 10\n",
    "    print(f'   • {feat}')\n",
    "if len(features_sem_leakage) > 10:\n",
    "    print(f'   • ... e mais {len(features_sem_leakage)-10} features')\n",
    "\n",
    "print(f'\\n⚠️ Features COM LEAKAGE (apenas para regressão): {len(features_com_leakage)}')\n",
    "for feat in features_com_leakage:\n",
    "    print(f'   • {feat}')\n",
    "\n",
    "print('\\n📊 Estatísticas do conjunto de treino (SEM LEAKAGE):')\n",
    "print(f'   • Shape: {train_features.shape}')\n",
    "print(f'   • Target vendeu: {train_features[\"vendeu\"].sum():,} positivos ({train_features[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   • Quantidade média: {train_features[\"quantidade\"].mean():.2f}')\n",
    "print(f'   • Registros com lag_4: {(train_features[\"quantidade_lag_4\"] > 0).sum():,}')\n",
    "\n",
    "print('\\n📊 Estatísticas do conjunto de validação (SEM LEAKAGE):')\n",
    "print(f'   • Shape: {validation_features.shape}')\n",
    "print(f'   • Target vendeu: {validation_features[\"vendeu\"].sum():,} positivos ({validation_features[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   • Quantidade média: {validation_features[\"quantidade\"].mean():.2f}')\n",
    "print(f'   • Registros com lag_4: {(validation_features[\"quantidade_lag_4\"] > 0).sum():,}')\n",
    "\n",
    "print('✅ Análise das features SEM TARGET LEAKAGE concluída')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Regenerando features com CORREÇÃO de target leakage...\n",
      "📂 Recarregando dados originais...\n",
      "🔗 Combinando dados para criar features temporais...\n",
      "⚡ Aplicando engenharia de features CORRIGIDA...\n",
      "\\n🔧 Criando features avançadas para DATASET_CORRIGIDO (VERSÃO CORRIGIDA - SEM TARGET LEAKAGE)...\n",
      "   🎯 Otimização inicial de tipos de dados (verificando limites)...\n",
      "      🔍 pdv_id: tipo=object\n",
      "         Exemplos: ['1008240744247283174', '1008240744247283174', '10097752152132198']\n",
      "   🔄 Coluna pdv_id: convertida de string para numérico\n",
      "   ⚠️ Coluna pdv_id: valores muito grandes para int32, mantendo int64\n",
      "      🔍 produto_id: tipo=object\n",
      "         Exemplos: ['1938760505411922162', '4098058333001424920', '1938760505411922162']\n",
      "   🔄 Coluna produto_id: convertida de string para numérico\n",
      "   ⚠️ Coluna produto_id: valores muito grandes para int32, mantendo int64\n",
      "      🔍 distributor_id: tipo=float32\n",
      "         Range: 4.0 até 11.0\n",
      "   ✅ Otimização inicial concluída.\n",
      "   🎯 Criando target e fazendo merge com dados auxiliares...\n",
      "   ✅ Target e merges concluídos.\n",
      "   🔄 Ordenando dados para consistência temporal...\n",
      "   💰 Criando features de preço (COM CORREÇÃO DE LEAKAGE)...\n",
      "      propagando preços para semanas sem vendas...\n",
      "   ✅ Features de preço criadas e corrigidas contra leakage.\n",
      "   📊 Criando features de lag e rolling (ETAPA CRÍTICA - SEM TARGET LEAKAGE)...\n",
      "   ✅ Features de lag e rolling criadas SEM TARGET LEAKAGE.\n",
      "   🔄 Adicionando features EWMA (Fase 2 otimizada - SEM TARGET LEAKAGE)...\n",
      "   📅 Adicionando features de calendário avançadas (Fase 2 otimizada)...\n",
      "   💸 Adicionando features de preço relativo (Fase 2 otimizada)...\n",
      "   ✅ Features da Fase 2 criadas e corrigidas contra target leakage.\n",
      "   📈 Criando features de tendência e hierarquia (otimizadas - SEM TARGET LEAKAGE)...\n",
      "   ✅ Features de tendência criadas SEM TARGET LEAKAGE.\n",
      "   📅 Criando features de calendário básicas (otimizadas)...\n",
      "   🎯 Criando features específicas para REGRESSÃO (otimizadas)...\n",
      "   🏷️ Criando features categóricas (otimizadas)...\n",
      "   🧹 Aplicando limpeza final e removendo coluna datetime pesada...\n",
      "   ✅ DATASET_CORRIGIDO - Features criadas com sucesso: (55348430, 55)\n",
      "   🆕 Inclui todas as features da Fase 2: EWMA, calendário avançado, preço relativo\n",
      "   🛡️ Memória otimizada a cada etapa - BLINDADO contra crash!\n",
      "   🔒 Downcasting seguro aplicado - conversão automática de strings para numérico\n",
      "   📅 Coluna semana_original preservada para separação treino/validação\n",
      "   🔧 CORREÇÃO DE TARGET LEAKAGE APLICADA nas ETAPAS 5, 6 e 7\n",
      "   ✅ Rolling, EWMA e features derivadas baseadas em quantidade_lag_1\n",
      "📊 Separando datasets corrigidos...\n",
      "🔄 Otimizando memória para train_features_corrigido...\n",
      "   📊 Memória inicial: 9465.33 MB\n",
      "   ✅ Memória final: 9130.70 MB (3.5% de redução)\n",
      "🔄 Otimizando memória para validation_features_corrigido...\n",
      "   📊 Memória inicial: 985.97 MB\n",
      "   ✅ Memória final: 941.16 MB (4.5% de redução)\n",
      "💾 Salvando dados CORRIGIDOS...\n",
      "✅ Arquivos CORRIGIDOS salvos:\n",
      "   • data/submissao3/train_features_CORRIGIDO.parquet\n",
      "   • data/submissao3/validation_features_CORRIGIDO.parquet\n",
      "\\n📊 ANÁLISE COMPARATIVA - ANTES vs DEPOIS DA CORREÇÃO:\n",
      "📐 Shape treino: (50126880, 54)\n",
      "📐 Shape validação: (5221550, 54)\n",
      "\\n🔍 Análise de features críticas (primeiros 10 registros):\n",
      "   quantidade_media_4w: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "   quantidade_std_4w: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "   quantidade_ewma_4w: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "   momentum_ratio: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "\\n🎯 Estatísticas do target:\n",
      "   Treino vendeu=1: 5,543,608 (11.1%)\n",
      "   Validação vendeu=1: 571,729 (10.9%)\n",
      "📋 Metadados da correção salvos!\n",
      "\\n🎉 CORREÇÃO DE TARGET LEAKAGE CONCLUÍDA!\n",
      "======================================================================\n",
      "✅ Próximo passo: Re-treinar o modelo de classificação com dados corrigidos\n",
      "📈 Espere métricas mais realistas (AUC ~0.85-0.95, F1 ~0.6-0.8)\n",
      "🧹 Memória limpa. Pronto para próximos passos!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# REGENERAR FEATURES CORRIGIDAS - SEM TARGET LEAKAGE\n",
    "# ============================================================================\n",
    "\n",
    "print('🔧 Regenerando features com CORREÇÃO de target leakage...')\n",
    "\n",
    "# Limpar dados anteriores\n",
    "import gc\n",
    "if 'train_features' in locals():\n",
    "    del train_features\n",
    "if 'validation_features' in locals():\n",
    "    del validation_features\n",
    "gc.collect()\n",
    "\n",
    "# Carregar dados originais novamente\n",
    "print('📂 Recarregando dados originais...')\n",
    "train_data = pd.read_parquet('../data/submissao3/train_data.parquet')\n",
    "validation_data = pd.read_parquet('../data/submissao3/validation_data.parquet')\n",
    "\n",
    "# Carregar dados auxiliares\n",
    "produtos = pd.read_parquet(\n",
    "    '../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet'\n",
    ")\n",
    "produtos = produtos.rename(columns={'produto': 'produto_id'})\n",
    "\n",
    "pdvs = pd.read_parquet(\n",
    "    '../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
    ")\n",
    "pdvs = pdvs.rename(columns={'pdv': 'pdv_id', 'categoria_pdv': 'tipo_loja'})\n",
    "\n",
    "# Converter tipos dos dados auxiliares\n",
    "if produtos['produto_id'].dtype == 'object':\n",
    "    produtos['produto_id'] = pd.to_numeric(produtos['produto_id'], errors='coerce').astype('int64')\n",
    "if pdvs['pdv_id'].dtype == 'object':\n",
    "    pdvs['pdv_id'] = pd.to_numeric(pdvs['pdv_id'], errors='coerce').astype('int64')\n",
    "\n",
    "# Combinar dados para criar features temporais consistentes\n",
    "print('🔗 Combinando dados para criar features temporais...')\n",
    "dados_completos = pd.concat([train_data, validation_data], ignore_index=True)\n",
    "dados_completos['semana_original'] = dados_completos['semana']\n",
    "\n",
    "del train_data, validation_data\n",
    "gc.collect()\n",
    "\n",
    "# Aplicar a função CORRIGIDA\n",
    "print('⚡ Aplicando engenharia de features CORRIGIDA...')\n",
    "dados_completos_corrigidos = criar_features_avancadas_otimizada(\n",
    "    dados_completos, produtos, pdvs, \"DATASET_CORRIGIDO\"\n",
    ")\n",
    "\n",
    "del dados_completos, produtos, pdvs\n",
    "gc.collect()\n",
    "\n",
    "# Separar novamente\n",
    "print('📊 Separando datasets corrigidos...')\n",
    "semanas_validacao = sorted(dados_completos_corrigidos['semana_original'].unique())[-5:]\n",
    "train_features_corrigido = dados_completos_corrigidos[\n",
    "    ~dados_completos_corrigidos['semana_original'].isin(semanas_validacao)\n",
    "].copy()\n",
    "validation_features_corrigido = dados_completos_corrigidos[\n",
    "    dados_completos_corrigidos['semana_original'].isin(semanas_validacao)\n",
    "].copy()\n",
    "\n",
    "# Remover coluna auxiliar\n",
    "train_features_corrigido = train_features_corrigido.drop('semana_original', axis=1)\n",
    "validation_features_corrigido = validation_features_corrigido.drop('semana_original', axis=1)\n",
    "\n",
    "del dados_completos_corrigidos\n",
    "gc.collect()\n",
    "\n",
    "# Otimizar memória\n",
    "train_features_corrigido = reduce_mem_usage(train_features_corrigido, \"train_features_corrigido\")\n",
    "validation_features_corrigido = reduce_mem_usage(validation_features_corrigido, \"validation_features_corrigido\")\n",
    "\n",
    "# Salvar dados corrigidos\n",
    "print('💾 Salvando dados CORRIGIDOS...')\n",
    "\n",
    "def fix_parquet_types(df):\n",
    "    \"\"\"Fix tipos problemáticos antes de salvar em parquet\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    for col in df_fixed.columns:\n",
    "        if df_fixed[col].dtype == 'object':\n",
    "            df_fixed[col] = df_fixed[col].astype(str)\n",
    "            df_fixed[col] = df_fixed[col].replace(['nan', 'None', '<NA>', 'NaN', 'null'], 'Unknown')\n",
    "    return df_fixed\n",
    "\n",
    "train_features_corrigido = fix_parquet_types(train_features_corrigido)\n",
    "validation_features_corrigido = fix_parquet_types(validation_features_corrigido)\n",
    "\n",
    "# Salvar arquivos corrigidos\n",
    "train_features_corrigido.to_parquet('../data/submissao3/train_features_CORRIGIDO.parquet', index=False)\n",
    "validation_features_corrigido.to_parquet('../data/submissao3/validation_features_CORRIGIDO.parquet', index=False)\n",
    "\n",
    "print('✅ Arquivos CORRIGIDOS salvos:')\n",
    "print('   • data/submissao3/train_features_CORRIGIDO.parquet')\n",
    "print('   • data/submissao3/validation_features_CORRIGIDO.parquet')\n",
    "\n",
    "# Análise rápida das diferenças\n",
    "print('\\\\n📊 ANÁLISE COMPARATIVA - ANTES vs DEPOIS DA CORREÇÃO:')\n",
    "print(f'📐 Shape treino: {train_features_corrigido.shape}')\n",
    "print(f'📐 Shape validação: {validation_features_corrigido.shape}')\n",
    "\n",
    "# Testar algumas features críticas\n",
    "exemplos_features = ['quantidade_media_4w', 'quantidade_std_4w', 'quantidade_ewma_4w', 'momentum_ratio']\n",
    "print('\\\\n🔍 Análise de features críticas (primeiros 10 registros):')\n",
    "\n",
    "for feat in exemplos_features:\n",
    "    if feat in train_features_corrigido.columns:\n",
    "        valores = train_features_corrigido[feat].head(10)\n",
    "        print(f'   {feat}: {valores.tolist()[:5]} ...')\n",
    "\n",
    "print('\\\\n🎯 Estatísticas do target:')\n",
    "print(f'   Treino vendeu=1: {train_features_corrigido[\"vendeu\"].sum():,} ({train_features_corrigido[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   Validação vendeu=1: {validation_features_corrigido[\"vendeu\"].sum():,} ({validation_features_corrigido[\"vendeu\"].mean()*100:.1f}%)')\n",
    "\n",
    "# Salvar metadados da correção\n",
    "import pickle\n",
    "metadados_correcao = {\n",
    "    'data_correcao': pd.Timestamp.now(),\n",
    "    'versao': 'CORRIGIDA - SEM TARGET LEAKAGE',\n",
    "    'principais_correcoes': [\n",
    "        'Rolling features baseadas em quantidade_lag_1 (não quantidade atual)',\n",
    "        'EWMA features baseadas em quantidade_lag_1 (não quantidade atual)', \n",
    "        'Features derivadas (momentum, volatilidade) usam features corrigidas',\n",
    "        'Separação clara entre features para classificação vs regressão'\n",
    "    ],\n",
    "    'features_seguras_classificacao': [f for f in train_features_corrigido.columns \n",
    "                                       if not any(x in f for x in ['atual', 'quantidade']) or 'lag' in f],\n",
    "    'total_features': len(train_features_corrigido.columns)\n",
    "}\n",
    "\n",
    "with open('../data/submissao3/correcao_target_leakage_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_correcao, f)\n",
    "\n",
    "print('📋 Metadados da correção salvos!')\n",
    "print('\\\\n🎉 CORREÇÃO DE TARGET LEAKAGE CONCLUÍDA!')\n",
    "print('=' * 70)\n",
    "print('✅ Próximo passo: Re-treinar o modelo de classificação com dados corrigidos')\n",
    "print('📈 Espere métricas mais realistas (AUC ~0.85-0.95, F1 ~0.6-0.8)')\n",
    "\n",
    "# Limpar variáveis para economizar memória\n",
    "del train_features_corrigido, validation_features_corrigido\n",
    "gc.collect()\n",
    "\n",
    "print('🧹 Memória limpa. Pronto para próximos passos!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
