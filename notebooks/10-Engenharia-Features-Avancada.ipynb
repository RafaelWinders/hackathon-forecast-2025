{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Engenharia de Features Avan√ßada\n",
    "\n",
    "Este notebook cria features avan√ßadas baseadas nos dados de treino e valida√ß√£o temporal.\n",
    "\n",
    "## Novas Features:\n",
    "- **Target de Classifica√ß√£o**: `vendeu` (1 se quantidade > 0, 0 caso contr√°rio)\n",
    "- **Features de Pre√ßo**: pre√ßo unit√°rio, varia√ß√µes, m√©dias\n",
    "- **Features de Calend√°rio**: dia do m√™s, semana do m√™s, flags temporais\n",
    "- **Features de Hierarquia**: agrega√ß√µes por categoria, zipcode\n",
    "- **Features de Tend√™ncia**: momentum, acelera√ß√£o de vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando Engenharia de Features Avan√ßada\n",
      "üéØ Objetivo: Criar vari√°veis preditivas poderosas para o modelo de dois est√°gios\n",
      "üìÅ Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('üöÄ Iniciando Engenharia de Features Avan√ßada')\n",
    "print('üéØ Objetivo: Criar vari√°veis preditivas poderosas para o modelo de dois est√°gios')\n",
    "\n",
    "# Criar pasta submissao3 se n√£o existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('üìÅ Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados de Treino e Valida√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados de treino e valida√ß√£o...\n",
      "üèãÔ∏è Dados de treino: (50126880, 6)\n",
      "üîç Dados de valida√ß√£o: (5221550, 6)\n",
      "üìÖ Treino: 2021-12-28 00:00:00 at√© 2022-11-22 00:00:00\n",
      "üìÖ Valida√ß√£o: 2022-11-29 00:00:00 at√© 2022-12-27 00:00:00\n",
      "‚úÖ Dados carregados com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados de treino e valida√ß√£o\n",
    "print('üìÇ Carregando dados de treino e valida√ß√£o...')\n",
    "\n",
    "train_data = pd.read_parquet('../data/submissao3/train_data.parquet')\n",
    "validation_data = pd.read_parquet('../data/submissao3/validation_data.parquet')\n",
    "\n",
    "print(f'üèãÔ∏è Dados de treino: {train_data.shape}')\n",
    "print(f'üîç Dados de valida√ß√£o: {validation_data.shape}')\n",
    "\n",
    "# Verificar per√≠odo\n",
    "print(f'üìÖ Treino: {train_data[\"semana\"].min()} at√© {train_data[\"semana\"].max()}')\n",
    "print(f'üìÖ Valida√ß√£o: {validation_data[\"semana\"].min()} at√© {validation_data[\"semana\"].max()}')\n",
    "\n",
    "print('‚úÖ Dados carregados com sucesso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados auxiliares...\n",
      "üìä Produtos: (7092, 8) - Categorias: 7\n",
      "üìä PDVs: (14419, 4) - Zipcodes: 788\n",
      "‚úÖ Dados auxiliares carregados\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados de produtos e PDVs para features adicionais\n",
    "print('üìÇ Carregando dados auxiliares...')\n",
    "\n",
    "# Produtos\n",
    "produtos = pd.read_parquet(\n",
    "    '../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet'\n",
    ")\n",
    "produtos = produtos.rename(columns={\n",
    "    'produto': 'produto_id'\n",
    "})\n",
    "\n",
    "# PDVs\n",
    "pdvs = pd.read_parquet(\n",
    "    '../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
    ")\n",
    "pdvs = pdvs.rename(columns={\n",
    "    'pdv': 'pdv_id',\n",
    "    'categoria_pdv': 'tipo_loja'\n",
    "})\n",
    "\n",
    "print(f'üìä Produtos: {produtos.shape} - Categorias: {produtos[\"categoria\"].nunique()}')\n",
    "print(f'üìä PDVs: {pdvs.shape} - Zipcodes: {pdvs[\"zipcode\"].nunique()}')\n",
    "\n",
    "print('‚úÖ Dados auxiliares carregados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fun√ß√£o de Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√£o CORRIGIDA contra TARGET LEAKAGE definida!\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# FUN√á√ÉO BLINDADA CONTRA CRASH DE MEM√ìRIA - DOWNCASTING INCREMENTAL SEGURO\n",
    "# =================================================================================\n",
    "\n",
    "def reduce_mem_usage(df, name):\n",
    "    \"\"\"\n",
    "    Reduz o uso de mem√≥ria de um dataframe, alterando os tipos de dados das colunas.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'üîÑ Otimizando mem√≥ria para {name}...')\n",
    "    print(f'   üìä Mem√≥ria inicial: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Para floats, vamos usar float32 que √© suficiente para a maioria dos casos\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'   ‚úÖ Mem√≥ria final: {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% de redu√ß√£o)')\n",
    "    return df\n",
    "\n",
    "def safe_downcast_column(series, target_dtype):\n",
    "    \"\"\"\n",
    "    Faz downcast seguro de uma coluna, verificando limites antes.\n",
    "    Converte strings para num√©rico se necess√°rio.\n",
    "    \"\"\"\n",
    "    # Se for string, converter para num√©rico primeiro\n",
    "    if series.dtype == 'object':\n",
    "        try:\n",
    "            series = pd.to_numeric(series, errors='coerce')\n",
    "            print(f'   üîÑ Coluna {series.name}: convertida de string para num√©rico')\n",
    "        except:\n",
    "            print(f'   ‚ö†Ô∏è Coluna {series.name}: n√£o foi poss√≠vel converter para num√©rico')\n",
    "            return series\n",
    "    \n",
    "    if target_dtype == 'int32':\n",
    "        if series.min() >= np.iinfo(np.int32).min and series.max() <= np.iinfo(np.int32).max:\n",
    "            return series.astype('int32')\n",
    "        else:\n",
    "            print(f'   ‚ö†Ô∏è Coluna {series.name}: valores muito grandes para int32, mantendo int64')\n",
    "            return series.astype('int64')\n",
    "    elif target_dtype == 'float32':\n",
    "        if series.min() >= np.finfo(np.float32).min and series.max() <= np.finfo(np.float32).max:\n",
    "            return series.astype('float32')\n",
    "        else:\n",
    "            return series.astype('float64')\n",
    "    else:\n",
    "        return series.astype(target_dtype)\n",
    "\n",
    "def criar_features_avancadas_otimizada(dados, dados_produtos, dados_pdvs, nome_conjunto=\"DATASET\"):\n",
    "    \"\"\"\n",
    "    Cria features avan√ßadas com downcasting incremental SEGURO para controle m√°ximo de mem√≥ria.\n",
    "    VERS√ÉO BLINDADA: Otimiza mem√≥ria a cada etapa para evitar crash, verificando limites.\n",
    "    VERS√ÉO CORRIGIDA: Remove target leakage das features rolantes.\n",
    "    \"\"\"\n",
    "    print(f'\\\\nüîß Criando features avan√ßadas para {nome_conjunto} (VERS√ÉO CORRIGIDA - SEM TARGET LEAKAGE)...')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 1: OTIMIZA√á√ÉO INICIAL DOS DADOS BASE (SEGURA)\n",
    "    # ============================================================================\n",
    "    print('   üéØ Otimiza√ß√£o inicial de tipos de dados (verificando limites)...')\n",
    "    \n",
    "    # Verificar e downcast de IDs com seguran√ßa\n",
    "    for col in ['pdv_id', 'produto_id', 'distributor_id']:\n",
    "        if col in dados.columns:\n",
    "            print(f'      üîç {col}: tipo={dados[col].dtype}')\n",
    "            # Se for string, mostrar alguns valores de exemplo\n",
    "            if dados[col].dtype == 'object':\n",
    "                print(f'         Exemplos: {dados[col].head(3).tolist()}')\n",
    "            else:\n",
    "                print(f'         Range: {dados[col].min()} at√© {dados[col].max()}')\n",
    "            dados[col] = safe_downcast_column(dados[col], 'int32')\n",
    "    \n",
    "    if 'quantidade' in dados.columns:\n",
    "        dados['quantidade'] = safe_downcast_column(dados['quantidade'], 'int32')\n",
    "    if 'faturamento' in dados.columns:\n",
    "        dados['faturamento'] = safe_downcast_column(dados['faturamento'], 'float32')\n",
    "    \n",
    "    print('   ‚úÖ Otimiza√ß√£o inicial conclu√≠da.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 2: TARGET E MERGES COM DOWNCASTING IMEDIATO\n",
    "    # ============================================================================\n",
    "    print('   üéØ Criando target e fazendo merge com dados auxiliares...')\n",
    "    \n",
    "    # Target como int8 (muito mais eficiente)\n",
    "    dados['vendeu'] = (dados['quantidade'] > 0).astype('int8')\n",
    "    \n",
    "    # Merge com dados auxiliares\n",
    "    dados = dados.merge(dados_produtos[['produto_id', 'categoria']], on='produto_id', how='left')\n",
    "    dados = dados.merge(dados_pdvs[['pdv_id', 'zipcode', 'tipo_loja']], on='pdv_id', how='left')\n",
    "    \n",
    "    # CORRE√á√ÉO: Verificar se semana_original existe antes de sobrescrever\n",
    "    if 'semana_original' not in dados.columns:\n",
    "        dados['semana_original'] = dados['semana']\n",
    "        print('   üìÖ Coluna semana_original criada para preservar separa√ß√£o treino/valida√ß√£o')\n",
    "    \n",
    "    # Converter para datetime apenas temporariamente para criar features\n",
    "    dados['semana'] = pd.to_datetime(dados['semana'])\n",
    "    \n",
    "    print('   ‚úÖ Target e merges conclu√≠dos.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 3: ORDENA√á√ÉO E PREPARA√á√ÉO\n",
    "    # ============================================================================\n",
    "    print('   üîÑ Ordenando dados para consist√™ncia temporal...')\n",
    "    dados.sort_values(['pdv_id', 'produto_id', 'semana'], inplace=True)\n",
    "    dados.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gb_cols = ['pdv_id', 'produto_id']\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # ETAPA 4: FEATURES DE PRE√áO COM DOWNCASTING IMEDIATO (VERS√ÉO CORRIGIDA)\n",
    "    # ===========================================================================\n",
    "    print('   üí∞ Criando features de pre√ßo (COM CORRE√á√ÉO DE LEAKAGE)...')\n",
    "\n",
    "    # 1. Calcule o pre√ßo unit√°rio apenas onde a venda ocorreu, o resto fica NaN.\n",
    "    # Isso quebra a liga√ß√£o direta com o target 'vendeu'.\n",
    "    preco_unitario_temp = np.where(\n",
    "        dados['quantidade'] > 0,\n",
    "        dados['faturamento'] / dados['quantidade'],\n",
    "        np.nan  # Usar NaN em vez de 0 √© a chave da corre√ß√£o!\n",
    "    )\n",
    "    dados['preco_unitario_atual'] = preco_unitario_temp.astype('float32')\n",
    "\n",
    "    # 2. Propague o √∫ltimo pre√ßo conhecido para frente (forward fill) para cada item/loja.\n",
    "    # Isso simula a realidade: o pre√ßo permanece o mesmo at√© ser alterado.\n",
    "    # Usamos bfill() em seguida para cobrir casos onde as primeiras semanas s√£o NaN.\n",
    "    print('      propagando pre√ßos para semanas sem vendas...')\n",
    "    dados['preco_unitario_atual'] = dados.groupby(gb_cols)['preco_unitario_atual'].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    # 3. Se ainda houver NaNs (itens que nunca venderam), preencha com 0.\n",
    "    dados['preco_unitario_atual'].fillna(0, inplace=True)\n",
    "\n",
    "    # Lags de pre√ßo (agora calculados sobre o pre√ßo corrigido e sem leakage)\n",
    "    gb_preco = dados.groupby(gb_cols)['preco_unitario_atual']\n",
    "    dados['preco_lag_1'] = gb_preco.shift(1).astype('float32')\n",
    "    dados['preco_lag_2'] = gb_preco.shift(2).astype('float32')\n",
    "    dados['variacao_preco_sku_semanal'] = (dados['preco_lag_1'] - dados['preco_lag_2']).fillna(0).astype('float32')\n",
    "\n",
    "    print('   ‚úÖ Features de pre√ßo criadas e corrigidas contra leakage.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 5: FEATURES DE LAG E ROLLING (CR√çTICA) - CORRE√á√ÉO TARGET LEAKAGE\n",
    "    # ============================================================================\n",
    "    print('   üìä Criando features de lag e rolling (ETAPA CR√çTICA - SEM TARGET LEAKAGE)...')\n",
    "\n",
    "    gb_quantidade = dados.groupby(gb_cols)['quantidade']\n",
    "\n",
    "    # Lags de quantidade como float32 (n√£o float64)\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        dados[f'quantidade_lag_{lag}'] = gb_quantidade.shift(lag).astype('float32')\n",
    "\n",
    "    # CORRE√á√ÉO CR√çTICA: Calcular rolling features SOBRE OS DADOS DEFASADOS\n",
    "    # Usar lag_1 como base para evitar que a semana atual influencie nas features\n",
    "    gb_quantidade_lag1 = dados.groupby(gb_cols)['quantidade_lag_1']\n",
    "\n",
    "    # Rolling features baseadas em dados do passado (sem a semana atual)\n",
    "    rolling_window_corrigido = gb_quantidade_lag1.rolling(window=4, min_periods=1)\n",
    "    dados['quantidade_media_4w'] = rolling_window_corrigido.mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    dados['quantidade_std_4w'] = rolling_window_corrigido.std().reset_index(level=gb_cols, drop=True).fillna(0).astype('float32')\n",
    "    dados['quantidade_max_4w'] = rolling_window_corrigido.max().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "\n",
    "    print('   ‚úÖ Features de lag e rolling criadas SEM TARGET LEAKAGE.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 6: FEATURES DA FASE 2 COM DOWNCASTING INCREMENTAL - CORRE√á√ÉO EWMA\n",
    "    # ============================================================================\n",
    "    print('   üîÑ Adicionando features EWMA (Fase 2 otimizada - SEM TARGET LEAKAGE)...')\n",
    "    \n",
    "    # CORRE√á√ÉO CR√çTICA: EWMA tamb√©m baseado em dados defasados\n",
    "    dados['quantidade_ewma_4w'] = gb_quantidade_lag1.ewm(span=4, adjust=False).mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    dados['quantidade_ewma_8w'] = gb_quantidade_lag1.ewm(span=8, adjust=False).mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    dados['preco_ewma_4w'] = gb_preco.ewm(span=4, adjust=False).mean().reset_index(level=gb_cols, drop=True).astype('float32')\n",
    "    \n",
    "    print('   üìÖ Adicionando features de calend√°rio avan√ßadas (Fase 2 otimizada)...')\n",
    "    dados['semana_do_ano'] = dados['semana'].dt.isocalendar().week.astype('int8')\n",
    "    dados['eh_primeira_semana_mes'] = (dados['semana'].dt.day <= 7).astype('int8')\n",
    "    dados['eh_dezembro'] = (dados['semana'].dt.month == 12).astype('int8')\n",
    "    dados['eh_janeiro'] = (dados['semana'].dt.month == 1).astype('int8')\n",
    "    dados['eh_pos_festas'] = ((dados['semana'].dt.month == 1) & (dados['semana'].dt.day <= 15)).astype('int8')\n",
    "    \n",
    "    # Features c√≠clicas como float32\n",
    "    dados['semana_ano_sin'] = np.sin(2 * np.pi * dados['semana_do_ano'] / 52).astype('float32')\n",
    "    dados['semana_ano_cos'] = np.cos(2 * np.pi * dados['semana_do_ano'] / 52).astype('float32')\n",
    "    \n",
    "    print('   üí∏ Adicionando features de pre√ßo relativo (Fase 2 otimizada)...')\n",
    "    preco_medio_categoria = dados.groupby(['semana', 'categoria'])['preco_lag_1'].transform('mean')\n",
    "    dados['preco_relativo_categoria'] = (dados['preco_lag_1'] / preco_medio_categoria).fillna(1.0).astype('float32')\n",
    "    \n",
    "    preco_medio_pdv = dados.groupby(['semana', 'pdv_id'])['preco_lag_1'].transform('mean')\n",
    "    dados['preco_relativo_pdv'] = (dados['preco_lag_1'] / preco_medio_pdv).fillna(1.0).astype('float32')\n",
    "    \n",
    "    # CORRE√á√ÉO CR√çTICA: preco_volatilidade baseada em features sem leakage\n",
    "    dados['preco_volatilidade'] = (dados['quantidade_std_4w'] / (dados['quantidade_media_4w'] + 1e-6)).astype('float32')\n",
    "    \n",
    "    print('   ‚úÖ Features da Fase 2 criadas e corrigidas contra target leakage.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 7: FEATURES DE TEND√äNCIA E HIERARQUIA OTIMIZADAS - CORRE√á√ÉO\n",
    "    # ============================================================================\n",
    "    print('   üìà Criando features de tend√™ncia e hierarquia (otimizadas - SEM TARGET LEAKAGE)...')\n",
    "    \n",
    "    dados['media_vendas_categoria_pdv_lag_1'] = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('mean').astype('float32')\n",
    "    vendas_categoria_pdv_lag = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('sum')\n",
    "    dados['share_vendas_sku_categoria_lag_1'] = (dados['quantidade_lag_1'] / vendas_categoria_pdv_lag).fillna(0).astype('float32')\n",
    "    \n",
    "    # CORRE√á√ÉO CR√çTICA: momentum baseado em features sem leakage\n",
    "    dados['momentum_ratio'] = (dados['quantidade_lag_1'] / dados['quantidade_media_4w']).fillna(0).astype('float32')\n",
    "    dados['momentum_ratio_ewma'] = (dados['quantidade_lag_1'] / dados['quantidade_ewma_4w']).fillna(0).astype('float32')\n",
    "    dados['aceleracao'] = (dados['quantidade_lag_1'] - dados['quantidade_lag_2']).fillna(0).astype('float32')\n",
    "    \n",
    "    print('   ‚úÖ Features de tend√™ncia criadas SEM TARGET LEAKAGE.')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 8: FEATURES DE CALEND√ÅRIO B√ÅSICAS OTIMIZADAS\n",
    "    # ============================================================================\n",
    "    print('   üìÖ Criando features de calend√°rio b√°sicas (otimizadas)...')\n",
    "    dados['dia_do_mes'] = dados['semana'].dt.day.astype('int8')\n",
    "    dados['semana_do_mes'] = ((dados['semana'].dt.day - 1) // 7 + 1).astype('int8')\n",
    "    dados['eh_inicio_mes'] = (dados['semana'].dt.day <= 7).astype('int8')\n",
    "    dados['eh_fim_mes'] = (dados['semana'].dt.day >= 22).astype('int8')\n",
    "    dados['mes'] = dados['semana'].dt.month.astype('int8')\n",
    "    \n",
    "    dados['mes_sin'] = np.sin(2 * np.pi * dados['mes'] / 12).astype('float32')\n",
    "    dados['mes_cos'] = np.cos(2 * np.pi * dados['mes'] / 12).astype('float32')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 9: FEATURES PARA REGRESS√ÉO OTIMIZADAS\n",
    "    # ============================================================================\n",
    "    print('   üéØ Criando features espec√≠ficas para REGRESS√ÉO (otimizadas)...')\n",
    "    dados['preco_medio_semanal_sku_atual'] = dados.groupby(['semana', 'produto_id'])['preco_unitario_atual'].transform('mean').astype('float32')\n",
    "    dados['media_vendas_categoria_pdv_atual'] = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('mean').astype('float32')\n",
    "    vendas_categoria_pdv_atual = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('sum')\n",
    "    dados['share_vendas_sku_categoria_atual'] = (dados['quantidade'] / vendas_categoria_pdv_atual).fillna(0).astype('float32')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 10: FEATURES CATEG√ìRICAS HASH OTIMIZADAS\n",
    "    # ============================================================================\n",
    "    print('   üè∑Ô∏è Criando features categ√≥ricas (otimizadas)...')\n",
    "    dados['pdv_hash'] = (dados['pdv_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados['produto_hash'] = (dados['produto_id'].astype(str).apply(hash).abs() % 100).astype('int8')\n",
    "    dados['categoria_hash'] = (dados['categoria'].astype(str).apply(hash).abs() % 50).astype('int8')\n",
    "    dados['zipcode_hash'] = (dados['zipcode'].astype(str).apply(hash).abs() % 1000).astype('int16')\n",
    "    \n",
    "    dados['pdv_produto_hash'] = (dados['pdv_hash'].astype('int16') * 100 + dados['produto_hash']).astype('int16')\n",
    "    dados['categoria_zipcode_hash'] = (dados['categoria_hash'].astype('int32') * 1000 + dados['zipcode_hash']).astype('int32')\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ETAPA 11: LIMPEZA FINAL E REMO√á√ÉO DA COLUNA DATETIME PESADA\n",
    "    # ============================================================================\n",
    "    print('   üßπ Aplicando limpeza final e removendo coluna datetime pesada...')\n",
    "    \n",
    "    # CORRE√á√ÉO: Preservar semana_original antes de remover a coluna datetime\n",
    "    # Remover apenas a coluna datetime tempor√°ria, manter semana_original\n",
    "    dados = dados.drop(columns=['semana'])\n",
    "    \n",
    "    # A coluna semana_original fica preservada para separa√ß√£o posterior\n",
    "    \n",
    "    # Limpeza de infinitos e NaNs\n",
    "    dados.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    dados.fillna(0, inplace=True)\n",
    "    \n",
    "    print(f'   ‚úÖ {nome_conjunto} - Features criadas com sucesso: {dados.shape}')\n",
    "    print('   üÜï Inclui todas as features da Fase 2: EWMA, calend√°rio avan√ßado, pre√ßo relativo')\n",
    "    print('   üõ°Ô∏è Mem√≥ria otimizada a cada etapa - BLINDADO contra crash!')\n",
    "    print('   üîí Downcasting seguro aplicado - convers√£o autom√°tica de strings para num√©rico')\n",
    "    print('   üìÖ Coluna semana_original preservada para separa√ß√£o treino/valida√ß√£o')\n",
    "    print('   üîß CORRE√á√ÉO DE TARGET LEAKAGE APLICADA nas ETAPAS 5, 6 e 7')\n",
    "    print('   ‚úÖ Rolling, EWMA e features derivadas baseadas em quantidade_lag_1')\n",
    "    \n",
    "    return dados\n",
    "\n",
    "print('‚úÖ Fun√ß√£o CORRIGIDA contra TARGET LEAKAGE definida!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aplica√ß√£o da Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Etapa 1: Criando features temporais no dataset combinado...\n",
      "   üîß Convertendo tipos dos dados auxiliares para compatibilidade...\n",
      "   ‚úÖ produtos[produto_id] convertido para int64\n",
      "   ‚úÖ pdvs[pdv_id] convertido para int64\n",
      "\\nüîß Criando features avan√ßadas para DATASET_COMBINADO (VERS√ÉO CORRIGIDA - SEM TARGET LEAKAGE)...\n",
      "   üéØ Otimiza√ß√£o inicial de tipos de dados (verificando limites)...\n",
      "      üîç pdv_id: tipo=object\n",
      "         Exemplos: ['1008240744247283174', '1008240744247283174', '10097752152132198']\n",
      "   üîÑ Coluna pdv_id: convertida de string para num√©rico\n",
      "   ‚ö†Ô∏è Coluna pdv_id: valores muito grandes para int32, mantendo int64\n",
      "      üîç produto_id: tipo=object\n",
      "         Exemplos: ['1938760505411922162', '4098058333001424920', '1938760505411922162']\n",
      "   üîÑ Coluna produto_id: convertida de string para num√©rico\n",
      "   ‚ö†Ô∏è Coluna produto_id: valores muito grandes para int32, mantendo int64\n",
      "      üîç distributor_id: tipo=float32\n",
      "         Range: 4.0 at√© 11.0\n",
      "   ‚úÖ Otimiza√ß√£o inicial conclu√≠da.\n",
      "   üéØ Criando target e fazendo merge com dados auxiliares...\n",
      "   ‚úÖ Target e merges conclu√≠dos.\n",
      "   üîÑ Ordenando dados para consist√™ncia temporal...\n",
      "   üí∞ Criando features de pre√ßo (COM CORRE√á√ÉO DE LEAKAGE)...\n",
      "      propagando pre√ßos para semanas sem vendas...\n",
      "   ‚úÖ Features de pre√ßo criadas e corrigidas contra leakage.\n",
      "   üìä Criando features de lag e rolling (ETAPA CR√çTICA - SEM TARGET LEAKAGE)...\n",
      "   ‚úÖ Features de lag e rolling criadas SEM TARGET LEAKAGE.\n",
      "   üîÑ Adicionando features EWMA (Fase 2 otimizada - SEM TARGET LEAKAGE)...\n",
      "   üìÖ Adicionando features de calend√°rio avan√ßadas (Fase 2 otimizada)...\n",
      "   üí∏ Adicionando features de pre√ßo relativo (Fase 2 otimizada)...\n",
      "   ‚úÖ Features da Fase 2 criadas e corrigidas contra target leakage.\n",
      "   üìà Criando features de tend√™ncia e hierarquia (otimizadas - SEM TARGET LEAKAGE)...\n",
      "   ‚úÖ Features de tend√™ncia criadas SEM TARGET LEAKAGE.\n",
      "   üìÖ Criando features de calend√°rio b√°sicas (otimizadas)...\n",
      "   üéØ Criando features espec√≠ficas para REGRESS√ÉO (otimizadas)...\n",
      "   üè∑Ô∏è Criando features categ√≥ricas (otimizadas)...\n",
      "   üßπ Aplicando limpeza final e removendo coluna datetime pesada...\n",
      "   ‚úÖ DATASET_COMBINADO - Features criadas com sucesso: (55348430, 55)\n",
      "   üÜï Inclui todas as features da Fase 2: EWMA, calend√°rio avan√ßado, pre√ßo relativo\n",
      "   üõ°Ô∏è Mem√≥ria otimizada a cada etapa - BLINDADO contra crash!\n",
      "   üîí Downcasting seguro aplicado - convers√£o autom√°tica de strings para num√©rico\n",
      "   üìÖ Coluna semana_original preservada para separa√ß√£o treino/valida√ß√£o\n",
      "   üîß CORRE√á√ÉO DE TARGET LEAKAGE APLICADA nas ETAPAS 5, 6 e 7\n",
      "   ‚úÖ Rolling, EWMA e features derivadas baseadas em quantidade_lag_1\n",
      "   ‚úÖ Features temporais criadas.\n",
      "\n",
      "üìä Etapa 2: Separando os datasets de volta para treino e valida√ß√£o...\n",
      "   üóëÔ∏è Liberando mem√≥ria do dataframe combinado...\n",
      "   ‚úÖ Mem√≥ria liberada.\n",
      "üîÑ Otimizando mem√≥ria para train_features...\n",
      "   üìä Mem√≥ria inicial: 9465.33 MB\n",
      "   ‚úÖ Mem√≥ria final: 9130.70 MB (3.5% de redu√ß√£o)\n",
      "üîÑ Otimizando mem√≥ria para validation_features...\n",
      "   üìä Mem√≥ria inicial: 985.97 MB\n",
      "   ‚úÖ Mem√≥ria final: 941.16 MB (4.5% de redu√ß√£o)\n",
      "\n",
      "üíæ Etapa Final: Salvando arquivos otimizados sequencialmente...\n",
      "   üîß Aplicando corre√ß√£o de tipos para compatibilidade com Parquet...\n",
      "   ‚úÖ Dados de TREINO salvos em: ../data/submissao3/train_features.parquet\n",
      "      Shape: (50126880, 54), Colunas: 54\n",
      "   ‚úÖ Dados de VALIDA√á√ÉO salvos em: ../data/submissao3/validation_features.parquet\n",
      "      Shape: (5221550, 54), Colunas: 54\n",
      "\n",
      "üìã Salvando metadados das features...\n",
      "   ‚úÖ Metadados salvos em: data/submissao3/advanced_features_metadata.pkl\n",
      "\n",
      "üéâ ENGENHARIA DE FEATURES AVAN√áADA CONCLU√çDA (COM OTIMIZA√á√ÉO DE MEM√ìRIA)!\n",
      "================================================================================\n",
      "üéØ Features criadas para modelo de dois est√°gios:\n",
      "   üìä CLASSIFICA√á√ÉO: Use target \"vendeu\" para treinar se vai vender\n",
      "   üìä REGRESS√ÉO: Use target \"quantidade\" (apenas onde vendeu=1) para quanto vai vender\n",
      "\n",
      "üí° Pr√≥ximos passos:\n",
      "   1. Treinar modelo de classifica√ß√£o (LGBMClassifier)\n",
      "   2. Treinar modelo de regress√£o (LGBMRegressor)\n",
      "   3. Combinar os dois no pipeline final\n",
      "\n",
      "üöÄ Dados prontos para modelagem avan√ßada!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VERS√ÉO CORRIGIDA - PROCESSAMENTO SEQUENCIAL PARA EVITAR CRASH DE MEM√ìRIA\n",
    "# ==============================================================================\n",
    "\n",
    "# Fun√ß√£o auxiliar para reduzir o uso de mem√≥ria\n",
    "def reduce_mem_usage(df, name):\n",
    "    \"\"\"\n",
    "    Reduz o uso de mem√≥ria de um dataframe, alterando os tipos de dados das colunas.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'üîÑ Otimizando mem√≥ria para {name}...')\n",
    "    print(f'   üìä Mem√≥ria inicial: {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'   ‚úÖ Mem√≥ria final: {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% de redu√ß√£o)')\n",
    "    return df\n",
    "\n",
    "# Fun√ß√£o para limpar tipos antes do salvamento - CORRE√á√ÉO APLICADA AQUI\n",
    "def fix_parquet_types(df):\n",
    "    \"\"\"Fix tipos problem√°ticos antes de salvar em parquet\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    \n",
    "    # Converter todas as colunas object para string de forma robusta\n",
    "    for col in df_fixed.columns:\n",
    "        if df_fixed[col].dtype == 'object':\n",
    "            # Primeiro converter todos valores para string\n",
    "            df_fixed[col] = df_fixed[col].astype(str)\n",
    "            # Limpar valores problem√°ticos\n",
    "            df_fixed[col] = df_fixed[col].replace(['nan', 'None', '<NA>', 'NaN', 'null'], 'Unknown')\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# --- NOVA L√ìGICA DE EXECU√á√ÉO SEQUENCIAL ---\n",
    "\n",
    "import gc\n",
    "\n",
    "# Passo 1: Concatenar dados APENAS para criar features que dependem da s√©rie temporal completa (lags, rolling)\n",
    "print('\\nüîó Etapa 1: Criando features temporais no dataset combinado...')\n",
    "dados_completos = pd.concat([train_data, validation_data], ignore_index=True)\n",
    "\n",
    "# Criar coluna semana_original para separar depois\n",
    "dados_completos['semana_original'] = dados_completos['semana']\n",
    "\n",
    "# CORRE√á√ÉO CR√çTICA: Converter tipos dos dados auxiliares ANTES do merge\n",
    "print('   üîß Convertendo tipos dos dados auxiliares para compatibilidade...')\n",
    "\n",
    "# Converter produto_id nos dados auxiliares para int64 (mesmo tipo do dados_completos)\n",
    "if produtos['produto_id'].dtype == 'object':\n",
    "    produtos['produto_id'] = pd.to_numeric(produtos['produto_id'], errors='coerce').astype('int64')\n",
    "    print('   ‚úÖ produtos[produto_id] convertido para int64')\n",
    "\n",
    "if pdvs['pdv_id'].dtype == 'object':\n",
    "    pdvs['pdv_id'] = pd.to_numeric(pdvs['pdv_id'], errors='coerce').astype('int64')\n",
    "    print('   ‚úÖ pdvs[pdv_id] convertido para int64')\n",
    "\n",
    "# Limpar mem√≥ria\n",
    "del train_data, validation_data\n",
    "gc.collect()\n",
    "\n",
    "# Aplicar a fun√ß√£o de engenharia de features\n",
    "dados_completos_com_features = criar_features_avancadas_otimizada(dados_completos, produtos, pdvs, \"DATASET_COMBINADO\")\n",
    "\n",
    "# Limpar mem√≥ria\n",
    "del dados_completos, produtos, pdvs\n",
    "gc.collect()\n",
    "\n",
    "print('   ‚úÖ Features temporais criadas.')\n",
    "\n",
    "# Passo 2: Separar novamente os conjuntos de treino e valida√ß√£o\n",
    "print('\\nüìä Etapa 2: Separando os datasets de volta para treino e valida√ß√£o...')\n",
    "semanas_validacao = sorted(dados_completos_com_features['semana_original'].unique())[-5:]\n",
    "train_features = dados_completos_com_features[~dados_completos_com_features['semana_original'].isin(semanas_validacao)].copy()\n",
    "validation_features = dados_completos_com_features[dados_completos_com_features['semana_original'].isin(semanas_validacao)].copy()\n",
    "\n",
    "# Remover coluna auxiliar\n",
    "train_features = train_features.drop('semana_original', axis=1)\n",
    "validation_features = validation_features.drop('semana_original', axis=1)\n",
    "\n",
    "# Passo CR√çTICO: Deletar o dataframe gigante e for√ßar a limpeza de mem√≥ria\n",
    "print('   üóëÔ∏è Liberando mem√≥ria do dataframe combinado...')\n",
    "del dados_completos_com_features\n",
    "gc.collect()\n",
    "print('   ‚úÖ Mem√≥ria liberada.')\n",
    "\n",
    "# Passo 3: Otimizar a mem√≥ria de cada dataframe SEPARADAMENTE\n",
    "train_features = reduce_mem_usage(train_features, \"train_features\")\n",
    "validation_features = reduce_mem_usage(validation_features, \"validation_features\")\n",
    "\n",
    "# Passo 4: Salvar os arquivos um de cada vez - COM CORRE√á√ÉO DE TIPOS\n",
    "print('\\nüíæ Etapa Final: Salvando arquivos otimizados sequencialmente...')\n",
    "\n",
    "# CORRE√á√ÉO: Aplicar fix de tipos antes de salvar\n",
    "print('   üîß Aplicando corre√ß√£o de tipos para compatibilidade com Parquet...')\n",
    "train_features = fix_parquet_types(train_features)\n",
    "validation_features = fix_parquet_types(validation_features)\n",
    "\n",
    "# Salvar dados de treino\n",
    "output_path_train = '../data/submissao3/train_features.parquet'\n",
    "train_features.to_parquet(output_path_train, index=False)\n",
    "print(f'   ‚úÖ Dados de TREINO salvos em: {output_path_train}')\n",
    "print(f'      Shape: {train_features.shape}, Colunas: {len(train_features.columns)}')\n",
    "\n",
    "# Liberar mem√≥ria do treino antes de salvar a valida√ß√£o\n",
    "del train_features\n",
    "gc.collect()\n",
    "\n",
    "# Salvar dados de valida√ß√£o\n",
    "output_path_validation = '../data/submissao3/validation_features.parquet'\n",
    "validation_features.to_parquet(output_path_validation, index=False)\n",
    "print(f'   ‚úÖ Dados de VALIDA√á√ÉO salvos em: {output_path_validation}')\n",
    "print(f'      Shape: {validation_features.shape}, Colunas: {len(validation_features.columns)}')\n",
    "\n",
    "# Liberar mem√≥ria da valida√ß√£o\n",
    "del validation_features\n",
    "gc.collect()\n",
    "\n",
    "# Salvar metadados (c√≥digo original mantido)\n",
    "print('\\nüìã Salvando metadados das features...')\n",
    "import pickle\n",
    "\n",
    "metadados_features = {\n",
    "    'data_criacao': pd.Timestamp.now(),\n",
    "    'estrategia': 'Engenharia de Features Avan√ßada para Modelo de Dois Est√°gios',\n",
    "    'total_features': len(pd.read_parquet(output_path_train).columns),\n",
    "    'target_classificacao': 'vendeu (1 se quantidade > 0, 0 caso contr√°rio)',\n",
    "    'target_regressao': 'quantidade (apenas onde vendeu = 1)',\n",
    "    'tipos_features': [\n",
    "        'Target \"vendeu\" para classifica√ß√£o',\n",
    "        'Features de pre√ßo (unit√°rio, lags, m√©dias)',\n",
    "        'Features de calend√°rio (semana, m√™s)',\n",
    "        'Features de lag de vendas (1 a 4 semanas)',\n",
    "        'Features de rolling window (m√©dias, std dev)',\n",
    "        'Features EWMA (m√©dias ponderadas exponencialmente)',\n",
    "        'Features de hierarquia (agregados por categoria/zipcode)',\n",
    "        'Features de tend√™ncia como momentum e acelera√ß√£o',\n",
    "        'Features hash criam embeddings para vari√°veis categ√≥ricas'\n",
    "    ],\n",
    "    'observacoes': [\n",
    "        'Processamento sequencial para evitar esgotamento de mem√≥ria',\n",
    "        'Otimiza√ß√£o de tipos de dados aplicada',\n",
    "        'Features de Fase 2 inclu√≠das: EWMA, calend√°rio avan√ßado, pre√ßo relativo',\n",
    "        'Compatibilidade de tipos garantida para merges',\n",
    "        'Corre√ß√£o de tipos aplicada para salvamento em Parquet'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../data/submissao3/advanced_features_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_features, f)\n",
    "\n",
    "print('   ‚úÖ Metadados salvos em: data/submissao3/advanced_features_metadata.pkl')\n",
    "\n",
    "print('\\nüéâ ENGENHARIA DE FEATURES AVAN√áADA CONCLU√çDA (COM OTIMIZA√á√ÉO DE MEM√ìRIA)!')\n",
    "print('=' * 80)\n",
    "print('üéØ Features criadas para modelo de dois est√°gios:')\n",
    "print('   üìä CLASSIFICA√á√ÉO: Use target \"vendeu\" para treinar se vai vender')\n",
    "print('   üìä REGRESS√ÉO: Use target \"quantidade\" (apenas onde vendeu=1) para quanto vai vender')\n",
    "print('\\nüí° Pr√≥ximos passos:')\n",
    "print('   1. Treinar modelo de classifica√ß√£o (LGBMClassifier)')\n",
    "print('   2. Treinar modelo de regress√£o (LGBMRegressor)')\n",
    "print('   3. Combinar os dois no pipeline final')\n",
    "print('\\nüöÄ Dados prontos para modelagem avan√ßada!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analisando features criadas SEM TARGET LEAKAGE...\n",
      "üìÇ Carregando dados salvos para an√°lise...\n",
      "\n",
      "üìã Features dispon√≠veis (54):\n",
      "\n",
      "üè∑Ô∏è B√°sicas (6):\n",
      "   ‚Ä¢ pdv_id\n",
      "   ‚Ä¢ produto_id\n",
      "   ‚Ä¢ quantidade\n",
      "   ‚Ä¢ faturamento\n",
      "   ‚Ä¢ distributor_id\n",
      "\n",
      "üè∑Ô∏è Target (1):\n",
      "   ‚Ä¢ vendeu\n",
      "\n",
      "üè∑Ô∏è Auxiliares (3):\n",
      "   ‚Ä¢ categoria\n",
      "   ‚Ä¢ zipcode\n",
      "   ‚Ä¢ tipo_loja\n",
      "\n",
      "üè∑Ô∏è Pre√ßo_SEM_Leakage (2):\n",
      "   ‚Ä¢ preco_lag_1\n",
      "   ‚Ä¢ preco_lag_2\n",
      "\n",
      "üè∑Ô∏è Pre√ßo_PARA_Regress√£o (2):\n",
      "   ‚Ä¢ preco_unitario_atual\n",
      "   ‚Ä¢ preco_medio_semanal_sku_atual\n",
      "\n",
      "üè∑Ô∏è Calend√°rio (12):\n",
      "   ‚Ä¢ quantidade_media_4w\n",
      "   ‚Ä¢ semana_do_ano\n",
      "   ‚Ä¢ eh_primeira_semana_mes\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_lag_1\n",
      "   ‚Ä¢ dia_do_mes\n",
      "   ‚Ä¢ semana_do_mes\n",
      "   ‚Ä¢ eh_inicio_mes\n",
      "   ‚Ä¢ eh_fim_mes\n",
      "   ‚Ä¢ mes\n",
      "   ‚Ä¢ mes_sin\n",
      "   ‚Ä¢ mes_cos\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_atual\n",
      "\n",
      "üè∑Ô∏è Lag (6):\n",
      "   ‚Ä¢ quantidade_lag_1\n",
      "   ‚Ä¢ quantidade_lag_2\n",
      "   ‚Ä¢ quantidade_lag_3\n",
      "   ‚Ä¢ quantidade_lag_4\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_lag_1\n",
      "   ‚Ä¢ share_vendas_sku_categoria_lag_1\n",
      "\n",
      "üè∑Ô∏è Rolling (3):\n",
      "   ‚Ä¢ quantidade_media_4w\n",
      "   ‚Ä¢ quantidade_std_4w\n",
      "   ‚Ä¢ quantidade_max_4w\n",
      "\n",
      "üè∑Ô∏è Hierarquia_SEM_Leakage (2):\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_lag_1\n",
      "   ‚Ä¢ share_vendas_sku_categoria_lag_1\n",
      "\n",
      "üè∑Ô∏è Hierarquia_PARA_Regress√£o (2):\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_atual\n",
      "   ‚Ä¢ share_vendas_sku_categoria_atual\n",
      "\n",
      "üè∑Ô∏è Tend√™ncia (3):\n",
      "   ‚Ä¢ momentum_ratio\n",
      "   ‚Ä¢ momentum_ratio_ewma\n",
      "   ‚Ä¢ aceleracao\n",
      "\n",
      "üè∑Ô∏è Hash (6):\n",
      "   ‚Ä¢ pdv_hash\n",
      "   ‚Ä¢ produto_hash\n",
      "   ‚Ä¢ categoria_hash\n",
      "   ‚Ä¢ zipcode_hash\n",
      "   ‚Ä¢ pdv_produto_hash\n",
      "   ‚Ä¢ categoria_zipcode_hash\n",
      "\n",
      "üö® AN√ÅLISE DE TARGET LEAKAGE:\n",
      "‚úÖ Features SEGURAS para classifica√ß√£o: 44\n",
      "   ‚Ä¢ categoria\n",
      "   ‚Ä¢ zipcode\n",
      "   ‚Ä¢ tipo_loja\n",
      "   ‚Ä¢ preco_lag_1\n",
      "   ‚Ä¢ preco_lag_2\n",
      "   ‚Ä¢ variacao_preco_sku_semanal\n",
      "   ‚Ä¢ quantidade_lag_1\n",
      "   ‚Ä¢ quantidade_lag_2\n",
      "   ‚Ä¢ quantidade_lag_3\n",
      "   ‚Ä¢ quantidade_lag_4\n",
      "   ‚Ä¢ ... e mais 34 features\n",
      "\n",
      "‚ö†Ô∏è Features COM LEAKAGE (apenas para regress√£o): 4\n",
      "   ‚Ä¢ preco_unitario_atual\n",
      "   ‚Ä¢ preco_medio_semanal_sku_atual\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_atual\n",
      "   ‚Ä¢ share_vendas_sku_categoria_atual\n",
      "\n",
      "üìä Estat√≠sticas do conjunto de treino (SEM LEAKAGE):\n",
      "   ‚Ä¢ Shape: (50126880, 54)\n",
      "   ‚Ä¢ Target vendeu: 5,543,608 positivos (11.1%)\n",
      "   ‚Ä¢ Quantidade m√©dia: 1.00\n",
      "   ‚Ä¢ Registros com lag_4: 5,077,534\n",
      "\n",
      "üìä Estat√≠sticas do conjunto de valida√ß√£o (SEM LEAKAGE):\n",
      "   ‚Ä¢ Shape: (5221550, 54)\n",
      "   ‚Ä¢ Target vendeu: 571,729 positivos (10.9%)\n",
      "   ‚Ä¢ Quantidade m√©dia: 0.57\n",
      "   ‚Ä¢ Registros com lag_4: 587,975\n",
      "‚úÖ An√°lise das features SEM TARGET LEAKAGE conclu√≠da\n"
     ]
    }
   ],
   "source": [
    "# Analisar features criadas SEM TARGET LEAKAGE\n",
    "print('üìä Analisando features criadas SEM TARGET LEAKAGE...')\n",
    "\n",
    "# Carregar os dados salvos para an√°lise (j√° que foram deletados para economizar mem√≥ria)\n",
    "print('üìÇ Carregando dados salvos para an√°lise...')\n",
    "train_features = pd.read_parquet('../data/submissao3/train_features.parquet')\n",
    "validation_features = pd.read_parquet('../data/submissao3/validation_features.parquet')\n",
    "\n",
    "# Listar todas as features\n",
    "features_disponiveis = list(train_features.columns)\n",
    "print(f'\\nüìã Features dispon√≠veis ({len(features_disponiveis)}):')\n",
    "\n",
    "# Categorizar features CORRIGIDAS\n",
    "features_categorias = {\n",
    "    'B√°sicas': ['semana', 'pdv_id', 'produto_id', 'quantidade', 'faturamento', 'distributor_id'],\n",
    "    'Target': ['vendeu'],\n",
    "    'Auxiliares': ['categoria', 'zipcode', 'tipo_loja'],\n",
    "    'Pre√ßo_SEM_Leakage': [col for col in features_disponiveis if 'preco' in col and 'lag' in col],\n",
    "    'Pre√ßo_PARA_Regress√£o': [col for col in features_disponiveis if 'preco' in col and 'atual' in col],\n",
    "    'Calend√°rio': [col for col in features_disponiveis if any(x in col for x in ['dia', 'semana_do', 'mes', 'inicio', 'fim'])],\n",
    "    'Lag': [col for col in features_disponiveis if 'lag_' in col and 'preco' not in col],\n",
    "    'Rolling': [col for col in features_disponiveis if any(x in col for x in ['media_4w', 'std_4w', 'max_4w'])],\n",
    "    'Hierarquia_SEM_Leakage': [col for col in features_disponiveis if any(x in col for x in ['media_vendas', 'share']) and 'lag' in col],\n",
    "    'Hierarquia_PARA_Regress√£o': [col for col in features_disponiveis if any(x in col for x in ['media_vendas', 'share']) and 'atual' in col],\n",
    "    'Tend√™ncia': [col for col in features_disponiveis if any(x in col for x in ['momentum', 'aceleracao'])],\n",
    "    'Hash': [col for col in features_disponiveis if 'hash' in col]\n",
    "}\n",
    "\n",
    "for categoria, features in features_categorias.items():\n",
    "    if features:\n",
    "        print(f'\\nüè∑Ô∏è {categoria} ({len(features)}):')\n",
    "        for feat in features:\n",
    "            if feat in features_disponiveis:\n",
    "                print(f'   ‚Ä¢ {feat}')\n",
    "\n",
    "print('\\nüö® AN√ÅLISE DE TARGET LEAKAGE:')\n",
    "features_sem_leakage = []\n",
    "features_com_leakage = []\n",
    "\n",
    "for feat in features_disponiveis:\n",
    "    if feat in ['semana', 'pdv_id', 'produto_id', 'quantidade', 'faturamento', 'distributor_id', 'vendeu']:\n",
    "        continue  # B√°sicas, n√£o contar\n",
    "    elif 'atual' in feat:\n",
    "        features_com_leakage.append(feat)  # Para regress√£o apenas\n",
    "    else:\n",
    "        features_sem_leakage.append(feat)  # Seguras para classifica√ß√£o\n",
    "\n",
    "print(f'‚úÖ Features SEGURAS para classifica√ß√£o: {len(features_sem_leakage)}')\n",
    "for feat in features_sem_leakage[:10]:  # Mostrar apenas primeiras 10\n",
    "    print(f'   ‚Ä¢ {feat}')\n",
    "if len(features_sem_leakage) > 10:\n",
    "    print(f'   ‚Ä¢ ... e mais {len(features_sem_leakage)-10} features')\n",
    "\n",
    "print(f'\\n‚ö†Ô∏è Features COM LEAKAGE (apenas para regress√£o): {len(features_com_leakage)}')\n",
    "for feat in features_com_leakage:\n",
    "    print(f'   ‚Ä¢ {feat}')\n",
    "\n",
    "print('\\nüìä Estat√≠sticas do conjunto de treino (SEM LEAKAGE):')\n",
    "print(f'   ‚Ä¢ Shape: {train_features.shape}')\n",
    "print(f'   ‚Ä¢ Target vendeu: {train_features[\"vendeu\"].sum():,} positivos ({train_features[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ Quantidade m√©dia: {train_features[\"quantidade\"].mean():.2f}')\n",
    "print(f'   ‚Ä¢ Registros com lag_4: {(train_features[\"quantidade_lag_4\"] > 0).sum():,}')\n",
    "\n",
    "print('\\nüìä Estat√≠sticas do conjunto de valida√ß√£o (SEM LEAKAGE):')\n",
    "print(f'   ‚Ä¢ Shape: {validation_features.shape}')\n",
    "print(f'   ‚Ä¢ Target vendeu: {validation_features[\"vendeu\"].sum():,} positivos ({validation_features[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ Quantidade m√©dia: {validation_features[\"quantidade\"].mean():.2f}')\n",
    "print(f'   ‚Ä¢ Registros com lag_4: {(validation_features[\"quantidade_lag_4\"] > 0).sum():,}')\n",
    "\n",
    "print('‚úÖ An√°lise das features SEM TARGET LEAKAGE conclu√≠da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Regenerando features com CORRE√á√ÉO de target leakage...\n",
      "üìÇ Recarregando dados originais...\n",
      "üîó Combinando dados para criar features temporais...\n",
      "‚ö° Aplicando engenharia de features CORRIGIDA...\n",
      "\\nüîß Criando features avan√ßadas para DATASET_CORRIGIDO (VERS√ÉO CORRIGIDA - SEM TARGET LEAKAGE)...\n",
      "   üéØ Otimiza√ß√£o inicial de tipos de dados (verificando limites)...\n",
      "      üîç pdv_id: tipo=object\n",
      "         Exemplos: ['1008240744247283174', '1008240744247283174', '10097752152132198']\n",
      "   üîÑ Coluna pdv_id: convertida de string para num√©rico\n",
      "   ‚ö†Ô∏è Coluna pdv_id: valores muito grandes para int32, mantendo int64\n",
      "      üîç produto_id: tipo=object\n",
      "         Exemplos: ['1938760505411922162', '4098058333001424920', '1938760505411922162']\n",
      "   üîÑ Coluna produto_id: convertida de string para num√©rico\n",
      "   ‚ö†Ô∏è Coluna produto_id: valores muito grandes para int32, mantendo int64\n",
      "      üîç distributor_id: tipo=float32\n",
      "         Range: 4.0 at√© 11.0\n",
      "   ‚úÖ Otimiza√ß√£o inicial conclu√≠da.\n",
      "   üéØ Criando target e fazendo merge com dados auxiliares...\n",
      "   ‚úÖ Target e merges conclu√≠dos.\n",
      "   üîÑ Ordenando dados para consist√™ncia temporal...\n",
      "   üí∞ Criando features de pre√ßo (COM CORRE√á√ÉO DE LEAKAGE)...\n",
      "      propagando pre√ßos para semanas sem vendas...\n",
      "   ‚úÖ Features de pre√ßo criadas e corrigidas contra leakage.\n",
      "   üìä Criando features de lag e rolling (ETAPA CR√çTICA - SEM TARGET LEAKAGE)...\n",
      "   ‚úÖ Features de lag e rolling criadas SEM TARGET LEAKAGE.\n",
      "   üîÑ Adicionando features EWMA (Fase 2 otimizada - SEM TARGET LEAKAGE)...\n",
      "   üìÖ Adicionando features de calend√°rio avan√ßadas (Fase 2 otimizada)...\n",
      "   üí∏ Adicionando features de pre√ßo relativo (Fase 2 otimizada)...\n",
      "   ‚úÖ Features da Fase 2 criadas e corrigidas contra target leakage.\n",
      "   üìà Criando features de tend√™ncia e hierarquia (otimizadas - SEM TARGET LEAKAGE)...\n",
      "   ‚úÖ Features de tend√™ncia criadas SEM TARGET LEAKAGE.\n",
      "   üìÖ Criando features de calend√°rio b√°sicas (otimizadas)...\n",
      "   üéØ Criando features espec√≠ficas para REGRESS√ÉO (otimizadas)...\n",
      "   üè∑Ô∏è Criando features categ√≥ricas (otimizadas)...\n",
      "   üßπ Aplicando limpeza final e removendo coluna datetime pesada...\n",
      "   ‚úÖ DATASET_CORRIGIDO - Features criadas com sucesso: (55348430, 55)\n",
      "   üÜï Inclui todas as features da Fase 2: EWMA, calend√°rio avan√ßado, pre√ßo relativo\n",
      "   üõ°Ô∏è Mem√≥ria otimizada a cada etapa - BLINDADO contra crash!\n",
      "   üîí Downcasting seguro aplicado - convers√£o autom√°tica de strings para num√©rico\n",
      "   üìÖ Coluna semana_original preservada para separa√ß√£o treino/valida√ß√£o\n",
      "   üîß CORRE√á√ÉO DE TARGET LEAKAGE APLICADA nas ETAPAS 5, 6 e 7\n",
      "   ‚úÖ Rolling, EWMA e features derivadas baseadas em quantidade_lag_1\n",
      "üìä Separando datasets corrigidos...\n",
      "üîÑ Otimizando mem√≥ria para train_features_corrigido...\n",
      "   üìä Mem√≥ria inicial: 9465.33 MB\n",
      "   ‚úÖ Mem√≥ria final: 9130.70 MB (3.5% de redu√ß√£o)\n",
      "üîÑ Otimizando mem√≥ria para validation_features_corrigido...\n",
      "   üìä Mem√≥ria inicial: 985.97 MB\n",
      "   ‚úÖ Mem√≥ria final: 941.16 MB (4.5% de redu√ß√£o)\n",
      "üíæ Salvando dados CORRIGIDOS...\n",
      "‚úÖ Arquivos CORRIGIDOS salvos:\n",
      "   ‚Ä¢ data/submissao3/train_features_CORRIGIDO.parquet\n",
      "   ‚Ä¢ data/submissao3/validation_features_CORRIGIDO.parquet\n",
      "\\nüìä AN√ÅLISE COMPARATIVA - ANTES vs DEPOIS DA CORRE√á√ÉO:\n",
      "üìê Shape treino: (50126880, 54)\n",
      "üìê Shape valida√ß√£o: (5221550, 54)\n",
      "\\nüîç An√°lise de features cr√≠ticas (primeiros 10 registros):\n",
      "   quantidade_media_4w: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "   quantidade_std_4w: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "   quantidade_ewma_4w: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "   momentum_ratio: [0.0, 0.0, 0.0, 0.0, 0.0] ...\n",
      "\\nüéØ Estat√≠sticas do target:\n",
      "   Treino vendeu=1: 5,543,608 (11.1%)\n",
      "   Valida√ß√£o vendeu=1: 571,729 (10.9%)\n",
      "üìã Metadados da corre√ß√£o salvos!\n",
      "\\nüéâ CORRE√á√ÉO DE TARGET LEAKAGE CONCLU√çDA!\n",
      "======================================================================\n",
      "‚úÖ Pr√≥ximo passo: Re-treinar o modelo de classifica√ß√£o com dados corrigidos\n",
      "üìà Espere m√©tricas mais realistas (AUC ~0.85-0.95, F1 ~0.6-0.8)\n",
      "üßπ Mem√≥ria limpa. Pronto para pr√≥ximos passos!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# REGENERAR FEATURES CORRIGIDAS - SEM TARGET LEAKAGE\n",
    "# ============================================================================\n",
    "\n",
    "print('üîß Regenerando features com CORRE√á√ÉO de target leakage...')\n",
    "\n",
    "# Limpar dados anteriores\n",
    "import gc\n",
    "if 'train_features' in locals():\n",
    "    del train_features\n",
    "if 'validation_features' in locals():\n",
    "    del validation_features\n",
    "gc.collect()\n",
    "\n",
    "# Carregar dados originais novamente\n",
    "print('üìÇ Recarregando dados originais...')\n",
    "train_data = pd.read_parquet('../data/submissao3/train_data.parquet')\n",
    "validation_data = pd.read_parquet('../data/submissao3/validation_data.parquet')\n",
    "\n",
    "# Carregar dados auxiliares\n",
    "produtos = pd.read_parquet(\n",
    "    '../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet'\n",
    ")\n",
    "produtos = produtos.rename(columns={'produto': 'produto_id'})\n",
    "\n",
    "pdvs = pd.read_parquet(\n",
    "    '../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
    ")\n",
    "pdvs = pdvs.rename(columns={'pdv': 'pdv_id', 'categoria_pdv': 'tipo_loja'})\n",
    "\n",
    "# Converter tipos dos dados auxiliares\n",
    "if produtos['produto_id'].dtype == 'object':\n",
    "    produtos['produto_id'] = pd.to_numeric(produtos['produto_id'], errors='coerce').astype('int64')\n",
    "if pdvs['pdv_id'].dtype == 'object':\n",
    "    pdvs['pdv_id'] = pd.to_numeric(pdvs['pdv_id'], errors='coerce').astype('int64')\n",
    "\n",
    "# Combinar dados para criar features temporais consistentes\n",
    "print('üîó Combinando dados para criar features temporais...')\n",
    "dados_completos = pd.concat([train_data, validation_data], ignore_index=True)\n",
    "dados_completos['semana_original'] = dados_completos['semana']\n",
    "\n",
    "del train_data, validation_data\n",
    "gc.collect()\n",
    "\n",
    "# Aplicar a fun√ß√£o CORRIGIDA\n",
    "print('‚ö° Aplicando engenharia de features CORRIGIDA...')\n",
    "dados_completos_corrigidos = criar_features_avancadas_otimizada(\n",
    "    dados_completos, produtos, pdvs, \"DATASET_CORRIGIDO\"\n",
    ")\n",
    "\n",
    "del dados_completos, produtos, pdvs\n",
    "gc.collect()\n",
    "\n",
    "# Separar novamente\n",
    "print('üìä Separando datasets corrigidos...')\n",
    "semanas_validacao = sorted(dados_completos_corrigidos['semana_original'].unique())[-5:]\n",
    "train_features_corrigido = dados_completos_corrigidos[\n",
    "    ~dados_completos_corrigidos['semana_original'].isin(semanas_validacao)\n",
    "].copy()\n",
    "validation_features_corrigido = dados_completos_corrigidos[\n",
    "    dados_completos_corrigidos['semana_original'].isin(semanas_validacao)\n",
    "].copy()\n",
    "\n",
    "# Remover coluna auxiliar\n",
    "train_features_corrigido = train_features_corrigido.drop('semana_original', axis=1)\n",
    "validation_features_corrigido = validation_features_corrigido.drop('semana_original', axis=1)\n",
    "\n",
    "del dados_completos_corrigidos\n",
    "gc.collect()\n",
    "\n",
    "# Otimizar mem√≥ria\n",
    "train_features_corrigido = reduce_mem_usage(train_features_corrigido, \"train_features_corrigido\")\n",
    "validation_features_corrigido = reduce_mem_usage(validation_features_corrigido, \"validation_features_corrigido\")\n",
    "\n",
    "# Salvar dados corrigidos\n",
    "print('üíæ Salvando dados CORRIGIDOS...')\n",
    "\n",
    "def fix_parquet_types(df):\n",
    "    \"\"\"Fix tipos problem√°ticos antes de salvar em parquet\"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    for col in df_fixed.columns:\n",
    "        if df_fixed[col].dtype == 'object':\n",
    "            df_fixed[col] = df_fixed[col].astype(str)\n",
    "            df_fixed[col] = df_fixed[col].replace(['nan', 'None', '<NA>', 'NaN', 'null'], 'Unknown')\n",
    "    return df_fixed\n",
    "\n",
    "train_features_corrigido = fix_parquet_types(train_features_corrigido)\n",
    "validation_features_corrigido = fix_parquet_types(validation_features_corrigido)\n",
    "\n",
    "# Salvar arquivos corrigidos\n",
    "train_features_corrigido.to_parquet('../data/submissao3/train_features_CORRIGIDO.parquet', index=False)\n",
    "validation_features_corrigido.to_parquet('../data/submissao3/validation_features_CORRIGIDO.parquet', index=False)\n",
    "\n",
    "print('‚úÖ Arquivos CORRIGIDOS salvos:')\n",
    "print('   ‚Ä¢ data/submissao3/train_features_CORRIGIDO.parquet')\n",
    "print('   ‚Ä¢ data/submissao3/validation_features_CORRIGIDO.parquet')\n",
    "\n",
    "# An√°lise r√°pida das diferen√ßas\n",
    "print('\\\\nüìä AN√ÅLISE COMPARATIVA - ANTES vs DEPOIS DA CORRE√á√ÉO:')\n",
    "print(f'üìê Shape treino: {train_features_corrigido.shape}')\n",
    "print(f'üìê Shape valida√ß√£o: {validation_features_corrigido.shape}')\n",
    "\n",
    "# Testar algumas features cr√≠ticas\n",
    "exemplos_features = ['quantidade_media_4w', 'quantidade_std_4w', 'quantidade_ewma_4w', 'momentum_ratio']\n",
    "print('\\\\nüîç An√°lise de features cr√≠ticas (primeiros 10 registros):')\n",
    "\n",
    "for feat in exemplos_features:\n",
    "    if feat in train_features_corrigido.columns:\n",
    "        valores = train_features_corrigido[feat].head(10)\n",
    "        print(f'   {feat}: {valores.tolist()[:5]} ...')\n",
    "\n",
    "print('\\\\nüéØ Estat√≠sticas do target:')\n",
    "print(f'   Treino vendeu=1: {train_features_corrigido[\"vendeu\"].sum():,} ({train_features_corrigido[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   Valida√ß√£o vendeu=1: {validation_features_corrigido[\"vendeu\"].sum():,} ({validation_features_corrigido[\"vendeu\"].mean()*100:.1f}%)')\n",
    "\n",
    "# Salvar metadados da corre√ß√£o\n",
    "import pickle\n",
    "metadados_correcao = {\n",
    "    'data_correcao': pd.Timestamp.now(),\n",
    "    'versao': 'CORRIGIDA - SEM TARGET LEAKAGE',\n",
    "    'principais_correcoes': [\n",
    "        'Rolling features baseadas em quantidade_lag_1 (n√£o quantidade atual)',\n",
    "        'EWMA features baseadas em quantidade_lag_1 (n√£o quantidade atual)', \n",
    "        'Features derivadas (momentum, volatilidade) usam features corrigidas',\n",
    "        'Separa√ß√£o clara entre features para classifica√ß√£o vs regress√£o'\n",
    "    ],\n",
    "    'features_seguras_classificacao': [f for f in train_features_corrigido.columns \n",
    "                                       if not any(x in f for x in ['atual', 'quantidade']) or 'lag' in f],\n",
    "    'total_features': len(train_features_corrigido.columns)\n",
    "}\n",
    "\n",
    "with open('../data/submissao3/correcao_target_leakage_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_correcao, f)\n",
    "\n",
    "print('üìã Metadados da corre√ß√£o salvos!')\n",
    "print('\\\\nüéâ CORRE√á√ÉO DE TARGET LEAKAGE CONCLU√çDA!')\n",
    "print('=' * 70)\n",
    "print('‚úÖ Pr√≥ximo passo: Re-treinar o modelo de classifica√ß√£o com dados corrigidos')\n",
    "print('üìà Espere m√©tricas mais realistas (AUC ~0.85-0.95, F1 ~0.6-0.8)')\n",
    "\n",
    "# Limpar vari√°veis para economizar mem√≥ria\n",
    "del train_features_corrigido, validation_features_corrigido\n",
    "gc.collect()\n",
    "\n",
    "print('üßπ Mem√≥ria limpa. Pronto para pr√≥ximos passos!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
