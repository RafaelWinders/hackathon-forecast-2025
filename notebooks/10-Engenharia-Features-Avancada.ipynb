{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Engenharia de Features Avan√ßada\n",
    "\n",
    "Este notebook cria features avan√ßadas baseadas nos dados de treino e valida√ß√£o temporal.\n",
    "\n",
    "## Novas Features:\n",
    "- **Target de Classifica√ß√£o**: `vendeu` (1 se quantidade > 0, 0 caso contr√°rio)\n",
    "- **Features de Pre√ßo**: pre√ßo unit√°rio, varia√ß√µes, m√©dias\n",
    "- **Features de Calend√°rio**: dia do m√™s, semana do m√™s, flags temporais\n",
    "- **Features de Hierarquia**: agrega√ß√µes por categoria, zipcode\n",
    "- **Features de Tend√™ncia**: momentum, acelera√ß√£o de vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando Engenharia de Features Avan√ßada\n",
      "üéØ Objetivo: Criar vari√°veis preditivas poderosas para o modelo de dois est√°gios\n",
      "üìÅ Pasta data/submissao3 criada/verificada\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('üöÄ Iniciando Engenharia de Features Avan√ßada')\n",
    "print('üéØ Objetivo: Criar vari√°veis preditivas poderosas para o modelo de dois est√°gios')\n",
    "\n",
    "# Criar pasta submissao3 se n√£o existir\n",
    "os.makedirs('../data/submissao3', exist_ok=True)\n",
    "print('üìÅ Pasta data/submissao3 criada/verificada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados de Treino e Valida√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados de treino e valida√ß√£o...\n",
      "üèãÔ∏è Dados de treino: (50126880, 6)\n",
      "üîç Dados de valida√ß√£o: (5221550, 6)\n",
      "üìÖ Treino: 2021-12-28 00:00:00 at√© 2022-11-22 00:00:00\n",
      "üìÖ Valida√ß√£o: 2022-11-29 00:00:00 at√© 2022-12-27 00:00:00\n",
      "‚úÖ Dados carregados com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados de treino e valida√ß√£o\n",
    "print('üìÇ Carregando dados de treino e valida√ß√£o...')\n",
    "\n",
    "train_data = pd.read_parquet('../data/submissao3/train_data.parquet')\n",
    "validation_data = pd.read_parquet('../data/submissao3/validation_data.parquet')\n",
    "\n",
    "print(f'üèãÔ∏è Dados de treino: {train_data.shape}')\n",
    "print(f'üîç Dados de valida√ß√£o: {validation_data.shape}')\n",
    "\n",
    "# Verificar per√≠odo\n",
    "print(f'üìÖ Treino: {train_data[\"semana\"].min()} at√© {train_data[\"semana\"].max()}')\n",
    "print(f'üìÖ Valida√ß√£o: {validation_data[\"semana\"].min()} at√© {validation_data[\"semana\"].max()}')\n",
    "\n",
    "print('‚úÖ Dados carregados com sucesso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados auxiliares...\n",
      "üìä Produtos: (7092, 8) - Categorias: 7\n",
      "üìä PDVs: (14419, 4) - Zipcodes: 788\n",
      "‚úÖ Dados auxiliares carregados\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados de produtos e PDVs para features adicionais\n",
    "print('üìÇ Carregando dados auxiliares...')\n",
    "\n",
    "# Produtos\n",
    "produtos = pd.read_parquet(\n",
    "    '../data/part-00000-tid-7173294866425216458-eae53fbf-d19e-4130-ba74-78f96b9675f1-4-1-c000.snappy.parquet'\n",
    ")\n",
    "produtos = produtos.rename(columns={\n",
    "    'produto': 'produto_id'\n",
    "})\n",
    "\n",
    "# PDVs\n",
    "pdvs = pd.read_parquet(\n",
    "    '../data/part-00000-tid-2779033056155408584-f6316110-4c9a-4061-ae48-69b77c7c8c36-4-1-c000.snappy.parquet'\n",
    ")\n",
    "pdvs = pdvs.rename(columns={\n",
    "    'pdv': 'pdv_id',\n",
    "    'categoria_pdv': 'tipo_loja'\n",
    "})\n",
    "\n",
    "print(f'üìä Produtos: {produtos.shape} - Categorias: {produtos[\"categoria\"].nunique()}')\n",
    "print(f'üìä PDVs: {pdvs.shape} - Zipcodes: {pdvs[\"zipcode\"].nunique()}')\n",
    "\n",
    "print('‚úÖ Dados auxiliares carregados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fun√ß√£o de Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√£o de engenharia de features OTIMIZADA definida\n"
     ]
    }
   ],
   "source": [
    "def criar_features_avancadas_otimizada(df, produtos_df, pdvs_df, nome_conjunto):\n",
    "    \"\"\"\n",
    "    Criar features avan√ßadas de forma OTIMIZADA e SEM TARGET LEAKAGE.\n",
    "    Esta vers√£o substitui o groupby().apply() por opera√ß√µes vetorizadas.\n",
    "    \"\"\"\n",
    "    print(f'\\nüîß Criando features avan√ßadas para {nome_conjunto} (VERS√ÉO OTIMIZADA)...')\n",
    "    \n",
    "    dados = df.copy()\n",
    "    \n",
    "    # ETAPAS INICIAIS (J√Å CORRETAS)\n",
    "    print('   üéØ Criando target de classifica√ß√£o...')\n",
    "    dados['vendeu'] = (dados['quantidade'] > 0).astype(int)\n",
    "    \n",
    "    print('   üîó Fazendo merge com dados auxiliares...')\n",
    "    dados = dados.merge(produtos_df[['produto_id', 'categoria']], on='produto_id', how='left')\n",
    "    dados = dados.merge(pdvs_df[['pdv_id', 'zipcode', 'tipo_loja']], on='pdv_id', how='left')\n",
    "    \n",
    "    print('   üîÑ Ordenando dados para consist√™ncia temporal...')\n",
    "    dados.sort_values(['pdv_id', 'produto_id', 'semana'], inplace=True)\n",
    "    dados.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gb_cols = ['pdv_id', 'produto_id']\n",
    "    \n",
    "    # ============================================================================\n",
    "    # REFAZENDO AS FEATURES DE FORMA OTIMIZADA\n",
    "    # ============================================================================\n",
    "    \n",
    "    # 1. FEATURES DE PRE√áO E LAGS (SEM LEAKAGE)\n",
    "    print('   üí∞ Criando features de pre√ßo e lags (otimizado)...')\n",
    "    preco_unitario_temp = np.where(dados['quantidade'] > 0, dados['faturamento'] / dados['quantidade'], 0)\n",
    "    \n",
    "    gb_preco = dados.assign(preco_unitario_temp=preco_unitario_temp).groupby(gb_cols)['preco_unitario_temp']\n",
    "    dados['preco_lag_1'] = gb_preco.shift(1)\n",
    "    dados['preco_lag_2'] = gb_preco.shift(2)\n",
    "    dados['variacao_preco_sku_semanal'] = (dados['preco_lag_1'] - dados['preco_lag_2']).fillna(0)\n",
    "    \n",
    "    # 2. FEATURES DE LAG E ROLLING (OTIMIZA√á√ÉO CR√çTICA)\n",
    "    print('   üìä Criando features de lag e rolling (OTIMIZA√á√ÉO CR√çTICA)...')\n",
    "    \n",
    "    # Agrupar uma √∫nica vez para performance\n",
    "    gb_quantidade = dados.groupby(gb_cols)['quantidade']\n",
    "    \n",
    "    # Lags de quantidade\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        dados[f'quantidade_lag_{lag}'] = gb_quantidade.shift(lag)\n",
    "        \n",
    "    # --- A MUDAN√áA MAIS IMPORTANTE EST√Å AQUI ---\n",
    "    # Substituindo o groupby().apply() lento por groupby().rolling() otimizado\n",
    "    rolling_window = gb_quantidade.rolling(window=4, min_periods=1)\n",
    "    \n",
    "    dados['quantidade_media_4w'] = rolling_window.mean().reset_index(level=gb_cols, drop=True)\n",
    "    dados['quantidade_std_4w'] = rolling_window.std().reset_index(level=gb_cols, drop=True)\n",
    "    dados['quantidade_max_4w'] = rolling_window.max().reset_index(level=gb_cols, drop=True)\n",
    "    # --- FIM DA MUDAN√áA CR√çTICA ---\n",
    "\n",
    "    # 3. DEMAIS FEATURES (J√Å ERAM RELATIVAMENTE EFICIENTES, MAS PODEM USAR OS RESULTADOS OTIMIZADOS)\n",
    "    print('   üìà Criando features de tend√™ncia e hierarquia...')\n",
    "    \n",
    "    # Hierarquia (usando lags j√° calculados)\n",
    "    dados['media_vendas_categoria_pdv_lag_1'] = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('mean')\n",
    "    vendas_categoria_pdv_lag = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade_lag_1'].transform('sum')\n",
    "    dados['share_vendas_sku_categoria_lag_1'] = (dados['quantidade_lag_1'] / vendas_categoria_pdv_lag).fillna(0)\n",
    "\n",
    "    # Tend√™ncia (usando lags j√° calculados)\n",
    "    dados['momentum_ratio'] = (dados['quantidade_lag_1'] / dados['quantidade_media_4w']).fillna(0)\n",
    "    dados['aceleracao'] = (dados['quantidade_lag_1'] - dados['quantidade_lag_2']).fillna(0)\n",
    "    \n",
    "    # 4. FEATURES DE CALEND√ÅRIO (SEMPRE SEGURAS)\n",
    "    print('   üìÖ Criando features de calend√°rio...')\n",
    "    dados['dia_do_mes'] = dados['semana'].dt.day\n",
    "    dados['semana_do_mes'] = (dados['semana'].dt.day - 1) // 7 + 1\n",
    "    dados['eh_inicio_mes'] = (dados['semana'].dt.day <= 7).astype(int)\n",
    "    dados['eh_fim_mes'] = (dados['semana'].dt.day >= 22).astype(int)\n",
    "    dados['mes'] = dados['semana'].dt.month\n",
    "    \n",
    "    # Features c√≠clicas para m√™s\n",
    "    dados['mes_sin'] = np.sin(2 * np.pi * dados['mes'] / 12)\n",
    "    dados['mes_cos'] = np.cos(2 * np.pi * dados['mes'] / 12)\n",
    "\n",
    "    # 5. FEATURES PARA REGRESS√ÉO (usando valores atuais)\n",
    "    print('   üéØ Criando features espec√≠ficas para REGRESS√ÉO...')\n",
    "    dados['preco_unitario_atual'] = preco_unitario_temp \n",
    "    dados['preco_medio_semanal_sku_atual'] = dados.groupby(['semana', 'produto_id'])['preco_unitario_atual'].transform('mean')\n",
    "    dados['media_vendas_categoria_pdv_atual'] = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('mean')\n",
    "    vendas_categoria_pdv_atual = dados.groupby(['semana', 'categoria', 'pdv_id'])['quantidade'].transform('sum')\n",
    "    dados['share_vendas_sku_categoria_atual'] = (dados['quantidade'] / vendas_categoria_pdv_atual).fillna(0)\n",
    "    \n",
    "    # 6. FEATURES CATEG√ìRICAS HASH\n",
    "    print('   üè∑Ô∏è Criando features categ√≥ricas...')\n",
    "    dados['pdv_hash'] = dados['pdv_id'].astype(str).apply(hash).abs() % 100\n",
    "    dados['produto_hash'] = dados['produto_id'].astype(str).apply(hash).abs() % 100\n",
    "    dados['categoria_hash'] = dados['categoria'].astype(str).apply(hash).abs() % 50\n",
    "    dados['zipcode_hash'] = dados['zipcode'].astype(str).apply(hash).abs() % 1000\n",
    "    \n",
    "    # Features de intera√ß√£o\n",
    "    dados['pdv_produto_hash'] = dados['pdv_hash'] * 100 + dados['produto_hash']\n",
    "    dados['categoria_zipcode_hash'] = dados['categoria_hash'] * 1000 + dados['zipcode_hash']\n",
    "    \n",
    "    # 7. LIMPEZA FINAL\n",
    "    print('   üßπ Aplicando limpeza final...')\n",
    "    dados.fillna(0, inplace=True)\n",
    "    \n",
    "    print(f'   ‚úÖ {nome_conjunto} - Features criadas com sucesso: {dados.shape}')\n",
    "    return dados\n",
    "\n",
    "print('‚úÖ Fun√ß√£o de engenharia de features OTIMIZADA definida')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aplica√ß√£o da Engenharia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Aplicando engenharia de features OTIMIZADA...\n",
      "\n",
      "üîß Criando features avan√ßadas para TREINO (VERS√ÉO OTIMIZADA)...\n",
      "   üéØ Criando target de classifica√ß√£o...\n",
      "   üîó Fazendo merge com dados auxiliares...\n",
      "   üîÑ Ordenando dados para consist√™ncia temporal...\n",
      "   üí∞ Criando features de pre√ßo e lags (otimizado)...\n",
      "   üìä Criando features de lag e rolling (OTIMIZA√á√ÉO CR√çTICA)...\n",
      "   üìà Criando features de tend√™ncia e hierarquia...\n",
      "   üìÖ Criando features de calend√°rio...\n",
      "   üéØ Criando features espec√≠ficas para REGRESS√ÉO...\n",
      "   üè∑Ô∏è Criando features categ√≥ricas...\n",
      "   üßπ Aplicando limpeza final...\n",
      "   ‚úÖ TREINO - Features criadas com sucesso: (50126880, 41)\n",
      "\n",
      "üîß Criando features avan√ßadas para VALIDA√á√ÉO (VERS√ÉO OTIMIZADA)...\n",
      "   üéØ Criando target de classifica√ß√£o...\n",
      "   üîó Fazendo merge com dados auxiliares...\n",
      "   üîÑ Ordenando dados para consist√™ncia temporal...\n",
      "   üí∞ Criando features de pre√ßo e lags (otimizado)...\n",
      "   üìä Criando features de lag e rolling (OTIMIZA√á√ÉO CR√çTICA)...\n",
      "   üìà Criando features de tend√™ncia e hierarquia...\n",
      "   üìÖ Criando features de calend√°rio...\n",
      "   üéØ Criando features espec√≠ficas para REGRESS√ÉO...\n",
      "   üè∑Ô∏è Criando features categ√≥ricas...\n",
      "   üßπ Aplicando limpeza final...\n",
      "   ‚úÖ VALIDA√á√ÉO - Features criadas com sucesso: (5221550, 41)\n",
      "\n",
      "‚úÖ Engenharia de features OTIMIZADA aplicada a ambos os conjuntos!\n"
     ]
    }
   ],
   "source": [
    "# Aplicar engenharia de features OTIMIZADA aos dois conjuntos\n",
    "print('üîß Aplicando engenharia de features OTIMIZADA...')\n",
    "\n",
    "train_features = criar_features_avancadas_otimizada(train_data, produtos, pdvs, \"TREINO\")\n",
    "validation_features = criar_features_avancadas_otimizada(validation_data, produtos, pdvs, \"VALIDA√á√ÉO\")\n",
    "\n",
    "print('\\n‚úÖ Engenharia de features OTIMIZADA aplicada a ambos os conjuntos!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. An√°lise das Features Criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analisando features criadas SEM TARGET LEAKAGE...\n",
      "\n",
      "üìã Features dispon√≠veis (41):\n",
      "\n",
      "üè∑Ô∏è B√°sicas (6):\n",
      "   ‚Ä¢ semana\n",
      "   ‚Ä¢ pdv_id\n",
      "   ‚Ä¢ produto_id\n",
      "   ‚Ä¢ quantidade\n",
      "   ‚Ä¢ faturamento\n",
      "   ‚Ä¢ distributor_id\n",
      "\n",
      "üè∑Ô∏è Target (1):\n",
      "   ‚Ä¢ vendeu\n",
      "\n",
      "üè∑Ô∏è Auxiliares (3):\n",
      "   ‚Ä¢ categoria\n",
      "   ‚Ä¢ zipcode\n",
      "   ‚Ä¢ tipo_loja\n",
      "\n",
      "üè∑Ô∏è Pre√ßo_SEM_Leakage (2):\n",
      "   ‚Ä¢ preco_lag_1\n",
      "   ‚Ä¢ preco_lag_2\n",
      "\n",
      "üè∑Ô∏è Pre√ßo_PARA_Regress√£o (2):\n",
      "   ‚Ä¢ preco_unitario_atual\n",
      "   ‚Ä¢ preco_medio_semanal_sku_atual\n",
      "\n",
      "üè∑Ô∏è Calend√°rio (10):\n",
      "   ‚Ä¢ quantidade_media_4w\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_lag_1\n",
      "   ‚Ä¢ dia_do_mes\n",
      "   ‚Ä¢ semana_do_mes\n",
      "   ‚Ä¢ eh_inicio_mes\n",
      "   ‚Ä¢ eh_fim_mes\n",
      "   ‚Ä¢ mes\n",
      "   ‚Ä¢ mes_sin\n",
      "   ‚Ä¢ mes_cos\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_atual\n",
      "\n",
      "üè∑Ô∏è Lag (6):\n",
      "   ‚Ä¢ quantidade_lag_1\n",
      "   ‚Ä¢ quantidade_lag_2\n",
      "   ‚Ä¢ quantidade_lag_3\n",
      "   ‚Ä¢ quantidade_lag_4\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_lag_1\n",
      "   ‚Ä¢ share_vendas_sku_categoria_lag_1\n",
      "\n",
      "üè∑Ô∏è Rolling (3):\n",
      "   ‚Ä¢ quantidade_media_4w\n",
      "   ‚Ä¢ quantidade_std_4w\n",
      "   ‚Ä¢ quantidade_max_4w\n",
      "\n",
      "üè∑Ô∏è Hierarquia_SEM_Leakage (2):\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_lag_1\n",
      "   ‚Ä¢ share_vendas_sku_categoria_lag_1\n",
      "\n",
      "üè∑Ô∏è Hierarquia_PARA_Regress√£o (2):\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_atual\n",
      "   ‚Ä¢ share_vendas_sku_categoria_atual\n",
      "\n",
      "üè∑Ô∏è Tend√™ncia (2):\n",
      "   ‚Ä¢ momentum_ratio\n",
      "   ‚Ä¢ aceleracao\n",
      "\n",
      "üè∑Ô∏è Hash (6):\n",
      "   ‚Ä¢ pdv_hash\n",
      "   ‚Ä¢ produto_hash\n",
      "   ‚Ä¢ categoria_hash\n",
      "   ‚Ä¢ zipcode_hash\n",
      "   ‚Ä¢ pdv_produto_hash\n",
      "   ‚Ä¢ categoria_zipcode_hash\n",
      "\n",
      "üö® AN√ÅLISE DE TARGET LEAKAGE:\n",
      "‚úÖ Features SEGURAS para classifica√ß√£o: 30\n",
      "   ‚Ä¢ categoria\n",
      "   ‚Ä¢ zipcode\n",
      "   ‚Ä¢ tipo_loja\n",
      "   ‚Ä¢ preco_lag_1\n",
      "   ‚Ä¢ preco_lag_2\n",
      "   ‚Ä¢ variacao_preco_sku_semanal\n",
      "   ‚Ä¢ quantidade_lag_1\n",
      "   ‚Ä¢ quantidade_lag_2\n",
      "   ‚Ä¢ quantidade_lag_3\n",
      "   ‚Ä¢ quantidade_lag_4\n",
      "   ‚Ä¢ ... e mais 20 features\n",
      "\n",
      "‚ö†Ô∏è Features COM LEAKAGE (apenas para regress√£o): 4\n",
      "   ‚Ä¢ preco_unitario_atual\n",
      "   ‚Ä¢ preco_medio_semanal_sku_atual\n",
      "   ‚Ä¢ media_vendas_categoria_pdv_atual\n",
      "   ‚Ä¢ share_vendas_sku_categoria_atual\n",
      "\n",
      "üìä Estat√≠sticas do conjunto de treino (SEM LEAKAGE):\n",
      "   ‚Ä¢ Shape: (50126880, 41)\n",
      "   ‚Ä¢ Target vendeu: 5,561,122 positivos (11.1%)\n",
      "   ‚Ä¢ Quantidade m√©dia: 1.00\n",
      "   ‚Ä¢ Registros com lag_4: 5,094,636\n",
      "\n",
      "üìä Estat√≠sticas do conjunto de valida√ß√£o (SEM LEAKAGE):\n",
      "   ‚Ä¢ Shape: (5221550, 41)\n",
      "   ‚Ä¢ Target vendeu: 572,247 positivos (11.0%)\n",
      "   ‚Ä¢ Quantidade m√©dia: 0.57\n",
      "   ‚Ä¢ Registros com lag_4: 122,019\n",
      "‚úÖ An√°lise das features SEM TARGET LEAKAGE conclu√≠da\n"
     ]
    }
   ],
   "source": [
    "# Analisar features criadas SEM TARGET LEAKAGE\n",
    "print('üìä Analisando features criadas SEM TARGET LEAKAGE...')\n",
    "\n",
    "# Listar todas as features\n",
    "features_disponiveis = list(train_features.columns)\n",
    "print(f'\\nüìã Features dispon√≠veis ({len(features_disponiveis)}):')\n",
    "\n",
    "# Categorizar features CORRIGIDAS\n",
    "features_categorias = {\n",
    "    'B√°sicas': ['semana', 'pdv_id', 'produto_id', 'quantidade', 'faturamento', 'distributor_id'],\n",
    "    'Target': ['vendeu'],\n",
    "    'Auxiliares': ['categoria', 'zipcode', 'tipo_loja'],\n",
    "    'Pre√ßo_SEM_Leakage': [col for col in features_disponiveis if 'preco' in col and 'lag' in col],\n",
    "    'Pre√ßo_PARA_Regress√£o': [col for col in features_disponiveis if 'preco' in col and 'atual' in col],\n",
    "    'Calend√°rio': [col for col in features_disponiveis if any(x in col for x in ['dia', 'semana_do', 'mes', 'inicio', 'fim'])],\n",
    "    'Lag': [col for col in features_disponiveis if 'lag_' in col and 'preco' not in col],\n",
    "    'Rolling': [col for col in features_disponiveis if any(x in col for x in ['media_4w', 'std_4w', 'max_4w'])],\n",
    "    'Hierarquia_SEM_Leakage': [col for col in features_disponiveis if any(x in col for x in ['media_vendas', 'share']) and 'lag' in col],\n",
    "    'Hierarquia_PARA_Regress√£o': [col for col in features_disponiveis if any(x in col for x in ['media_vendas', 'share']) and 'atual' in col],\n",
    "    'Tend√™ncia': [col for col in features_disponiveis if any(x in col for x in ['momentum', 'aceleracao'])],\n",
    "    'Hash': [col for col in features_disponiveis if 'hash' in col]\n",
    "}\n",
    "\n",
    "for categoria, features in features_categorias.items():\n",
    "    if features:\n",
    "        print(f'\\nüè∑Ô∏è {categoria} ({len(features)}):')\n",
    "        for feat in features:\n",
    "            if feat in features_disponiveis:\n",
    "                print(f'   ‚Ä¢ {feat}')\n",
    "\n",
    "print('\\nüö® AN√ÅLISE DE TARGET LEAKAGE:')\n",
    "features_sem_leakage = []\n",
    "features_com_leakage = []\n",
    "\n",
    "for feat in features_disponiveis:\n",
    "    if feat in ['semana', 'pdv_id', 'produto_id', 'quantidade', 'faturamento', 'distributor_id', 'vendeu']:\n",
    "        continue  # B√°sicas, n√£o contar\n",
    "    elif 'atual' in feat:\n",
    "        features_com_leakage.append(feat)  # Para regress√£o apenas\n",
    "    else:\n",
    "        features_sem_leakage.append(feat)  # Seguras para classifica√ß√£o\n",
    "\n",
    "print(f'‚úÖ Features SEGURAS para classifica√ß√£o: {len(features_sem_leakage)}')\n",
    "for feat in features_sem_leakage[:10]:  # Mostrar apenas primeiras 10\n",
    "    print(f'   ‚Ä¢ {feat}')\n",
    "if len(features_sem_leakage) > 10:\n",
    "    print(f'   ‚Ä¢ ... e mais {len(features_sem_leakage)-10} features')\n",
    "\n",
    "print(f'\\n‚ö†Ô∏è Features COM LEAKAGE (apenas para regress√£o): {len(features_com_leakage)}')\n",
    "for feat in features_com_leakage:\n",
    "    print(f'   ‚Ä¢ {feat}')\n",
    "\n",
    "print('\\nüìä Estat√≠sticas do conjunto de treino (SEM LEAKAGE):')\n",
    "print(f'   ‚Ä¢ Shape: {train_features.shape}')\n",
    "print(f'   ‚Ä¢ Target vendeu: {train_features[\"vendeu\"].sum():,} positivos ({train_features[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ Quantidade m√©dia: {train_features[\"quantidade\"].mean():.2f}')\n",
    "print(f'   ‚Ä¢ Registros com lag_4: {(train_features[\"quantidade_lag_4\"] > 0).sum():,}')\n",
    "\n",
    "print('\\nüìä Estat√≠sticas do conjunto de valida√ß√£o (SEM LEAKAGE):')\n",
    "print(f'   ‚Ä¢ Shape: {validation_features.shape}')\n",
    "print(f'   ‚Ä¢ Target vendeu: {validation_features[\"vendeu\"].sum():,} positivos ({validation_features[\"vendeu\"].mean()*100:.1f}%)')\n",
    "print(f'   ‚Ä¢ Quantidade m√©dia: {validation_features[\"quantidade\"].mean():.2f}')\n",
    "print(f'   ‚Ä¢ Registros com lag_4: {(validation_features[\"quantidade_lag_4\"] > 0).sum():,}')\n",
    "\n",
    "print('‚úÖ An√°lise das features SEM TARGET LEAKAGE conclu√≠da')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Salvamento dos Dados Enriquecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Salvando datasets enriquecidos...\n",
      "üßπ Limpando tipos de dados...\n",
      "‚úÖ Arquivos salvos:\n",
      "   ‚Ä¢ data/submissao3/train_features.parquet\n",
      "   ‚Ä¢ data/submissao3/validation_features.parquet\n",
      "üìã Metadados salvos em: data/submissao3/advanced_features_metadata.pkl\n",
      "\n",
      "üéâ ENGENHARIA DE FEATURES AVAN√áADA CONCLU√çDA!\n",
      "======================================================================\n",
      "üéØ Features criadas para modelo de dois est√°gios:\n",
      "   üìä CLASSIFICA√á√ÉO: Use target \"vendeu\" para treinar se vai vender\n",
      "   üìä REGRESS√ÉO: Use target \"quantidade\" (apenas onde vendeu=1) para quanto vai vender\n",
      "\n",
      "üí° Pr√≥ximos passos:\n",
      "   1. Treinar modelo de classifica√ß√£o (LGBMClassifier)\n",
      "   2. Treinar modelo de regress√£o (LGBMRegressor)\n",
      "   3. Combinar os dois no pipeline final\n",
      "\n",
      "üöÄ Dados prontos para modelagem avan√ßada!\n"
     ]
    }
   ],
   "source": [
    "# Salvar datasets enriquecidos\n",
    "print('üíæ Salvando datasets enriquecidos...')\n",
    "\n",
    "# Fun√ß√£o para limpar tipos de dados antes de salvar\n",
    "def limpar_tipos_dados(df):\n",
    "    \"\"\"Limpa tipos de dados problem√°ticos para salvar em Parquet\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Converter colunas object que s√£o num√©ricas\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            try:\n",
    "                # Tentar converter para numeric\n",
    "                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "                # Se ainda tem NaN, preencher com 0\n",
    "                if df_clean[col].isna().any():\n",
    "                    df_clean[col] = df_clean[col].fillna(0)\n",
    "            except:\n",
    "                # Se n√£o conseguir converter, manter como string\n",
    "                df_clean[col] = df_clean[col].astype(str)\n",
    "    \n",
    "    # Garantir que colunas ID sejam inteiras\n",
    "    id_cols = ['pdv_id', 'produto_id', 'distributor_id']\n",
    "    for col in id_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].astype('int64')\n",
    "    \n",
    "    # Garantir que colunas categ√≥ricas sejam string\n",
    "    cat_cols = ['categoria', 'zipcode', 'tipo_loja']\n",
    "    for col in cat_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].astype(str)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Limpar tipos dos datasets\n",
    "print('üßπ Limpando tipos de dados...')\n",
    "train_features_clean = limpar_tipos_dados(train_features)\n",
    "validation_features_clean = limpar_tipos_dados(validation_features)\n",
    "\n",
    "# Salvar em data/submissao3\n",
    "train_features_clean.to_parquet('../data/submissao3/train_features.parquet', index=False)\n",
    "validation_features_clean.to_parquet('../data/submissao3/validation_features.parquet', index=False)\n",
    "\n",
    "print('‚úÖ Arquivos salvos:')\n",
    "print('   ‚Ä¢ data/submissao3/train_features.parquet')\n",
    "print('   ‚Ä¢ data/submissao3/validation_features.parquet')\n",
    "\n",
    "# Salvar metadados das features\n",
    "import pickle\n",
    "\n",
    "metadados_features = {\n",
    "    'data_criacao': pd.Timestamp.now(),\n",
    "    'estrategia': 'Engenharia de Features Avan√ßada para Modelo de Dois Est√°gios',\n",
    "    'total_features': len(train_features_clean.columns),\n",
    "    'features_por_categoria': {cat: len(feats) for cat, feats in features_categorias.items() if feats},\n",
    "    'target_classificacao': 'vendeu (1 se quantidade > 0, 0 caso contr√°rio)',\n",
    "    'target_regressao': 'quantidade (apenas onde vendeu = 1)',\n",
    "    'registros_treino': len(train_features_clean),\n",
    "    'registros_validacao': len(validation_features_clean),\n",
    "    'features_lista': list(train_features_clean.columns),\n",
    "    'observacoes': [\n",
    "        'Features de pre√ßo incluem varia√ß√µes e m√©dias temporais',\n",
    "        'Features de calend√°rio incluem componentes c√≠clicos',\n",
    "        'Features de hierarquia agregam por categoria e zipcode',\n",
    "        'Features de tend√™ncia capturam momentum e acelera√ß√£o',\n",
    "        'Features hash criam embeddings para vari√°veis categ√≥ricas'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../data/submissao3/advanced_features_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadados_features, f)\n",
    "\n",
    "print('üìã Metadados salvos em: data/submissao3/advanced_features_metadata.pkl')\n",
    "\n",
    "print('\\nüéâ ENGENHARIA DE FEATURES AVAN√áADA CONCLU√çDA!')\n",
    "print('=' * 70)\n",
    "print('üéØ Features criadas para modelo de dois est√°gios:')\n",
    "print('   üìä CLASSIFICA√á√ÉO: Use target \"vendeu\" para treinar se vai vender')\n",
    "print('   üìä REGRESS√ÉO: Use target \"quantidade\" (apenas onde vendeu=1) para quanto vai vender')\n",
    "print('\\nüí° Pr√≥ximos passos:')\n",
    "print('   1. Treinar modelo de classifica√ß√£o (LGBMClassifier)')\n",
    "print('   2. Treinar modelo de regress√£o (LGBMRegressor)')\n",
    "print('   3. Combinar os dois no pipeline final')\n",
    "print('\\nüöÄ Dados prontos para modelagem avan√ßada!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
