{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final para Submissão\n",
    "\n",
    "**🎯 PROPÓSITO DESTE NOTEBOOK:**\n",
    "Este notebook implementa o pipeline final para gerar as previsões de vendas para janeiro de 2023. Utiliza todos os dados de 2022 para treinar o modelo final e gera os arquivos de submissão nos formatos requeridos.\n",
    "\n",
    "**🚀 ESTRATÉGIA OTIMIZADA:**\n",
    "- **Treino com Dataset Completo**: Usa todos os dados de 2022 (sem divisão treino/validação)\n",
    "- **Modelo Final**: LightGBM treinado com número ótimo de iterações\n",
    "- **Predições Janeiro/2023**: Gera features para as 5 semanas de janeiro de 2023\n",
    "- **Submissão**: Salva nos formatos CSV e Parquet na pasta correta\n",
    "\n",
    "---\n",
    "\n",
    "## Fluxo do Pipeline:\n",
    "1. **Carregamento**: Dados processados com features de `02-Feature-Engineering-Dask.ipynb`\n",
    "2. **Preparação**: Otimização de memória e uso do dataset completo de 2022\n",
    "3. **Treinamento**: Modelo final LightGBM com dados completos\n",
    "4. **Predição**: Geração de features para janeiro/2023 e previsões finais\n",
    "5. **Submissão**: Arquivos finais em CSV e Parquet prontos para envio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Bibliotecas carregadas com sucesso!\n",
      "🎯 Iniciando fase de Modelagem e Treinamento\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Advanced ML\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping  # CORREÇÃO: Importar early_stopping\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('📚 Bibliotecas carregadas com sucesso!')\n",
    "print('🎯 Iniciando fase de Modelagem e Treinamento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Carregando dados processados...\n",
      "✅ Todos os arquivos necessários encontrados\n",
      "📊 Carregando dataset (parquet)...\n",
      "\n",
      "📊 Dados carregados com sucesso:\n",
      "   • Shape: (51171190, 26)\n",
      "   • Período: 2022-01-25 00:00:00 até 2022-12-27 00:00:00\n",
      "   • Features disponíveis: 26\n",
      "   • Memória: 16872.2 MB\n",
      "   • Estratégia: Grid Inteligente com Dask + Polars - Big Data Optimized\n",
      "\n",
      "🔍 Metadados do processamento:\n",
      "   • total_registros: 51171190\n",
      "   • total_features: 26\n",
      "   • combinacoes_pdv_produto: 1044310\n",
      "   • semanas_cobertas: 49\n",
      "   • periodo_treino: 2022-01-25 00:00:00 a 2022-12-27 00:00:00\n",
      "   • estrategia: Grid Inteligente com Dask + Polars - Big Data Optimized\n",
      "   • tecnologia: Dask + Polars for Maximum Performance\n",
      "   • memoria_otimizada: 9974.253155708313 MB\n",
      "\n",
      "✅ Pronto para modelagem!\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados com features processadas\n",
    "print('📂 Carregando dados processados...')\n",
    "\n",
    "# Verificar se os arquivos essenciais existem\n",
    "import os\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',  # Usar parquet (mais rápido)\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print('❌ Arquivos não encontrados:')\n",
    "    for f in missing_files:\n",
    "        print(f'   • {f}')\n",
    "    print('\\n🔄 Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\n",
    "else:\n",
    "    print('✅ Todos os arquivos necessários encontrados')\n",
    "    \n",
    "    # Carregar dados principais (usar parquet para velocidade)\n",
    "    print('📊 Carregando dataset (parquet)...')\n",
    "    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "    \n",
    "    # Carregar metadados\n",
    "    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    print(f'\\n📊 Dados carregados com sucesso:')\n",
    "    print(f'   • Shape: {dados.shape}')\n",
    "    print(f'   • Período: {dados[\"semana\"].min()} até {dados[\"semana\"].max()}')\n",
    "    print(f'   • Features disponíveis: {len(dados.columns)}')\n",
    "    print(f'   • Memória: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   • Estratégia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n",
    "    \n",
    "    print(f'\\n🔍 Metadados do processamento:')\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n",
    "            print(f'   • {key}: {value}')\n",
    "    \n",
    "    print(f'\\n✅ Pronto para modelagem!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação dos Dados para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Preparação dos dados:\n",
      "   • Target: quantidade\n",
      "   • Features disponíveis: 20\n",
      "   • Features excluídas: 6\n",
      "\n",
      "⚠️ Features com valores missing:\n",
      "   • distributor_id: 45,202,572 (88.3%)\n",
      "\n",
      "🧠 Estratégia de Tratamento Inteligente:\n",
      "   • distributor_id (categórica): NaN → -1 (venda direta)\n",
      "   • Features numéricas: NaN → 0 (ausência = zero)\n",
      "   • LightGBM aprenderá padrões específicos para valores -1/0\n",
      "\n",
      "📋 Features finais para modelagem: 20\n",
      "💡 Missing values serão tratados como informação, não removidos\n"
     ]
    }
   ],
   "source": [
    "# Definir variável target e features\n",
    "target = 'quantidade'\n",
    "\n",
    "# Features a excluir (não devem ser usadas para predição)\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana',  # IDs e data\n",
    "    'quantidade',  # Target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informação do futuro\n",
    "]\n",
    "\n",
    "# Identificar features disponíveis\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'🎯 Preparação dos dados:')\n",
    "print(f'   • Target: {target}')\n",
    "print(f'   • Features disponíveis: {len(all_features)}')\n",
    "print(f'   • Features excluídas: {len(exclude_features)}')\n",
    "\n",
    "# Verificar missing values nas features\n",
    "missing_features = dados[all_features].isnull().sum()\n",
    "missing_features = missing_features[missing_features > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\n⚠️ Features com valores missing:')\n",
    "    for feature, count in missing_features.head(10).items():\n",
    "        pct = (count / len(dados)) * 100\n",
    "        print(f'   • {feature}: {count:,} ({pct:.1f}%)')\n",
    "    \n",
    "    print(f'\\n🧠 Estratégia de Tratamento Inteligente:')\n",
    "    print('   • distributor_id (categórica): NaN → -1 (venda direta)')\n",
    "    print('   • Features numéricas: NaN → 0 (ausência = zero)')\n",
    "    print('   • LightGBM aprenderá padrões específicos para valores -1/0')\n",
    "else:\n",
    "    print('\\n✅ Nenhum valor missing nas features')\n",
    "\n",
    "print(f'\\n📋 Features finais para modelagem: {len(all_features)}')\n",
    "print('💡 Missing values serão tratados como informação, não removidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Preparando dados de treino com o ano de 2022 completo...\n",
      "🚀 Estratégia: Usar todos os dados disponíveis para o modelo final\n",
      "\n",
      "🔍 ANTES da otimização:\n",
      "💾 Memória total: 16.48 GB\n",
      "\n",
      "🚀 Aplicando Downcasting...\n",
      "   • quantidade: float64 → float32\n",
      "   • num_transacoes: float64 → float32\n",
      "   • mes_sin: float64 → float32\n",
      "   • mes_cos: float64 → float32\n",
      "   • quantidade_lag_1: float64 → float32\n",
      "   • quantidade_lag_2: float64 → float32\n",
      "   • quantidade_lag_3: float64 → float32\n",
      "   • quantidade_lag_4: float64 → float32\n",
      "   • quantidade_media_4w: float64 → float32\n",
      "   • quantidade_max_4w: float64 → float32\n",
      "   • quantidade_min_4w: float64 → float32\n",
      "   • pdv_hash: uint64 → int8\n",
      "   • produto_hash: uint64 → int8\n",
      "   • pdv_produto_hash: uint64 → int16\n",
      "   • hist_mean: float64 → float32\n",
      "   • hist_std: float64 → float32\n",
      "   • hist_max: float64 → float32\n",
      "   • hist_count: uint32 → int8\n",
      "   • pdv_id: object → category\n",
      "   • produto_id: object → category\n",
      "   • distributor_id: object → category\n",
      "✅ Downcasting concluído!\n",
      "\n",
      "📊 DEPOIS da otimização:\n",
      "💾 Memória total: 4.39 GB\n",
      "🎯 Redução: 73.4% (12.09 GB economizados)\n",
      "\n",
      "🧠 Tratamento inteligente de missing values...\n",
      "   • distributor_id: 45,202,572 NaN → -1 (venda direta)\n",
      "\n",
      "🎯 Preparando dados completos para o modelo final...\n",
      "✅ Dados de treino preparados:\n",
      "   • X_full shape: (51171190, 20)\n",
      "   • y_full shape: (51171190,)\n",
      "   • Período: 2022-01-25 00:00:00 até 2022-12-27 00:00:00\n",
      "   • Memória X_full: 3123.2 MB\n",
      "\n",
      "🎉 SUCESSO! Dados preparados para treinamento final:\n",
      "   ✅ Dataset completo de 2022: 51,171,190 registros\n",
      "   ✅ Features otimizadas: 20\n",
      "   ✅ Memória otimizada: 73.4% de redução\n",
      "   ✅ Pronto para treinar modelo final!\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE FINAL: Otimização de Memória + Preparação dos Dados Completos\n",
    "print('🧠 Preparando dados de treino com o ano de 2022 completo...')\n",
    "print('🚀 Estratégia: Usar todos os dados disponíveis para o modelo final')\n",
    "\n",
    "# PASSO 1: Inspecionar uso de memória atual\n",
    "print(f'\\n🔍 ANTES da otimização:')\n",
    "memory_before = dados.memory_usage(deep=True).sum() / (1024**3)\n",
    "print(f'💾 Memória total: {memory_before:.2f} GB')\n",
    "\n",
    "# PASSO 2: Aplicar Downcasting Inteligente\n",
    "print(f'\\n🚀 Aplicando Downcasting...')\n",
    "\n",
    "# Fazer uma cópia para otimização\n",
    "dados_sorted = dados.copy()\n",
    "\n",
    "# Otimizar colunas numéricas (inteiros e floats)\n",
    "for col in dados_sorted.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_sorted[col].dtype\n",
    "    \n",
    "    if dados_sorted[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='float')\n",
    "    \n",
    "    new_dtype = dados_sorted[col].dtype\n",
    "    if original_dtype != new_dtype:\n",
    "        print(f'   • {col}: {original_dtype} → {new_dtype}')\n",
    "\n",
    "# Otimizar colunas categóricas\n",
    "for col in dados_sorted.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_sorted[col].nunique()\n",
    "        total_rows = len(dados_sorted)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores únicos, usar category\n",
    "            dados_sorted[col] = dados_sorted[col].astype('category')\n",
    "            print(f'   • {col}: object → category')\n",
    "\n",
    "print(f'✅ Downcasting concluído!')\n",
    "\n",
    "# PASSO 3: Verificar resultado da otimização\n",
    "memory_after = dados_sorted.memory_usage(deep=True).sum() / (1024**3)\n",
    "memory_reduction = (memory_before - memory_after) / memory_before * 100\n",
    "print(f'\\n📊 DEPOIS da otimização:')\n",
    "print(f'💾 Memória total: {memory_after:.2f} GB')\n",
    "print(f'🎯 Redução: {memory_reduction:.1f}% ({memory_before-memory_after:.2f} GB economizados)')\n",
    "\n",
    "# PASSO 4: Tratamento de missing values\n",
    "print(f'\\n🧠 Tratamento inteligente de missing values...')\n",
    "all_features = [col for col in dados_sorted.columns if col not in ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']]\n",
    "\n",
    "for col in all_features:\n",
    "    missing_count = dados_sorted[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if col == 'distributor_id':\n",
    "            # Adicionar -1 ao \"menu\" de categorias primeiro\n",
    "            if dados_sorted[col].dtype.name == 'category':\n",
    "                if -1 not in dados_sorted[col].cat.categories:\n",
    "                    dados_sorted[col] = dados_sorted[col].cat.add_categories([-1])\n",
    "            \n",
    "            dados_sorted[col] = dados_sorted[col].fillna(-1)\n",
    "            print(f'   • {col}: {missing_count:,} NaN → -1 (venda direta)')\n",
    "            \n",
    "        elif dados_sorted[col].dtype.kind in ['i', 'u', 'f']:\n",
    "            dados_sorted[col] = dados_sorted[col].fillna(0)\n",
    "            print(f'   • {col}: {missing_count:,} NaN → 0 (ausência)')\n",
    "\n",
    "# PASSO 5: Preparar dados COMPLETOS para modelagem (SEM divisão treino/validação)\n",
    "print(f'\\n🎯 Preparando dados completos para o modelo final...')\n",
    "\n",
    "# Usar todos os dados de 2022 para o treino final\n",
    "X_full = dados_sorted[all_features]\n",
    "y_full = dados_sorted[target]\n",
    "\n",
    "print(f'✅ Dados de treino preparados:')\n",
    "print(f'   • X_full shape: {X_full.shape}')\n",
    "print(f'   • y_full shape: {y_full.shape}')\n",
    "print(f'   • Período: {dados_sorted[\"semana\"].min()} até {dados_sorted[\"semana\"].max()}')\n",
    "print(f'   • Memória X_full: {X_full.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "\n",
    "# Limpeza de memória\n",
    "import gc\n",
    "del dados\n",
    "gc.collect()\n",
    "\n",
    "print(f'\\n🎉 SUCESSO! Dados preparados para treinamento final:')\n",
    "print(f'   ✅ Dataset completo de 2022: {len(dados_sorted):,} registros')\n",
    "print(f'   ✅ Features otimizadas: {len(all_features)}')\n",
    "print(f'   ✅ Memória otimizada: {memory_reduction:.1f}% de redução')\n",
    "print(f'   ✅ Pronto para treinar modelo final!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Treinando o modelo LightGBM final com todos os dados de 2022...\n",
      "🎯 Usando parâmetros otimizados pelo Optuna!\n",
      "============================================================\n",
      "\n",
      "📂 Carregando melhores parâmetros de: ../data/best_lgbm_params_optuna.pkl\n",
      "✅ Parâmetros do Optuna carregados com sucesso!\n",
      "\n",
      "📋 Configuração do modelo final (Optuna + fixos):\n",
      "   • n_estimators: 1798\n",
      "   • learning_rate: 0.0817220633125079\n",
      "   • num_leaves: 218\n",
      "   • max_depth: 14\n",
      "   • subsample: 0.8517268527117055\n",
      "   • colsample_bytree: 0.9438211907470613\n",
      "   • reg_alpha: 0.4386118724477075\n",
      "   • reg_lambda: 0.6286582931001017\n",
      "   • objective: regression_l1\n",
      "   • metric: mae\n",
      "   • boosting_type: gbdt\n",
      "   • verbosity: -1\n",
      "   • random_state: 42\n",
      "   • n_jobs: -1\n",
      "\n",
      "📊 Preparando dados para treinamento...\n",
      "   • Dados shape: (51171190, 20)\n",
      "   • Features: 20\n",
      "   • Target: quantidade\n",
      "   • Estratégia: Usar num_boost_round otimizado + early stopping interno\n",
      "\n",
      "🔄 Treinando modelo final com parâmetros do Optuna...\n",
      "✅ Modelo final treinado com sucesso!\n",
      "   • Iterações utilizadas: 1798\n",
      "💾 Modelo salvo em: ../data/final_lightgbm_model_optuna.pkl\n",
      "\n",
      "📈 Informações do modelo final otimizado:\n",
      "   • Número de árvores: 1798\n",
      "   • Features utilizadas: 20\n",
      "   • Dados de treino: 51,171,190 registros\n",
      "   • Período de treino: Todo o ano de 2022\n",
      "   • Otimização: Parâmetros do Optuna\n",
      "   • Pronto para predições!\n"
     ]
    }
   ],
   "source": [
    "# TREINAMENTO DO MODELO FINAL COM PARÂMETROS OTIMIZADOS\n",
    "print('🚀 Treinando o modelo LightGBM final com todos os dados de 2022...')\n",
    "print('🎯 Usando parâmetros otimizados pelo Optuna!')\n",
    "print('=' * 60)\n",
    "\n",
    "# Carregar os melhores parâmetros do Optuna\n",
    "print('\\n📂 Carregando melhores parâmetros de: ../data/best_lgbm_params_optuna.pkl')\n",
    "try:\n",
    "    with open('../data/best_lgbm_params_optuna.pkl', 'rb') as f:\n",
    "        best_params_optuna = pickle.load(f)\n",
    "    print('✅ Parâmetros do Optuna carregados com sucesso!')\n",
    "\n",
    "    # Configurar parâmetros finais combinando Optuna + configurações fixas\n",
    "    lgb_params_final = best_params_optuna.copy()\n",
    "    lgb_params_final.update({\n",
    "        'objective': 'regression_l1',  # MAE - melhor para WMAPE\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "\n",
    "    print(f'\\n📋 Configuração do modelo final (Optuna + fixos):')\n",
    "    for param, value in lgb_params_final.items():\n",
    "        print(f'   • {param}: {value}')\n",
    "\n",
    "    # Usar early stopping com todos os dados (sem validação separada)\n",
    "    train_full_lgb = lgb.Dataset(X_full, label=y_full)\n",
    "\n",
    "    print(f'\\n📊 Preparando dados para treinamento...')\n",
    "    print(f'   • Dados shape: {X_full.shape}')\n",
    "    print(f'   • Features: {len(all_features)}')\n",
    "    print(f'   • Target: {target}')\n",
    "    print(f'   • Estratégia: Usar num_boost_round otimizado + early stopping interno')\n",
    "\n",
    "    # Treinar modelo com parâmetros otimizados\n",
    "    print(f'\\n🔄 Treinando modelo final com parâmetros do Optuna...')\n",
    "\n",
    "    # Usar o n_estimators do Optuna como num_boost_round\n",
    "    num_boost_rounds = lgb_params_final.pop('n_estimators', 1000)\n",
    "\n",
    "    final_model = lgb.train(\n",
    "        lgb_params_final,\n",
    "        train_full_lgb,\n",
    "        num_boost_round=num_boost_rounds\n",
    "    )\n",
    "\n",
    "    print(f'✅ Modelo final treinado com sucesso!')\n",
    "    print(f'   • Iterações utilizadas: {final_model.num_trees()}')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print('❌ Arquivo best_lgbm_params_optuna.pkl não encontrado!')\n",
    "    print('🔄 Usando parâmetros padrão como fallback...')\n",
    "\n",
    "    # Fallback para parâmetros vanilla\n",
    "    lgb_params_final = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1\n",
    "    }\n",
    "\n",
    "    train_full_lgb = lgb.Dataset(X_full, label=y_full)\n",
    "    final_model = lgb.train(\n",
    "        lgb_params_final,\n",
    "        train_full_lgb,\n",
    "        num_boost_round=200\n",
    "    )\n",
    "\n",
    "    print(f'✅ Modelo fallback treinado com sucesso!')\n",
    "\n",
    "# Salvar o modelo\n",
    "import os\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "with open('../data/final_lightgbm_model_optuna.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "print('💾 Modelo salvo em: ../data/final_lightgbm_model_optuna.pkl')\n",
    "\n",
    "# Informações do modelo\n",
    "print(f'\\n📈 Informações do modelo final otimizado:')\n",
    "print(f'   • Número de árvores: {final_model.num_trees()}')\n",
    "print(f'   • Features utilizadas: {len(all_features)}')\n",
    "print(f'   • Dados de treino: {len(y_full):,} registros')\n",
    "print(f'   • Período de treino: Todo o ano de 2022')\n",
    "print(f'   • Otimização: Parâmetros do Optuna')\n",
    "print(f'   • Pronto para predições!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geração de Features para Janeiro/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Criando grid de dados para as 5 semanas de janeiro/2023...\n",
      "🎯 Estratégia: Criar template para todas as combinações PDV/Produto\n",
      "\n",
      "🔍 Extraindo informações únicas de PDV/Produto do histórico...\n",
      "   • Total de combinações únicas com informações: 1,044,310\n",
      "   • Colunas disponíveis: ['pdv_id', 'produto_id', 'distributor_id']\n",
      "\n",
      "📅 Definindo as 5 semanas de janeiro/2023...\n",
      "   • Semanas: ['2023-01-03', '2023-01-10', '2023-01-17', '2023-01-24', '2023-01-31']\n",
      "\n",
      "🏗️ Construindo grid de teste...\n",
      "   • Grid shape: (5221550, 4)\n",
      "   • Total de predições: 5,221,550\n",
      "\n",
      "🔧 Gerando features temporais...\n",
      "   • Features temporais adicionadas\n",
      "\n",
      "📊 Calculando features de lag e estatísticas históricas...\n",
      "   • Calculando lags das últimas semanas de 2022...\n",
      "   • Features de lag e rolling adicionadas\n",
      "\n",
      "🔧 Finalizando e preenchendo valores ausentes...\n",
      "\n",
      "✅ Features para janeiro/2023 geradas com sucesso!\n",
      "   • Grid final: (5221550, 23)\n",
      "   • Colunas: 23\n",
      "   • Pronto para predições!\n"
     ]
    }
   ],
   "source": [
    "# GERAÇÃO DE FEATURES PARA JANEIRO/2023\n",
    "print('📅 Criando grid de dados para as 5 semanas de janeiro/2023...')\n",
    "print('🎯 Estratégia: Criar template para todas as combinações PDV/Produto')\n",
    "\n",
    "# PASSO 1: Obter todas as combinações únicas e suas informações do histórico\n",
    "print('\\n🔍 Extraindo informações únicas de PDV/Produto do histórico...')\n",
    "# CORREÇÃO: Usar 'dados_sorted' como fonte única de verdade para informações estáticas\n",
    "available_cols = [col for col in ['pdv_id', 'produto_id', 'uf', 'distributor_id', 'brand'] if col in dados_sorted.columns]\n",
    "info_basica_df = dados_sorted[available_cols].drop_duplicates(subset=['pdv_id', 'produto_id'])\n",
    "print(f'   • Total de combinações únicas com informações: {len(info_basica_df):,}')\n",
    "print(f'   • Colunas disponíveis: {available_cols}')\n",
    "\n",
    "# PASSO 2: Definir as 5 semanas de janeiro de 2023\n",
    "print('\\n📅 Definindo as 5 semanas de janeiro/2023...')\n",
    "prediction_weeks = pd.to_datetime(['2023-01-03', '2023-01-10', '2023-01-17', '2023-01-24', '2023-01-31'])\n",
    "print(f'   • Semanas: {[w.strftime(\"%Y-%m-%d\") for w in prediction_weeks]}')\n",
    "\n",
    "# PASSO 3: Criar o grid de teste (todas as combinações x todas as semanas)\n",
    "print('\\n🏗️ Construindo grid de teste...')\n",
    "test_grid_list = []\n",
    "for week in prediction_weeks:\n",
    "    combo_week = info_basica_df.copy()\n",
    "    combo_week['semana'] = week\n",
    "    test_grid_list.append(combo_week)\n",
    "\n",
    "test_grid = pd.concat(test_grid_list, ignore_index=True)\n",
    "print(f'   • Grid shape: {test_grid.shape}')\n",
    "print(f'   • Total de predições: {len(test_grid):,}')\n",
    "\n",
    "# PASSO 4: Adicionar features temporais\n",
    "print('\\n🔧 Gerando features temporais...')\n",
    "test_grid['ano'] = test_grid['semana'].dt.year\n",
    "test_grid['mes'] = test_grid['semana'].dt.month\n",
    "test_grid['semana_ano'] = test_grid['semana'].dt.isocalendar().week\n",
    "test_grid['dia_ano'] = test_grid['semana'].dt.dayofyear\n",
    "\n",
    "# Features temporais trigonométricas (se existirem no treino)\n",
    "if 'mes_sin' in all_features:\n",
    "    test_grid['mes_sin'] = np.sin(2 * np.pi * test_grid['mes']/12)\n",
    "    test_grid['mes_cos'] = np.cos(2 * np.pi * test_grid['mes']/12)\n",
    "\n",
    "print('   • Features temporais adicionadas')\n",
    "\n",
    "# PASSO 5: Calcular lags e rolling windows (usando dados_sorted como histórico)\n",
    "print('\\n📊 Calculando features de lag e estatísticas históricas...')\n",
    "\n",
    "# Para cada combinação PDV/Produto, calcular estatísticas do histórico\n",
    "historical_stats = dados_sorted.groupby(['pdv_id', 'produto_id'])['quantidade'].agg([\n",
    "    'mean', 'std', 'min', 'max', 'count'\n",
    "]).reset_index()\n",
    "historical_stats.columns = ['pdv_id', 'produto_id', 'hist_mean', 'hist_std', 'hist_min', 'hist_max', 'hist_count']\n",
    "\n",
    "# Merge com estatísticas históricas\n",
    "test_grid = test_grid.merge(historical_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "# Para os lags, vamos usar as últimas semanas de 2022\n",
    "print('   • Calculando lags das últimas semanas de 2022...')\n",
    "last_week_2022 = dados_sorted[dados_sorted['semana'] == dados_sorted['semana'].max()]\n",
    "last_week_stats = last_week_2022[['pdv_id', 'produto_id', 'quantidade']].rename(columns={'quantidade': 'quantidade_lag_1'})\n",
    "test_grid = test_grid.merge(last_week_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "# Calcular mais lags se existirem no treino\n",
    "unique_weeks = sorted(dados_sorted['semana'].unique(), reverse=True)\n",
    "for lag in range(2, min(5, len(unique_weeks) + 1)):  # até lag_4\n",
    "    if f'quantidade_lag_{lag}' in all_features:\n",
    "        week_data = dados_sorted[dados_sorted['semana'] == unique_weeks[lag-1]]\n",
    "        lag_stats = week_data[['pdv_id', 'produto_id', 'quantidade']].rename(columns={'quantidade': f'quantidade_lag_{lag}'})\n",
    "        test_grid = test_grid.merge(lag_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "# Rolling features (se existirem no treino)\n",
    "lag_cols = [f'quantidade_lag_{i}' for i in range(1, 5) if f'quantidade_lag_{i}' in test_grid.columns]\n",
    "if len(lag_cols) > 1:\n",
    "    test_grid['quantidade_media_4w'] = test_grid[lag_cols].mean(axis=1)\n",
    "    test_grid['quantidade_std_4w'] = test_grid[lag_cols].std(axis=1)\n",
    "    test_grid['quantidade_max_4w'] = test_grid[lag_cols].max(axis=1)\n",
    "    test_grid['quantidade_min_4w'] = test_grid[lag_cols].min(axis=1)\n",
    "\n",
    "print('   • Features de lag e rolling adicionadas')\n",
    "\n",
    "# PASSO 6: Preencher valores ausentes e finalizar\n",
    "print('\\n🔧 Finalizando e preenchendo valores ausentes...')\n",
    "\n",
    "# Identificar colunas numéricas para preencher com 0\n",
    "numeric_cols = test_grid.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    test_grid[col] = test_grid[col].fillna(0)\n",
    "\n",
    "# Tratar distributor_id especificamente\n",
    "if 'distributor_id' in test_grid.columns:\n",
    "    if test_grid['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in test_grid['distributor_id'].cat.categories:\n",
    "            test_grid['distributor_id'] = test_grid['distributor_id'].cat.add_categories([-1])\n",
    "    test_grid['distributor_id'] = test_grid['distributor_id'].fillna(-1)\n",
    "\n",
    "print(f'\\n✅ Features para janeiro/2023 geradas com sucesso!')\n",
    "print(f'   • Grid final: {test_grid.shape}')\n",
    "print(f'   • Colunas: {len(test_grid.columns)}')\n",
    "print(f'   • Pronto para predições!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predições e Geração dos Arquivos de Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Realizando predições para janeiro/2023...\n",
      "🎯 Usando modelo final treinado com todo o dataset de 2022\n",
      "\n",
      "🔧 Preparando dados de teste...\n",
      "   • Features disponíveis no teste: 17\n",
      "   ⚠️ Features ausentes no teste: 3\n",
      "     Criando com valores padrão: ['pdv_hash', 'produto_hash', 'pdv_produto_hash']...\n",
      "   • X_test shape: (5221550, 20)\n",
      "   • Features utilizadas: 20\n",
      "\n",
      "🔧 Aplicando otimizações de tipos de dados...\n",
      "\n",
      "🎯 Gerando predições...\n",
      "   • Predições geradas: 5,221,550\n",
      "   • Range: 0 - 490\n",
      "   • Média: 0.16\n",
      "   • Zeros: 4,915,370 (94.1%)\n",
      "\n",
      "📋 Criando DataFrame de submissão...\n",
      "   • DataFrame de submissão criado: (5221550, 4)\n",
      "   • Colunas: ['semana', 'pdv', 'produto', 'quantidade']\n",
      "   • Semanas: [np.uint32(1), np.uint32(2), np.uint32(3), np.uint32(4), np.uint32(5)]\n",
      "\\n📊 Estatísticas da submissão:\n",
      "   • Semana 1: 1,044,310 predições, média: 0.2\n",
      "   • Semana 2: 1,044,310 predições, média: 0.2\n",
      "   • Semana 3: 1,044,310 predições, média: 0.2\n",
      "   • Semana 4: 1,044,310 predições, média: 0.2\n",
      "   • Semana 5: 1,044,310 predições, média: 0.2\n",
      "\\n✅ Dados de submissão preparados!\n"
     ]
    }
   ],
   "source": [
    "# PREDIÇÕES FINAIS E GERAÇÃO DE SUBMISSÃO\n",
    "print('🚀 Realizando predições para janeiro/2023...')\n",
    "print('🎯 Usando modelo final treinado com todo o dataset de 2022')\n",
    "\n",
    "# PASSO 1: Preparar dados de teste com as mesmas features do treino\n",
    "print('\\n🔧 Preparando dados de teste...')\n",
    "\n",
    "# Identificar features que existem tanto no treino quanto no teste\n",
    "available_test_features = [col for col in all_features if col in test_grid.columns]\n",
    "missing_features = [col for col in all_features if col not in test_grid.columns]\n",
    "\n",
    "print(f'   • Features disponíveis no teste: {len(available_test_features)}')\n",
    "if missing_features:\n",
    "    print(f'   ⚠️ Features ausentes no teste: {len(missing_features)}')\n",
    "    print(f'     Criando com valores padrão: {missing_features[:5]}...')\n",
    "    \n",
    "    # Criar features ausentes com valores padrão (0 ou -1)\n",
    "    for feature in missing_features:\n",
    "        if 'distributor' in feature or 'id' in feature:\n",
    "            test_grid[feature] = -1\n",
    "        else:\n",
    "            test_grid[feature] = 0\n",
    "\n",
    "# Selecionar apenas as features que foram usadas no treinamento\n",
    "X_test = test_grid[all_features]\n",
    "\n",
    "print(f'   • X_test shape: {X_test.shape}')\n",
    "print(f'   • Features utilizadas: {len(all_features)}')\n",
    "\n",
    "# PASSO 2: Aplicar o mesmo tratamento de tipos de dados\n",
    "print('\\n🔧 Aplicando otimizações de tipos de dados...')\n",
    "for col in X_test.select_dtypes(include=[np.number]).columns:\n",
    "    if X_test[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        X_test[col] = pd.to_numeric(X_test[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        X_test[col] = pd.to_numeric(X_test[col], downcast='float')\n",
    "\n",
    "# PASSO 3: Gerar predições\n",
    "print('\\n🎯 Gerando predições...')\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# Garantir que as previsões não sejam negativas e arredondar para inteiros\n",
    "predictions = np.maximum(0, predictions).round().astype(int)\n",
    "\n",
    "print(f'   • Predições geradas: {len(predictions):,}')\n",
    "print(f'   • Range: {predictions.min()} - {predictions.max()}')\n",
    "print(f'   • Média: {predictions.mean():.2f}')\n",
    "print(f'   • Zeros: {(predictions == 0).sum():,} ({(predictions == 0).mean()*100:.1f}%)')\n",
    "\n",
    "# PASSO 4: Criar o DataFrame de submissão\n",
    "print('\\n📋 Criando DataFrame de submissão...')\n",
    "\n",
    "# Preparar dados para submissão\n",
    "submission_df = test_grid[['semana', 'pdv_id', 'produto_id']].copy()\n",
    "submission_df['quantidade'] = predictions\n",
    "\n",
    "# Converter semana para formato numérico (semana 1-5 de janeiro)\n",
    "submission_df['semana_num'] = submission_df['semana'].dt.isocalendar().week\n",
    "# Como são as semanas 1-5 de janeiro de 2023, ajustar numeração\n",
    "primeira_semana = submission_df['semana_num'].min()\n",
    "submission_df['semana'] = submission_df['semana_num'] - primeira_semana + 1\n",
    "\n",
    "# Renomear colunas para o padrão da submissão\n",
    "submission_df = submission_df.rename(columns={\n",
    "    'semana': 'semana',\n",
    "    'pdv_id': 'pdv', \n",
    "    'produto_id': 'produto'\n",
    "})\n",
    "\n",
    "# Selecionar apenas as colunas finais\n",
    "submission_df = submission_df[['semana', 'pdv', 'produto', 'quantidade']]\n",
    "\n",
    "print(f'   • DataFrame de submissão criado: {submission_df.shape}')\n",
    "print(f'   • Colunas: {list(submission_df.columns)}')\n",
    "print(f'   • Semanas: {sorted(submission_df[\"semana\"].unique())}')\n",
    "\n",
    "print(f'\\\\n📊 Estatísticas da submissão:')\n",
    "for semana in sorted(submission_df['semana'].unique()):\n",
    "    week_data = submission_df[submission_df['semana'] == semana]['quantidade']\n",
    "    print(f'   • Semana {semana}: {len(week_data):,} predições, média: {week_data.mean():.1f}')\n",
    "\n",
    "print(f'\\\\n✅ Dados de submissão preparados!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Salvando arquivos de submissão FINAL OPTUNA...\n",
      "🎯 Formatos: CSV (separador ;) e Parquet\n",
      "🚀 Integração: Remover zeros automaticamente (como no notebook 05)\n",
      "   • Pasta criada: ../submissions\n",
      "\n",
      "🔍 Validação final do DataFrame:\n",
      "   • Shape: (5221550, 4)\n",
      "   • Colunas: ['semana', 'pdv', 'produto', 'quantidade']\n",
      "   • Tipos: {'semana': UInt32Dtype(), 'pdv': CategoricalDtype(categories=['1000237487041964405', '1000275275922029725',\n",
      "                  '1000285336744471896', '1000699279948182033',\n",
      "                  '1001371918471115422', '1001617387599143258',\n",
      "                  '100190811186115530', '1002176693407703711',\n",
      "                  '1002531555980957298', '1002789537408335681',\n",
      "                  ...\n",
      "                  '994649300795071249', '994716099648444815',\n",
      "                  '995131700954448909', '996054429407066849',\n",
      "                  '996069461880825028', '996297376394222790',\n",
      "                  '997081098753891817', '997105059065723759',\n",
      "                  '997413166693459372', '997907800111849739'],\n",
      ", ordered=False, categories_dtype=object), 'produto': CategoricalDtype(categories=['1000423277513436210', '1000794189319388082',\n",
      "                  '1000817480846862155', '1004943868572044494',\n",
      "                  '1005164764874054592', '1006481780268592038',\n",
      "                  '1009179103632945474', '1010742812441057506',\n",
      "                  '1010928172993885810', '1011132854339272786',\n",
      "                  ...\n",
      "                  '989775702953852021', '990218543515228742',\n",
      "                  '992547330294993072', '993859875711154215',\n",
      "                  '994706710729219179', '996135836802743128',\n",
      "                  '997545005038320324', '997691518031628045',\n",
      "                  '998529431391212014', '999285078291803499'],\n",
      ", ordered=False, categories_dtype=object), 'quantidade': dtype('int64')}\n",
      "   • Missing values: 0\n",
      "   • Valores negativos: 0\n",
      "   • Valores zero: 4,915,370 (94.1%)\n",
      "\n",
      "🔧 PASSO CRÍTICO: Filtrando para manter apenas previsões com quantidade > 0...\n",
      "   • Linhas removidas (zeros): 4,915,370\n",
      "   • Linhas mantidas: 306,180\n",
      "   • Redução: 94.1%\n",
      "\n",
      "💾 Salvando arquivo COMPLETO (backup):\n",
      "   • CSV completo: ../submissions/submission_complete_with_zeros.csv\n",
      "   • Parquet completo: ../submissions/submission_complete_with_zeros.parquet\n",
      "\n",
      "🏆 Salvando arquivo FINAL OPTUNA (pronto para submissão):\n",
      "   • CSV FINAL: ../submissions/submission_final_optuna.csv\n",
      "   • Parquet FINAL: ../submissions/submission_final_optuna.parquet\n",
      "\n",
      "📊 Estatísticas dos arquivos FINAIS:\n",
      "   • CSV FINAL: 12.79 MB\n",
      "   • Parquet FINAL: 1.13 MB\n",
      "\n",
      "📈 Estatísticas da SUBMISSÃO FINAL OPTUNA:\n",
      "   • Semana 1: 61,236 predições, média: 2.7\n",
      "   • Semana 2: 61,236 predições, média: 2.7\n",
      "   • Semana 3: 61,236 predições, média: 2.7\n",
      "   • Semana 4: 61,236 predições, média: 2.7\n",
      "   • Semana 5: 61,236 predições, média: 2.7\n",
      "\n",
      "📊 Distribuição de quantidade na submissão final:\n",
      "   • Mín: 1\n",
      "   • Máx: 490\n",
      "   • Média: 2.68\n",
      "   • Mediana: 1.0\n",
      "   • Total de predições: 821,585\n",
      "\n",
      "👀 Preview da SUBMISSÃO FINAL OPTUNA (primeiras 10 linhas):\n",
      " semana                 pdv             produto  quantidade\n",
      "      1 1001371918471115422 2239307647969388381           2\n",
      "      1 1001371918471115422 2429796414123983930           1\n",
      "      1 1001371918471115422 3018796109558820617           1\n",
      "      1 1001371918471115422 3435249648591979862           1\n",
      "      1 1001371918471115422 3516587220187345473           1\n",
      "      1 1001371918471115422 3726736891643803768           1\n",
      "      1 1001371918471115422 3894706280449257667           1\n",
      "      1 1001371918471115422 4570988632387137438           1\n",
      "      1 1001371918471115422 4913890621258584650           1\n",
      "      1 1001371918471115422 5620797377392348813           1\n",
      "\n",
      "🎉 PROCESSO FINALIZADO - SUBMISSION FINAL OPTUNA PRONTA!\n",
      "======================================================================\n",
      "✅ Modelo LightGBM Optuna treinado com 51,171,190 registros de 2022\n",
      "✅ Parâmetros otimizados pelo Optuna aplicados\n",
      "✅ Predições filtradas: 306,180 (sem zeros)\n",
      "✅ Arquivo pronto: submission_final_optuna.csv\n",
      "✅ Formato: CSV com separador \";\" conforme regulamento\n",
      "\n",
      "🚀 ARQUIVO PRONTO PARA SUBMISSÃO:\n",
      "   📂 ../submissions/submission_final_optuna.csv\n",
      "   📏 12.79 MB\n",
      "   📊 306,180 linhas\n",
      "\n",
      "🏆 BOA SORTE NO HACKATHON!\n"
     ]
    }
   ],
   "source": [
    "# SALVAMENTO DOS ARQUIVOS DE SUBMISSÃO FINAL OPTUNA\n",
    "print('💾 Salvando arquivos de submissão FINAL OPTUNA...')\n",
    "print('🎯 Formatos: CSV (separador ;) e Parquet')\n",
    "print('🚀 Integração: Remover zeros automaticamente (como no notebook 05)')\n",
    "\n",
    "# PASSO 1: Criar pasta de submissão\n",
    "output_dir = '../submissions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f'   • Pasta criada: {output_dir}')\n",
    "\n",
    "# PASSO 2: Validar DataFrame antes de salvar\n",
    "print(f'\\n🔍 Validação final do DataFrame:')\n",
    "print(f'   • Shape: {submission_df.shape}')\n",
    "print(f'   • Colunas: {list(submission_df.columns)}')\n",
    "print(f'   • Tipos: {submission_df.dtypes.to_dict()}')\n",
    "print(f'   • Missing values: {submission_df.isnull().sum().sum()}')\n",
    "print(f'   • Valores negativos: {(submission_df[\"quantidade\"] < 0).sum()}')\n",
    "print(f'   • Valores zero: {(submission_df[\"quantidade\"] == 0).sum():,} ({(submission_df[\"quantidade\"] == 0).mean()*100:.1f}%)')\n",
    "\n",
    "# Verificar se há problemas\n",
    "if submission_df.isnull().sum().sum() > 0:\n",
    "    print('   ⚠️ Há valores missing - preenchendo com 0')\n",
    "    submission_df = submission_df.fillna(0)\n",
    "\n",
    "if (submission_df[\"quantidade\"] < 0).sum() > 0:\n",
    "    print('   ⚠️ Há valores negativos - convertendo para 0')\n",
    "    submission_df[\"quantidade\"] = submission_df[\"quantidade\"].clip(lower=0)\n",
    "\n",
    "# PASSO 3: 🎯 FILTRAR ZEROS (INTEGRAÇÃO DO NOTEBOOK 05)\n",
    "print(f'\\n🔧 PASSO CRÍTICO: Filtrando para manter apenas previsões com quantidade > 0...')\n",
    "submission_df_full = submission_df.copy()  # Backup da submissão completa\n",
    "submission_final = submission_df[submission_df['quantidade'] > 0].copy()\n",
    "\n",
    "linhas_removidas = len(submission_df_full) - len(submission_final)\n",
    "print(f'   • Linhas removidas (zeros): {linhas_removidas:,}')\n",
    "print(f'   • Linhas mantidas: {len(submission_final):,}')\n",
    "print(f'   • Redução: {(linhas_removidas/len(submission_df_full))*100:.1f}%')\n",
    "\n",
    "# PASSO 4: Salvar arquivo COMPLETO (com zeros) - para backup\n",
    "print(f'\\n💾 Salvando arquivo COMPLETO (backup):')\n",
    "csv_complete_path = os.path.join(output_dir, 'submission_complete_with_zeros.csv')\n",
    "parquet_complete_path = os.path.join(output_dir, 'submission_complete_with_zeros.parquet')\n",
    "\n",
    "submission_df_full.to_csv(csv_complete_path, sep=';', index=False, encoding='utf-8')\n",
    "submission_df_full.to_parquet(parquet_complete_path, index=False)\n",
    "print(f'   • CSV completo: {csv_complete_path}')\n",
    "print(f'   • Parquet completo: {parquet_complete_path}')\n",
    "\n",
    "# PASSO 5: Salvar arquivo FINAL OPTUNA (sem zeros) - para submissão\n",
    "print(f'\\n🏆 Salvando arquivo FINAL OPTUNA (pronto para submissão):')\n",
    "csv_final_path = os.path.join(output_dir, 'submission_final_optuna.csv')\n",
    "parquet_final_path = os.path.join(output_dir, 'submission_final_optuna.parquet')\n",
    "\n",
    "submission_final.to_csv(csv_final_path, sep=';', index=False, encoding='utf-8')\n",
    "submission_final.to_parquet(parquet_final_path, index=False)\n",
    "\n",
    "print(f'   • CSV FINAL: {csv_final_path}')\n",
    "print(f'   • Parquet FINAL: {parquet_final_path}')\n",
    "\n",
    "# PASSO 6: Verificar tamanhos dos arquivos\n",
    "csv_final_size = os.path.getsize(csv_final_path) / (1024**2)  # MB\n",
    "parquet_final_size = os.path.getsize(parquet_final_path) / (1024**2)  # MB\n",
    "\n",
    "print(f'\\n📊 Estatísticas dos arquivos FINAIS:')\n",
    "print(f'   • CSV FINAL: {csv_final_size:.2f} MB')\n",
    "print(f'   • Parquet FINAL: {parquet_final_size:.2f} MB')\n",
    "\n",
    "# PASSO 7: Estatísticas finais da submissão\n",
    "print(f'\\n📈 Estatísticas da SUBMISSÃO FINAL OPTUNA:')\n",
    "for semana in sorted(submission_final['semana'].unique()):\n",
    "    week_data = submission_final[submission_final['semana'] == semana]\n",
    "    print(f'   • Semana {semana}: {len(week_data):,} predições, média: {week_data[\"quantidade\"].mean():.1f}')\n",
    "\n",
    "print(f'\\n📊 Distribuição de quantidade na submissão final:')\n",
    "print(f'   • Mín: {submission_final[\"quantidade\"].min()}')\n",
    "print(f'   • Máx: {submission_final[\"quantidade\"].max()}')\n",
    "print(f'   • Média: {submission_final[\"quantidade\"].mean():.2f}')\n",
    "print(f'   • Mediana: {submission_final[\"quantidade\"].median():.1f}')\n",
    "print(f'   • Total de predições: {submission_final[\"quantidade\"].sum():,}')\n",
    "\n",
    "# PASSO 8: Visualizar preview da submissão final\n",
    "print(f'\\n👀 Preview da SUBMISSÃO FINAL OPTUNA (primeiras 10 linhas):')\n",
    "print(submission_final.head(10).to_string(index=False))\n",
    "\n",
    "print(f'\\n🎉 PROCESSO FINALIZADO - SUBMISSION FINAL OPTUNA PRONTA!')\n",
    "print(f'=' * 70)\n",
    "print(f'✅ Modelo LightGBM Optuna treinado com {len(y_full):,} registros de 2022')\n",
    "print(f'✅ Parâmetros otimizados pelo Optuna aplicados')\n",
    "print(f'✅ Predições filtradas: {len(submission_final):,} (sem zeros)')\n",
    "print(f'✅ Arquivo pronto: submission_final_optuna.csv')\n",
    "print(f'✅ Formato: CSV com separador \";\" conforme regulamento')\n",
    "print(f'')\n",
    "print(f'🚀 ARQUIVO PRONTO PARA SUBMISSÃO:')\n",
    "print(f'   📂 {csv_final_path}')\n",
    "print(f'   📏 {csv_final_size:.2f} MB')\n",
    "print(f'   📊 {len(submission_final):,} linhas')\n",
    "print(f'')\n",
    "print(f'🏆 BOA SORTE NO HACKATHON!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hackathon-forecast)",
   "language": "python",
   "name": "hackathon-forecast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
