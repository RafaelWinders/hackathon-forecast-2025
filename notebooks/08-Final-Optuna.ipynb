{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Pipeline Final para Submiss√£o\n",
    "\n",
    "**üéØ PROP√ìSITO DESTE NOTEBOOK:**\n",
    "Este notebook implementa o pipeline final para gerar as previs√µes de vendas para janeiro de 2023. Utiliza todos os dados de 2022 para treinar o modelo final e gera os arquivos de submiss√£o nos formatos requeridos.\n",
    "\n",
    "**üöÄ ESTRAT√âGIA OTIMIZADA:**\n",
    "- **Treino com Dataset Completo**: Usa todos os dados de 2022 (sem divis√£o treino/valida√ß√£o)\n",
    "- **Modelo Final**: LightGBM treinado com n√∫mero √≥timo de itera√ß√µes\n",
    "- **Predi√ß√µes Janeiro/2023**: Gera features para as 5 semanas de janeiro de 2023\n",
    "- **Submiss√£o**: Salva nos formatos CSV e Parquet na pasta correta\n",
    "\n",
    "---\n",
    "\n",
    "## Fluxo do Pipeline:\n",
    "1. **Carregamento**: Dados processados com features de `02-Feature-Engineering-Dask.ipynb`\n",
    "2. **Prepara√ß√£o**: Otimiza√ß√£o de mem√≥ria e uso do dataset completo de 2022\n",
    "3. **Treinamento**: Modelo final LightGBM com dados completos\n",
    "4. **Predi√ß√£o**: Gera√ß√£o de features para janeiro/2023 e previs√µes finais\n",
    "5. **Submiss√£o**: Arquivos finais em CSV e Parquet prontos para envio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Advanced ML\n",
    "import lightgbm as lgb\n",
    "from lightgbm.callback import early_stopping  # CORRE√á√ÉO: Importar early_stopping\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time series\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print('üìö Bibliotecas carregadas com sucesso!')\n",
    "print('üéØ Iniciando fase de Modelagem e Treinamento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados com features processadas\n",
    "print('üìÇ Carregando dados processados...')\n",
    "\n",
    "# Verificar se os arquivos essenciais existem\n",
    "import os\n",
    "required_files = [\n",
    "    '../data/dados_features_completo.parquet',  # Usar parquet (mais r√°pido)\n",
    "    '../data/feature_engineering_metadata.pkl'\n",
    "]\n",
    "\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print('‚ùå Arquivos n√£o encontrados:')\n",
    "    for f in missing_files:\n",
    "        print(f'   ‚Ä¢ {f}')\n",
    "    print('\\nüîÑ Execute primeiro o notebook 02-Feature-Engineering-Dask.ipynb')\n",
    "else:\n",
    "    print('‚úÖ Todos os arquivos necess√°rios encontrados')\n",
    "    \n",
    "    # Carregar dados principais (usar parquet para velocidade)\n",
    "    print('üìä Carregando dataset (parquet)...')\n",
    "    dados = pd.read_parquet('../data/dados_features_completo.parquet')\n",
    "    \n",
    "    # Carregar metadados\n",
    "    with open('../data/feature_engineering_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    print(f'\\nüìä Dados carregados com sucesso:')\n",
    "    print(f'   ‚Ä¢ Shape: {dados.shape}')\n",
    "    print(f'   ‚Ä¢ Per√≠odo: {dados[\"semana\"].min()} at√© {dados[\"semana\"].max()}')\n",
    "    print(f'   ‚Ä¢ Features dispon√≠veis: {len(dados.columns)}')\n",
    "    print(f'   ‚Ä¢ Mem√≥ria: {dados.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "    print(f'   ‚Ä¢ Estrat√©gia: {metadata.get(\"estrategia\", \"Grid Inteligente\")}')\n",
    "    \n",
    "    print(f'\\nüîç Metadados do processamento:')\n",
    "    for key, value in metadata.items():\n",
    "        if key not in ['features_criadas', 'data_processamento']:  # Skip long items\n",
    "            print(f'   ‚Ä¢ {key}: {value}')\n",
    "    \n",
    "    print(f'\\n‚úÖ Pronto para modelagem!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepara√ß√£o dos Dados para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir vari√°vel target e features\n",
    "target = 'quantidade'\n",
    "\n",
    "# Features a excluir (n√£o devem ser usadas para predi√ß√£o)\n",
    "exclude_features = [\n",
    "    'pdv_id', 'produto_id', 'semana',  # IDs e data\n",
    "    'quantidade',  # Target\n",
    "    'valor', 'num_transacoes',  # Features que vazam informa√ß√£o do futuro\n",
    "]\n",
    "\n",
    "# Identificar features dispon√≠veis\n",
    "all_features = [col for col in dados.columns if col not in exclude_features]\n",
    "\n",
    "print(f'üéØ Prepara√ß√£o dos dados:')\n",
    "print(f'   ‚Ä¢ Target: {target}')\n",
    "print(f'   ‚Ä¢ Features dispon√≠veis: {len(all_features)}')\n",
    "print(f'   ‚Ä¢ Features exclu√≠das: {len(exclude_features)}')\n",
    "\n",
    "# Verificar missing values nas features\n",
    "missing_features = dados[all_features].isnull().sum()\n",
    "missing_features = missing_features[missing_features > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\n‚ö†Ô∏è Features com valores missing:')\n",
    "    for feature, count in missing_features.head(10).items():\n",
    "        pct = (count / len(dados)) * 100\n",
    "        print(f'   ‚Ä¢ {feature}: {count:,} ({pct:.1f}%)')\n",
    "    \n",
    "    print(f'\\nüß† Estrat√©gia de Tratamento Inteligente:')\n",
    "    print('   ‚Ä¢ distributor_id (categ√≥rica): NaN ‚Üí -1 (venda direta)')\n",
    "    print('   ‚Ä¢ Features num√©ricas: NaN ‚Üí 0 (aus√™ncia = zero)')\n",
    "    print('   ‚Ä¢ LightGBM aprender√° padr√µes espec√≠ficos para valores -1/0')\n",
    "else:\n",
    "    print('\\n‚úÖ Nenhum valor missing nas features')\n",
    "\n",
    "print(f'\\nüìã Features finais para modelagem: {len(all_features)}')\n",
    "print('üí° Missing values ser√£o tratados como informa√ß√£o, n√£o removidos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE FINAL: Otimiza√ß√£o de Mem√≥ria + Prepara√ß√£o dos Dados Completos\n",
    "print('üß† Preparando dados de treino com o ano de 2022 completo...')\n",
    "print('üöÄ Estrat√©gia: Usar todos os dados dispon√≠veis para o modelo final')\n",
    "\n",
    "# PASSO 1: Inspecionar uso de mem√≥ria atual\n",
    "print(f'\\nüîç ANTES da otimiza√ß√£o:')\n",
    "memory_before = dados.memory_usage(deep=True).sum() / (1024**3)\n",
    "print(f'üíæ Mem√≥ria total: {memory_before:.2f} GB')\n",
    "\n",
    "# PASSO 2: Aplicar Downcasting Inteligente\n",
    "print(f'\\nüöÄ Aplicando Downcasting...')\n",
    "\n",
    "# Fazer uma c√≥pia para otimiza√ß√£o\n",
    "dados_sorted = dados.copy()\n",
    "\n",
    "# Otimizar colunas num√©ricas (inteiros e floats)\n",
    "for col in dados_sorted.select_dtypes(include=[np.number]).columns:\n",
    "    original_dtype = dados_sorted[col].dtype\n",
    "    \n",
    "    if dados_sorted[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        dados_sorted[col] = pd.to_numeric(dados_sorted[col], downcast='float')\n",
    "    \n",
    "    new_dtype = dados_sorted[col].dtype\n",
    "    if original_dtype != new_dtype:\n",
    "        print(f'   ‚Ä¢ {col}: {original_dtype} ‚Üí {new_dtype}')\n",
    "\n",
    "# Otimizar colunas categ√≥ricas\n",
    "for col in dados_sorted.select_dtypes(include=['object']).columns:\n",
    "    if col not in ['semana']:  # Preservar datetime\n",
    "        nunique = dados_sorted[col].nunique()\n",
    "        total_rows = len(dados_sorted)\n",
    "        if nunique / total_rows < 0.5:  # Se <50% valores √∫nicos, usar category\n",
    "            dados_sorted[col] = dados_sorted[col].astype('category')\n",
    "            print(f'   ‚Ä¢ {col}: object ‚Üí category')\n",
    "\n",
    "print(f'‚úÖ Downcasting conclu√≠do!')\n",
    "\n",
    "# PASSO 3: Verificar resultado da otimiza√ß√£o\n",
    "memory_after = dados_sorted.memory_usage(deep=True).sum() / (1024**3)\n",
    "memory_reduction = (memory_before - memory_after) / memory_before * 100\n",
    "print(f'\\nüìä DEPOIS da otimiza√ß√£o:')\n",
    "print(f'üíæ Mem√≥ria total: {memory_after:.2f} GB')\n",
    "print(f'üéØ Redu√ß√£o: {memory_reduction:.1f}% ({memory_before-memory_after:.2f} GB economizados)')\n",
    "\n",
    "# PASSO 4: Tratamento de missing values\n",
    "print(f'\\nüß† Tratamento inteligente de missing values...')\n",
    "all_features = [col for col in dados_sorted.columns if col not in ['pdv_id', 'produto_id', 'semana', 'quantidade', 'valor', 'num_transacoes']]\n",
    "\n",
    "for col in all_features:\n",
    "    missing_count = dados_sorted[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        if col == 'distributor_id':\n",
    "            # Adicionar -1 ao \"menu\" de categorias primeiro\n",
    "            if dados_sorted[col].dtype.name == 'category':\n",
    "                if -1 not in dados_sorted[col].cat.categories:\n",
    "                    dados_sorted[col] = dados_sorted[col].cat.add_categories([-1])\n",
    "            \n",
    "            dados_sorted[col] = dados_sorted[col].fillna(-1)\n",
    "            print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí -1 (venda direta)')\n",
    "            \n",
    "        elif dados_sorted[col].dtype.kind in ['i', 'u', 'f']:\n",
    "            dados_sorted[col] = dados_sorted[col].fillna(0)\n",
    "            print(f'   ‚Ä¢ {col}: {missing_count:,} NaN ‚Üí 0 (aus√™ncia)')\n",
    "\n",
    "# PASSO 5: Preparar dados COMPLETOS para modelagem (SEM divis√£o treino/valida√ß√£o)\n",
    "print(f'\\nüéØ Preparando dados completos para o modelo final...')\n",
    "\n",
    "# Usar todos os dados de 2022 para o treino final\n",
    "X_full = dados_sorted[all_features]\n",
    "y_full = dados_sorted[target]\n",
    "\n",
    "print(f'‚úÖ Dados de treino preparados:')\n",
    "print(f'   ‚Ä¢ X_full shape: {X_full.shape}')\n",
    "print(f'   ‚Ä¢ y_full shape: {y_full.shape}')\n",
    "print(f'   ‚Ä¢ Per√≠odo: {dados_sorted[\"semana\"].min()} at√© {dados_sorted[\"semana\"].max()}')\n",
    "print(f'   ‚Ä¢ Mem√≥ria X_full: {X_full.memory_usage(deep=True).sum() / (1024**2):.1f} MB')\n",
    "\n",
    "# Limpeza de mem√≥ria\n",
    "import gc\n",
    "del dados\n",
    "gc.collect()\n",
    "\n",
    "print(f'\\nüéâ SUCESSO! Dados preparados para treinamento final:')\n",
    "print(f'   ‚úÖ Dataset completo de 2022: {len(dados_sorted):,} registros')\n",
    "print(f'   ‚úÖ Features otimizadas: {len(all_features)}')\n",
    "print(f'   ‚úÖ Mem√≥ria otimizada: {memory_reduction:.1f}% de redu√ß√£o')\n",
    "print(f'   ‚úÖ Pronto para treinar modelo final!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TREINAMENTO DO MODELO FINAL COM PAR√ÇMETROS OTIMIZADOS\nprint('üöÄ Treinando o modelo LightGBM final com todos os dados de 2022...')\nprint('üéØ Usando par√¢metros otimizados pelo Optuna!')\nprint('=' * 60)\n\n# Carregar os melhores par√¢metros do Optuna\nprint('\\nüìÇ Carregando melhores par√¢metros de: ../data/best_lgbm_params_optuna.pkl')\ntry:\n    with open('../data/best_lgbm_params_optuna.pkl', 'rb') as f:\n        best_params_optuna = pickle.load(f)\n    print('‚úÖ Par√¢metros do Optuna carregados com sucesso!')\n\n    # Configurar par√¢metros finais combinando Optuna + configura√ß√µes fixas\n    lgb_params_final = best_params_optuna.copy()\n    lgb_params_final.update({\n        'objective': 'regression_l1',  # MAE - melhor para WMAPE\n        'metric': 'mae',\n        'boosting_type': 'gbdt',\n        'verbosity': -1,\n        'random_state': 42,\n        'n_jobs': -1\n    })\n\n    print(f'\\nüìã Configura√ß√£o do modelo final (Optuna + fixos):')\n    for param, value in lgb_params_final.items():\n        print(f'   ‚Ä¢ {param}: {value}')\n\n    # Usar early stopping com todos os dados (sem valida√ß√£o separada)\n    train_full_lgb = lgb.Dataset(X_full, label=y_full)\n\n    print(f'\\nüìä Preparando dados para treinamento...')\n    print(f'   ‚Ä¢ Dados shape: {X_full.shape}')\n    print(f'   ‚Ä¢ Features: {len(all_features)}')\n    print(f'   ‚Ä¢ Target: {target}')\n    print(f'   ‚Ä¢ Estrat√©gia: Usar num_boost_round otimizado + early stopping interno')\n\n    # Treinar modelo com par√¢metros otimizados\n    print(f'\\nüîÑ Treinando modelo final com par√¢metros do Optuna...')\n\n    # Usar o n_estimators do Optuna como num_boost_round\n    num_boost_rounds = lgb_params_final.pop('n_estimators', 1000)\n\n    final_model = lgb.train(\n        lgb_params_final,\n        train_full_lgb,\n        num_boost_round=num_boost_rounds\n    )\n\n    print(f'‚úÖ Modelo final treinado com sucesso!')\n    print(f'   ‚Ä¢ Itera√ß√µes utilizadas: {final_model.num_trees()}')\n\nexcept FileNotFoundError:\n    print('‚ùå Arquivo best_lgbm_params_optuna.pkl n√£o encontrado!')\n    print('üîÑ Usando par√¢metros padr√£o como fallback...')\n\n    # Fallback para par√¢metros vanilla\n    lgb_params_final = {\n        'objective': 'regression_l1',\n        'metric': 'mae',\n        'boosting_type': 'gbdt',\n        'verbosity': -1,\n        'random_state': 42,\n        'n_jobs': -1,\n        'learning_rate': 0.1,\n        'num_leaves': 31,\n        'max_depth': -1\n    }\n\n    train_full_lgb = lgb.Dataset(X_full, label=y_full)\n    final_model = lgb.train(\n        lgb_params_final,\n        train_full_lgb,\n        num_boost_round=200\n    )\n\n    print(f'‚úÖ Modelo fallback treinado com sucesso!')\n\n# Salvar o modelo\nimport os\nos.makedirs('../data', exist_ok=True)\n\nwith open('../data/final_lightgbm_model_optuna.pkl', 'wb') as f:\n    pickle.dump(final_model, f)\n\nprint('üíæ Modelo salvo em: ../data/final_lightgbm_model_optuna.pkl')\n\n# Informa√ß√µes do modelo\nprint(f'\\nüìà Informa√ß√µes do modelo final otimizado:')\nprint(f'   ‚Ä¢ N√∫mero de √°rvores: {final_model.num_trees()}')\nprint(f'   ‚Ä¢ Features utilizadas: {len(all_features)}')\nprint(f'   ‚Ä¢ Dados de treino: {len(y_full):,} registros')\nprint(f'   ‚Ä¢ Per√≠odo de treino: Todo o ano de 2022')\nprint(f'   ‚Ä¢ Otimiza√ß√£o: Par√¢metros do Optuna')\nprint(f'   ‚Ä¢ Pronto para predi√ß√µes!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gera√ß√£o de Features para Janeiro/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERA√á√ÉO DE FEATURES PARA JANEIRO/2023\n",
    "print('üìÖ Criando grid de dados para as 5 semanas de janeiro/2023...')\n",
    "print('üéØ Estrat√©gia: Criar template para todas as combina√ß√µes PDV/Produto')\n",
    "\n",
    "# PASSO 1: Obter todas as combina√ß√µes √∫nicas e suas informa√ß√µes do hist√≥rico\n",
    "print('\\nüîç Extraindo informa√ß√µes √∫nicas de PDV/Produto do hist√≥rico...')\n",
    "# CORRE√á√ÉO: Usar 'dados_sorted' como fonte √∫nica de verdade para informa√ß√µes est√°ticas\n",
    "available_cols = [col for col in ['pdv_id', 'produto_id', 'uf', 'distributor_id', 'brand'] if col in dados_sorted.columns]\n",
    "info_basica_df = dados_sorted[available_cols].drop_duplicates(subset=['pdv_id', 'produto_id'])\n",
    "print(f'   ‚Ä¢ Total de combina√ß√µes √∫nicas com informa√ß√µes: {len(info_basica_df):,}')\n",
    "print(f'   ‚Ä¢ Colunas dispon√≠veis: {available_cols}')\n",
    "\n",
    "# PASSO 2: Definir as 5 semanas de janeiro de 2023\n",
    "print('\\nüìÖ Definindo as 5 semanas de janeiro/2023...')\n",
    "prediction_weeks = pd.to_datetime(['2023-01-03', '2023-01-10', '2023-01-17', '2023-01-24', '2023-01-31'])\n",
    "print(f'   ‚Ä¢ Semanas: {[w.strftime(\"%Y-%m-%d\") for w in prediction_weeks]}')\n",
    "\n",
    "# PASSO 3: Criar o grid de teste (todas as combina√ß√µes x todas as semanas)\n",
    "print('\\nüèóÔ∏è Construindo grid de teste...')\n",
    "test_grid_list = []\n",
    "for week in prediction_weeks:\n",
    "    combo_week = info_basica_df.copy()\n",
    "    combo_week['semana'] = week\n",
    "    test_grid_list.append(combo_week)\n",
    "\n",
    "test_grid = pd.concat(test_grid_list, ignore_index=True)\n",
    "print(f'   ‚Ä¢ Grid shape: {test_grid.shape}')\n",
    "print(f'   ‚Ä¢ Total de predi√ß√µes: {len(test_grid):,}')\n",
    "\n",
    "# PASSO 4: Adicionar features temporais\n",
    "print('\\nüîß Gerando features temporais...')\n",
    "test_grid['ano'] = test_grid['semana'].dt.year\n",
    "test_grid['mes'] = test_grid['semana'].dt.month\n",
    "test_grid['semana_ano'] = test_grid['semana'].dt.isocalendar().week\n",
    "test_grid['dia_ano'] = test_grid['semana'].dt.dayofyear\n",
    "\n",
    "# Features temporais trigonom√©tricas (se existirem no treino)\n",
    "if 'mes_sin' in all_features:\n",
    "    test_grid['mes_sin'] = np.sin(2 * np.pi * test_grid['mes']/12)\n",
    "    test_grid['mes_cos'] = np.cos(2 * np.pi * test_grid['mes']/12)\n",
    "\n",
    "print('   ‚Ä¢ Features temporais adicionadas')\n",
    "\n",
    "# PASSO 5: Calcular lags e rolling windows (usando dados_sorted como hist√≥rico)\n",
    "print('\\nüìä Calculando features de lag e estat√≠sticas hist√≥ricas...')\n",
    "\n",
    "# Para cada combina√ß√£o PDV/Produto, calcular estat√≠sticas do hist√≥rico\n",
    "historical_stats = dados_sorted.groupby(['pdv_id', 'produto_id'])['quantidade'].agg([\n",
    "    'mean', 'std', 'min', 'max', 'count'\n",
    "]).reset_index()\n",
    "historical_stats.columns = ['pdv_id', 'produto_id', 'hist_mean', 'hist_std', 'hist_min', 'hist_max', 'hist_count']\n",
    "\n",
    "# Merge com estat√≠sticas hist√≥ricas\n",
    "test_grid = test_grid.merge(historical_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "# Para os lags, vamos usar as √∫ltimas semanas de 2022\n",
    "print('   ‚Ä¢ Calculando lags das √∫ltimas semanas de 2022...')\n",
    "last_week_2022 = dados_sorted[dados_sorted['semana'] == dados_sorted['semana'].max()]\n",
    "last_week_stats = last_week_2022[['pdv_id', 'produto_id', 'quantidade']].rename(columns={'quantidade': 'quantidade_lag_1'})\n",
    "test_grid = test_grid.merge(last_week_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "# Calcular mais lags se existirem no treino\n",
    "unique_weeks = sorted(dados_sorted['semana'].unique(), reverse=True)\n",
    "for lag in range(2, min(5, len(unique_weeks) + 1)):  # at√© lag_4\n",
    "    if f'quantidade_lag_{lag}' in all_features:\n",
    "        week_data = dados_sorted[dados_sorted['semana'] == unique_weeks[lag-1]]\n",
    "        lag_stats = week_data[['pdv_id', 'produto_id', 'quantidade']].rename(columns={'quantidade': f'quantidade_lag_{lag}'})\n",
    "        test_grid = test_grid.merge(lag_stats, on=['pdv_id', 'produto_id'], how='left')\n",
    "\n",
    "# Rolling features (se existirem no treino)\n",
    "lag_cols = [f'quantidade_lag_{i}' for i in range(1, 5) if f'quantidade_lag_{i}' in test_grid.columns]\n",
    "if len(lag_cols) > 1:\n",
    "    test_grid['quantidade_media_4w'] = test_grid[lag_cols].mean(axis=1)\n",
    "    test_grid['quantidade_std_4w'] = test_grid[lag_cols].std(axis=1)\n",
    "    test_grid['quantidade_max_4w'] = test_grid[lag_cols].max(axis=1)\n",
    "    test_grid['quantidade_min_4w'] = test_grid[lag_cols].min(axis=1)\n",
    "\n",
    "print('   ‚Ä¢ Features de lag e rolling adicionadas')\n",
    "\n",
    "# PASSO 6: Preencher valores ausentes e finalizar\n",
    "print('\\nüîß Finalizando e preenchendo valores ausentes...')\n",
    "\n",
    "# Identificar colunas num√©ricas para preencher com 0\n",
    "numeric_cols = test_grid.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    test_grid[col] = test_grid[col].fillna(0)\n",
    "\n",
    "# Tratar distributor_id especificamente\n",
    "if 'distributor_id' in test_grid.columns:\n",
    "    if test_grid['distributor_id'].dtype.name == 'category':\n",
    "        if -1 not in test_grid['distributor_id'].cat.categories:\n",
    "            test_grid['distributor_id'] = test_grid['distributor_id'].cat.add_categories([-1])\n",
    "    test_grid['distributor_id'] = test_grid['distributor_id'].fillna(-1)\n",
    "\n",
    "print(f'\\n‚úÖ Features para janeiro/2023 geradas com sucesso!')\n",
    "print(f'   ‚Ä¢ Grid final: {test_grid.shape}')\n",
    "print(f'   ‚Ä¢ Colunas: {len(test_grid.columns)}')\n",
    "print(f'   ‚Ä¢ Pronto para predi√ß√µes!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predi√ß√µes e Gera√ß√£o dos Arquivos de Submiss√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDI√á√ïES FINAIS E GERA√á√ÉO DE SUBMISS√ÉO\n",
    "print('üöÄ Realizando predi√ß√µes para janeiro/2023...')\n",
    "print('üéØ Usando modelo final treinado com todo o dataset de 2022')\n",
    "\n",
    "# PASSO 1: Preparar dados de teste com as mesmas features do treino\n",
    "print('\\nüîß Preparando dados de teste...')\n",
    "\n",
    "# Identificar features que existem tanto no treino quanto no teste\n",
    "available_test_features = [col for col in all_features if col in test_grid.columns]\n",
    "missing_features = [col for col in all_features if col not in test_grid.columns]\n",
    "\n",
    "print(f'   ‚Ä¢ Features dispon√≠veis no teste: {len(available_test_features)}')\n",
    "if missing_features:\n",
    "    print(f'   ‚ö†Ô∏è Features ausentes no teste: {len(missing_features)}')\n",
    "    print(f'     Criando com valores padr√£o: {missing_features[:5]}...')\n",
    "    \n",
    "    # Criar features ausentes com valores padr√£o (0 ou -1)\n",
    "    for feature in missing_features:\n",
    "        if 'distributor' in feature or 'id' in feature:\n",
    "            test_grid[feature] = -1\n",
    "        else:\n",
    "            test_grid[feature] = 0\n",
    "\n",
    "# Selecionar apenas as features que foram usadas no treinamento\n",
    "X_test = test_grid[all_features]\n",
    "\n",
    "print(f'   ‚Ä¢ X_test shape: {X_test.shape}')\n",
    "print(f'   ‚Ä¢ Features utilizadas: {len(all_features)}')\n",
    "\n",
    "# PASSO 2: Aplicar o mesmo tratamento de tipos de dados\n",
    "print('\\nüîß Aplicando otimiza√ß√µes de tipos de dados...')\n",
    "for col in X_test.select_dtypes(include=[np.number]).columns:\n",
    "    if X_test[col].dtype.kind in ['i', 'u']:  # Inteiros\n",
    "        X_test[col] = pd.to_numeric(X_test[col], downcast='integer')\n",
    "    else:  # Floats\n",
    "        X_test[col] = pd.to_numeric(X_test[col], downcast='float')\n",
    "\n",
    "# PASSO 3: Gerar predi√ß√µes\n",
    "print('\\nüéØ Gerando predi√ß√µes...')\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# Garantir que as previs√µes n√£o sejam negativas e arredondar para inteiros\n",
    "predictions = np.maximum(0, predictions).round().astype(int)\n",
    "\n",
    "print(f'   ‚Ä¢ Predi√ß√µes geradas: {len(predictions):,}')\n",
    "print(f'   ‚Ä¢ Range: {predictions.min()} - {predictions.max()}')\n",
    "print(f'   ‚Ä¢ M√©dia: {predictions.mean():.2f}')\n",
    "print(f'   ‚Ä¢ Zeros: {(predictions == 0).sum():,} ({(predictions == 0).mean()*100:.1f}%)')\n",
    "\n",
    "# PASSO 4: Criar o DataFrame de submiss√£o\n",
    "print('\\nüìã Criando DataFrame de submiss√£o...')\n",
    "\n",
    "# Preparar dados para submiss√£o\n",
    "submission_df = test_grid[['semana', 'pdv_id', 'produto_id']].copy()\n",
    "submission_df['quantidade'] = predictions\n",
    "\n",
    "# Converter semana para formato num√©rico (semana 1-5 de janeiro)\n",
    "submission_df['semana_num'] = submission_df['semana'].dt.isocalendar().week\n",
    "# Como s√£o as semanas 1-5 de janeiro de 2023, ajustar numera√ß√£o\n",
    "primeira_semana = submission_df['semana_num'].min()\n",
    "submission_df['semana'] = submission_df['semana_num'] - primeira_semana + 1\n",
    "\n",
    "# Renomear colunas para o padr√£o da submiss√£o\n",
    "submission_df = submission_df.rename(columns={\n",
    "    'semana': 'semana',\n",
    "    'pdv_id': 'pdv', \n",
    "    'produto_id': 'produto'\n",
    "})\n",
    "\n",
    "# Selecionar apenas as colunas finais\n",
    "submission_df = submission_df[['semana', 'pdv', 'produto', 'quantidade']]\n",
    "\n",
    "print(f'   ‚Ä¢ DataFrame de submiss√£o criado: {submission_df.shape}')\n",
    "print(f'   ‚Ä¢ Colunas: {list(submission_df.columns)}')\n",
    "print(f'   ‚Ä¢ Semanas: {sorted(submission_df[\"semana\"].unique())}')\n",
    "\n",
    "print(f'\\\\nüìä Estat√≠sticas da submiss√£o:')\n",
    "for semana in sorted(submission_df['semana'].unique()):\n",
    "    week_data = submission_df[submission_df['semana'] == semana]['quantidade']\n",
    "    print(f'   ‚Ä¢ Semana {semana}: {len(week_data):,} predi√ß√µes, m√©dia: {week_data.mean():.1f}')\n",
    "\n",
    "print(f'\\\\n‚úÖ Dados de submiss√£o preparados!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SALVAMENTO DOS ARQUIVOS DE SUBMISS√ÉO FINAL OPTUNA\nprint('üíæ Salvando arquivos de submiss√£o FINAL OPTUNA...')\nprint('üéØ Formatos: CSV (separador ;) e Parquet')\nprint('üöÄ Integra√ß√£o: Remover zeros automaticamente (como no notebook 05)')\n\n# PASSO 1: Criar pasta de submiss√£o\noutput_dir = '../submissions'\nos.makedirs(output_dir, exist_ok=True)\nprint(f'   ‚Ä¢ Pasta criada: {output_dir}')\n\n# PASSO 2: Validar DataFrame antes de salvar\nprint(f'\\nüîç Valida√ß√£o final do DataFrame:')\nprint(f'   ‚Ä¢ Shape: {submission_df.shape}')\nprint(f'   ‚Ä¢ Colunas: {list(submission_df.columns)}')\nprint(f'   ‚Ä¢ Tipos: {submission_df.dtypes.to_dict()}')\nprint(f'   ‚Ä¢ Missing values: {submission_df.isnull().sum().sum()}')\nprint(f'   ‚Ä¢ Valores negativos: {(submission_df[\"quantidade\"] < 0).sum()}')\nprint(f'   ‚Ä¢ Valores zero: {(submission_df[\"quantidade\"] == 0).sum():,} ({(submission_df[\"quantidade\"] == 0).mean()*100:.1f}%)')\n\n# Verificar se h√° problemas\nif submission_df.isnull().sum().sum() > 0:\n    print('   ‚ö†Ô∏è H√° valores missing - preenchendo com 0')\n    submission_df = submission_df.fillna(0)\n\nif (submission_df[\"quantidade\"] < 0).sum() > 0:\n    print('   ‚ö†Ô∏è H√° valores negativos - convertendo para 0')\n    submission_df[\"quantidade\"] = submission_df[\"quantidade\"].clip(lower=0)\n\n# PASSO 3: üéØ FILTRAR ZEROS (INTEGRA√á√ÉO DO NOTEBOOK 05)\nprint(f'\\nüîß PASSO CR√çTICO: Filtrando para manter apenas previs√µes com quantidade > 0...')\nsubmission_df_full = submission_df.copy()  # Backup da submiss√£o completa\nsubmission_final = submission_df[submission_df['quantidade'] > 0].copy()\n\nlinhas_removidas = len(submission_df_full) - len(submission_final)\nprint(f'   ‚Ä¢ Linhas removidas (zeros): {linhas_removidas:,}')\nprint(f'   ‚Ä¢ Linhas mantidas: {len(submission_final):,}')\nprint(f'   ‚Ä¢ Redu√ß√£o: {(linhas_removidas/len(submission_df_full))*100:.1f}%')\n\n# PASSO 4: Salvar arquivo COMPLETO (com zeros) - para backup\nprint(f'\\nüíæ Salvando arquivo COMPLETO (backup):')\ncsv_complete_path = os.path.join(output_dir, 'submission_complete_with_zeros.csv')\nparquet_complete_path = os.path.join(output_dir, 'submission_complete_with_zeros.parquet')\n\nsubmission_df_full.to_csv(csv_complete_path, sep=';', index=False, encoding='utf-8')\nsubmission_df_full.to_parquet(parquet_complete_path, index=False)\nprint(f'   ‚Ä¢ CSV completo: {csv_complete_path}')\nprint(f'   ‚Ä¢ Parquet completo: {parquet_complete_path}')\n\n# PASSO 5: Salvar arquivo FINAL OPTUNA (sem zeros) - para submiss√£o\nprint(f'\\nüèÜ Salvando arquivo FINAL OPTUNA (pronto para submiss√£o):')\ncsv_final_path = os.path.join(output_dir, 'submission_final_optuna.csv')\nparquet_final_path = os.path.join(output_dir, 'submission_final_optuna.parquet')\n\nsubmission_final.to_csv(csv_final_path, sep=';', index=False, encoding='utf-8')\nsubmission_final.to_parquet(parquet_final_path, index=False)\n\nprint(f'   ‚Ä¢ CSV FINAL: {csv_final_path}')\nprint(f'   ‚Ä¢ Parquet FINAL: {parquet_final_path}')\n\n# PASSO 6: Verificar tamanhos dos arquivos\ncsv_final_size = os.path.getsize(csv_final_path) / (1024**2)  # MB\nparquet_final_size = os.path.getsize(parquet_final_path) / (1024**2)  # MB\n\nprint(f'\\nüìä Estat√≠sticas dos arquivos FINAIS:')\nprint(f'   ‚Ä¢ CSV FINAL: {csv_final_size:.2f} MB')\nprint(f'   ‚Ä¢ Parquet FINAL: {parquet_final_size:.2f} MB')\n\n# PASSO 7: Estat√≠sticas finais da submiss√£o\nprint(f'\\nüìà Estat√≠sticas da SUBMISS√ÉO FINAL OPTUNA:')\nfor semana in sorted(submission_final['semana'].unique()):\n    week_data = submission_final[submission_final['semana'] == semana]\n    print(f'   ‚Ä¢ Semana {semana}: {len(week_data):,} predi√ß√µes, m√©dia: {week_data[\"quantidade\"].mean():.1f}')\n\nprint(f'\\nüìä Distribui√ß√£o de quantidade na submiss√£o final:')\nprint(f'   ‚Ä¢ M√≠n: {submission_final[\"quantidade\"].min()}')\nprint(f'   ‚Ä¢ M√°x: {submission_final[\"quantidade\"].max()}')\nprint(f'   ‚Ä¢ M√©dia: {submission_final[\"quantidade\"].mean():.2f}')\nprint(f'   ‚Ä¢ Mediana: {submission_final[\"quantidade\"].median():.1f}')\nprint(f'   ‚Ä¢ Total de predi√ß√µes: {submission_final[\"quantidade\"].sum():,}')\n\n# PASSO 8: Visualizar preview da submiss√£o final\nprint(f'\\nüëÄ Preview da SUBMISS√ÉO FINAL OPTUNA (primeiras 10 linhas):')\nprint(submission_final.head(10).to_string(index=False))\n\nprint(f'\\nüéâ PROCESSO FINALIZADO - SUBMISSION FINAL OPTUNA PRONTA!')\nprint(f'=' * 70)\nprint(f'‚úÖ Modelo LightGBM Optuna treinado com {len(y_full):,} registros de 2022')\nprint(f'‚úÖ Par√¢metros otimizados pelo Optuna aplicados')\nprint(f'‚úÖ Predi√ß√µes filtradas: {len(submission_final):,} (sem zeros)')\nprint(f'‚úÖ Arquivo pronto: submission_final_optuna.csv')\nprint(f'‚úÖ Formato: CSV com separador \";\" conforme regulamento')\nprint(f'')\nprint(f'üöÄ ARQUIVO PRONTO PARA SUBMISS√ÉO:')\nprint(f'   üìÇ {csv_final_path}')\nprint(f'   üìè {csv_final_size:.2f} MB')\nprint(f'   üìä {len(submission_final):,} linhas')\nprint(f'')\nprint(f'üèÜ BOA SORTE NO HACKATHON!')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}